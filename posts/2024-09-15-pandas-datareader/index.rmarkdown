---
title: "Analyzing CPI with Python and R"
description: "Accessing, analyzing, and visualizing data from the Federal Reserve"
author:
  - name: Mickey Rafa
    url: https://mrafa3.github.io/
    #orcid: 0000-0002-5300-3075
date: 09-11-2024
categories: [Python, pandas, api, data-viz, plotnine, R, Quarto, regression, matplotlib] # self-defined categories
#citation: 
 # url: https://mrafa3.github.io/posts/2024-07-15-wikipedia-international-mens-soccer/ 
code-annotations: hover
image: cpi_plot.png
draft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!
editor: 
  markdown: 
    wrap: 72
---


# Introduction

The `pandas-datareader` package is a powerful tool for easily accessing financial and economic data through various APIs. In this post, we'll explore how to use it to fetch data from FRED (Federal Reserve Economic Data). Then, I'll show some visualization techniques using `matplotlib`, `plotnine`, and `ggplot2`.

## Setup

First, import the packages:


```{python packages, include=TRUE}
import pandas as pd # <1>
from matplotlib import pyplot as plt # <1>
import pandas_datareader as pdr # <1>
from datetime import datetime # <1>
```


1. If it's your first time running code with these libraries, you'll need to first use the pip install command. Since these are already installed for me locally, I can just import.

Next, I'll set a variable for the time frame that I'd like to use for this demonstration.


```{python set_times, include=TRUE}
start = datetime(2016, 1, 1)
end = datetime.now()
```


# Fetching Data from the Federal Reserve

Let's fetch the US GDP data from Federal Reserve Economic Data (FRED) API:


```{python gdp_data, include=TRUE}
gdp_data = pdr.get_data_fred('GDP', start=start, end=end)

gdp_data.reset_index(inplace=True)

print(gdp_data.head())
```


And I'll show a quick plot with `matplotlib`. It's that fast to go from gathering data to creating a viz!


```{python gdp_plot, include=TRUE, message=FALSE}
plt.plot(gdp_data['DATE'], gdp_data['GDP'], color='green')
plt.suptitle('Monthly Gross Domestic Product for the United States')
plt.title('2016-2024')
plt.show()
```

```{python close1, include=TRUE, echo=FALSE}
plt.close('all')
```


You can grab stock market capitalization data by changing the first argument of the `pdr.get_data_fred()` function.


```{python market_cap_data, include=TRUE}
nasdaq_data = pdr.get_data_fred('NASDAQ100', start, end).reset_index()

sap_data = pdr.get_data_fred('SP500', start, end).reset_index()
```


And plot those together!


```{python plt_nasdaq_sp, include=TRUE}
plt.plot(nasdaq_data['DATE'], nasdaq_data['NASDAQ100'])
plt.plot(sap_data['DATE'], sap_data['SP500'])
plt.legend(['NASDAQ 100', 'S&P 500'])
plt.show()
```

```{python close2, include=TRUE, echo=FALSE}
plt.close('all')
```


With just a few lines of code, we've accessed data from the U.S. Federal Reserve. The pandas-datareader package simplifies the process of fetching data, which makes it a great package to get familiar with.

# Plotting with plotnine

Because I'm quite comfortable with R's ggplot syntax, I'll demonstrate how the plotnine package can visualize your Python data.


```{python import_plotnine, include=TRUE}
from plotnine import ggplot, aes, geom_line, theme_minimal, labs
```


I'll fetch Colorado's unemployment rate during this time period (January 2016 to August 2024).


```{python colorado_unemployment, include=TRUE}
colorado_unemployment = pdr.get_data_fred('COUR', start=start, end=end).reset_index()
```


Now, I'll visualize the monthly unemployment data using `plotnine`, the package to use ggplot in Python.


```{python plot_co_unemployment, message=FALSE, warning=FALSE, include=TRUE, fig.align='center'}
(
ggplot(colorado_unemployment, aes(x='DATE', y='COUR')) + 
  geom_line() + 
  labs(title='Colorado Unemployment Rate',
        subtitle='2016-2024',
        x='Date',
        y='Unemployment Rate (%)')+
  theme_minimal()
)
```


This code does the following:

*  Fetches the Colorado unemployment rate data using the FRED series ID 'COUR'.
*  Resets the index to make the date a regular column for plotting.
*  Uses `plotnine` (Python's implementation of ggplot2), we create a line plot of the unemployment rate over time.
*  Makes cosmetic adjustments, including adding the minimal theme and appropriate labels.

`plotnine`'s syntax, similar to R's `ggplot2::`, allows for easy customization and layering of plot elements. You can further enhance this plot by adding vertical lines for recession periods, changing color schemes, or adding annotations for significant events.

While this works, it is somewhat clunky. The x-axis is only showing one value, and the figure size can't be controlled with the code chunk options. I'll switch over to `ggplot2` and make a more polished graphic.


```{r load_r_packages, include=TRUE, echo=FALSE, output=FALSE}
library(tidyverse)
library(reticulate)
library(tidymodels)
```

```{r reticulate_python_to_r, include=TRUE}
colorado_unemployment <- reticulate::py$colorado_unemployment
```


Then, I'll produce the graphic in `ggplot2::` (notice that the x-axis renders better this way).


```{r ggplot_co_unemployment, include=TRUE, echo=FALSE, fig.width=10, fig.height=6.2, fig.align='center'}
colorado_unemployment %>% 
  ggplot(aes(x=DATE,
             y=COUR / 100)) + 
  geom_line() + 
  labs(title='Colorado Unemployment Rate',
       subtitle='2016 - 2024',
       x='',
       y='Unemployment Rate (%)') + 
  scale_y_continuous(labels = scales::percent) + 
  theme_minimal()
```


# Analysis of the Consumer Price Index 

The Federal Reserve is the resource for gathering all data related to the Consumer Price Index. There are many ways to look at this, including decomposing by the type of good, looking at it over different horizons or localities, and more. 
For this demo, I'll just look at the U.S. city average for all urban consumers.[^1] Again, I'll use `pandas_datareader` and pass through some simple arguments to fetch the data.

[^1]: The Federal Reserve website is a great place to comb through to see all the available data (https://fred.stlouisfed.org/). 


```{python get_cpi_data, include=TRUE}
cpi = pdr.get_data_fred('CPIAUCSL', start=start, end=end).reset_index()
```

For this section, I'd like to work in R, so I'll use `reticulate::` to make it an R object.


```{r retculate_cpi, include=TRUE, echo=FALSE}
cpi <- reticulate::py$cpi
```


When you're working with CPI data, it is common to index the data, or compare recent values to a historical value. I'll create an `indexed_cpi_2016` field, along with a `date_seq` field to allow some simple linear models.


```{r filter_2016, include=TRUE, echo=FALSE}
cpi <- cpi %>% 
  filter(DATE > '2016-01-01') %>% 
  mutate(indexed_cpi_2016 = CPIAUCSL / first(CPIAUCSL))
```

```{r ggplot_cpi, include=TRUE, fig.width=10, fig.height=6.2, fig.align='center'}
cpi %>% 
  ggplot(aes(x=DATE,
             y=indexed_cpi_2016)) + 
  geom_line() + 
  labs(title='Consumer Price Index in the United States',
       subtitle='January 2016 to August 2024, Index = January 2016',
       caption='Source: U.S. Federal Reserve\nIndicator = CPIAUCSL',
       x='',
       y='') + 
  theme_minimal()
```


This plot shows a clear rise in the indexed CPI from 2016 to present day. What stands out, too, is that there appear to be some distinct trends (possibly three) -- pre-pandemic, pandemic, and post-pandemic. I'd like to extend this mini-analysis to find the slopes of these periods and plot them on the above graphic.


```{r cpi_add_date_seq, include=TRUE, echo=FALSE}
cpi <- cpi %>% 
  arrange(DATE) %>%
  mutate(date_seq = row_number())
```


To do so, I'll create a function and use `tidymodels::` to build linear models that I can easily create and compare with one another.


```{r fit_cpi_model, include-TRUE}
fit_cpi_model <- function(df) {
  lm_model <- linear_reg() %>% set_engine("lm") # <2>
  lm_fit <- lm_model %>% # <3>
    fit(indexed_cpi_2016 ~ date_seq, data = df) # <3>
  return(lm_fit)
}
```


2. This sets up the model type as a simple linear model.
3. This is the function that will take in the `df` parameter and run the model

These create a `parsnip::` model object, which we can then explore the model results in tidy dataframes.


```{r run_cpi_models, include=TRUE}
lm_1 <- fit_cpi_model(cpi %>% filter(DATE <= '2020-03-01'))
lm_2 <- fit_cpi_model(cpi %>% filter(DATE > '2020-03-01' & DATE <= '2022-06-01'))
lm_3 <- fit_cpi_model(cpi %>% filter(DATE > '2022-06-01'))
```


The `broom::tidy()` function returns a row for each coefficient your model, which I'll use to compare the slopes of the different periods.


```{r tidy_lms, include=TRUE, echo=FALSE}
tidy_lm_1 <- tidy(lm_1) %>% as.data.frame() %>% mutate(model = 'Period 1') %>% select(model, everything())
tidy_lm_2 <- tidy(lm_2) %>% as.data.frame() %>% mutate(model = 'Period 2') %>% select(model, everything())
tidy_lm_3 <- tidy(lm_3) %>% as.data.frame() %>% mutate(model = 'Period 3') %>% select(model, everything())
```

```{r show_slopes, include=TRUE, echo=FALSE}
bind_rows(tidy_lm_1, tidy_lm_2, tidy_lm_3) %>% filter(term == 'date_seq') %>% select(1:3)
```

```{r slope_objects, include=TRUE, echo=FALSE}
slope_1 <- tidy_lm_1 %>% filter(model == 'Period 1', term == 'date_seq') %>% pull(estimate)
slope_2 <- tidy_lm_1 %>% filter(model == 'Period 2', term == 'date_seq') %>% pull(estimate)
slope_3 <- tidy_lm_1 %>% filter(model == 'Period 3', term == 'date_seq') %>% pull(estimate)
```


Here's what this tells us:

*  Pre-pandemic slope of .002
*  Pandemic-era slope of .006
*  Post-pandemic slope of .003

This is also apparent visually, as seen below.


```{r ggplot_cpi_polished, include=TRUE, fig.width=10, fig.height=6.2, fig.align='center'}
cpi %>% 
  ggplot(aes(x=DATE,
             y=indexed_cpi_2016)) + 
  geom_line() + 
  geom_smooth(data=. %>% filter(DATE <= '2020-03-01'),
              formula=y~ x,
              se=FALSE,
              color='blue',
              method='lm') + 
  geom_smooth(data=. %>% filter(DATE > '2020-03-01' & DATE <= '2022-06-01'),
              formula=y~ x,
              se=FALSE,
              color='orange',
              method='lm') + 
  geom_smooth(data=. %>% filter(DATE > '2022-06-01'),
              formula=y~ x,
              se=FALSE,
              color='blue',
              method='lm') + 
  labs(title='Consumer Price Index up 32% since January 2016...\nbut month over month growth is returning to pre-pandemic levels',
       subtitle='Consumer Price Index in the United States\nJanuary 2016 to August 2024, Index = January 2016',
       caption='Source: U.S. Federal Reserve\nIndicator = CPIAUCSL',
       x='',
       y='') + 
  theme_minimal() + 
  theme(plot.title = element_text(family = 'Palatino', size=20),
        plot.subtitle = element_text(family = 'Palatino', size=13),
        axis.text = element_text(family = 'Palatino', size=10))
```

Indexing off of January 2016, the Consumer Price Index is 32% higher in August 2024. What is interesting in this plot is the distinct trajectories seen pre-pandemic, from roughly 2020-2022, and from 2022 since. This shows that the CPI seems to be returning to its pre-pandemic trajectory.

# Conclusion

In this post on Pandas DataReader, I demonstrated:

*  How to fetch data from the U.S. Federal Reserve and World Bank using `pandas_datareader`
*  How to build basic plots using `matplotlib` and `plotnine`
*  How to convert Python to R objects using `reticulate::`
*  How to use `tidymodels::` to build some quick regression models and compare results

I'll be digging into some decomposed CPI analysis in future posts!
