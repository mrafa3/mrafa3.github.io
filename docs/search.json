[
  {
    "objectID": "posts/2024-08-13-tidycensus-exploration/index.html",
    "href": "posts/2024-08-13-tidycensus-exploration/index.html",
    "title": "Part One: Exploring child poverty data with the tidycensus R package",
    "section": "",
    "text": "I’ve used data from the U.S. Census Bureau several times, and for this project, I wanted to reacquaint myself with the tidycensus:: package to gather and wrangle data. I also wanted to use the usmap:: package to generate a simple U.S. map, and the gt:: package to display the data in a nice table format.\n\n\n\nlibrary(tidyverse)\nlibrary(tidycensus)\n1library(scales)\n2library(janitor)\n3library(glue)\n4library(gt)\nlibrary(usmap)\n\n5# census_api_key('INSERT KEY HERE', install = TRUE)\n\n\n1\n\nLoading the scales:: package to transform ggplot scales simply (some people choose to explicitly define scales:: in their code rather than loading the library).\n\n2\n\nThe janitor::clean_names() function tidies the column names of your dataset to use the snake case convention. Very handy!\n\n3\n\nThe glue:: package allows for simple addition of HTML to ggplot graphics.\n\n4\n\nThe gt:: library provides functionality for creating ggplot-esque tables.\n\n5\n\nThe first time that you’re working with the tidycensus:: package, you need to request an API key at https://api.census.gov/data/key_signup.html. The install= argument will install your personal key to the .Renviron file, and you won’t need to use the census_api_key() function again.\n\n\n\n\n\n\n\nFor this analysis, I’m interested in looking at the most recent state-level child poverty data available from the U.S. Census Bureau. The tidycensus:: package allows API access to the decennial Census, as well as the more frequent American Community Survey (ACS), which I’ll use in this project.\nIf you’ve worked with ACS data before, you may know that there are a few survey products offered in the ACS suite. Most commonly, the choice of data is between the 1-year estimates and the 5-year estimates.\nWhat’s the difference between these, and how do you choose which survey product to use for your purposes?1\n\n\n\n\n\n\n\n\nFeature\nACS 1-Year Estimates\nACS 5-Year Estimates\n\n\n\n\nData Collection Period\n12 months\n60 months\n\n\nPopulation Coverage\nAreas with 65,000 or more people\nAll geographic areas, including those with fewer than 65,000 people\n\n\nSample Size\nSmallest\nLargest\n\n\nReliability\nLess reliable due to smaller sample size\nMore reliable due to larger sample size\n\n\nCurrency\nMost current data\nLess current, includes older data\n\n\nRelease Frequency\nAnnually\nAnnually\n\n\nBest Used For\nAnalyzing large populations, when currency is more important than precision\nAnalyzing small populations, where precision is more important than currency\n\n\nExample Usage\nExamining recent economic changes\nExamining trends in small geographic areas or small population subgroups\n\n\n\n\nTo start, I am interested in reviewing the most stable, geographically-available data on child poverty. Given that I’m less concerned with recency and more interested in broad availability, the ACS 5-year estimates are what I’ll use here.\n\n\n\nThe tidycensus:: package has so much to offer (and I still have plenty to learn!). The tidycensus::load_variables() function provides a simple way to query the available data within each survey. Combining this with stringr::str_detect() is a nice way to search through the tens of thousands of data series that are available through the U.S. Census API.\n\nload_variables(2022, \"acs5\", cache = TRUE) %&gt;% \n6  filter(str_detect(label, \"Under 5 years\"))\n\n\n6\n\nThis searches all variable labels for “Under 5 years” to help identify data of interest.\n\n\n\n\n# A tibble: 240 × 4\n   name        label                                    concept        geography\n   &lt;chr&gt;       &lt;chr&gt;                                    &lt;chr&gt;          &lt;chr&gt;    \n 1 B01001A_003 Estimate!!Total:!!Male:!!Under 5 years   Sex by Age (W… tract    \n 2 B01001A_018 Estimate!!Total:!!Female:!!Under 5 years Sex by Age (W… tract    \n 3 B01001B_003 Estimate!!Total:!!Male:!!Under 5 years   Sex by Age (B… tract    \n 4 B01001B_018 Estimate!!Total:!!Female:!!Under 5 years Sex by Age (B… tract    \n 5 B01001C_003 Estimate!!Total:!!Male:!!Under 5 years   Sex by Age (A… tract    \n 6 B01001C_018 Estimate!!Total:!!Female:!!Under 5 years Sex by Age (A… tract    \n 7 B01001D_003 Estimate!!Total:!!Male:!!Under 5 years   Sex by Age (A… tract    \n 8 B01001D_018 Estimate!!Total:!!Female:!!Under 5 years Sex by Age (A… tract    \n 9 B01001E_003 Estimate!!Total:!!Male:!!Under 5 years   Sex by Age (N… tract    \n10 B01001E_018 Estimate!!Total:!!Female:!!Under 5 years Sex by Age (N… tract    \n# ℹ 230 more rows\n\n\nFor this demo, I’ll use the following series:\n\nB01001_003: Estimate!!Total:!!Male:!!Under 5 years (all racial groups)\nB01001_027: Estimate!!Total:!!Female:!!Under 5 years (all racial groups)\nB17001_004: Estimate!!Total:!!Income in the past 12 months below poverty level:!!Male:!!Under 5 years\nB17001_018: Estimate!!Total:!!Income in the past 12 months below poverty level:!!Female:!!Under 5 years\n\nThere are a bunch of useful helper functions/arguments to assist in fetching data from the Census API. Some noteworthy ones include:\n\nEach variable returns the geography, an estimate, and the margin of error (“moe”). Geographies can span from states, regions and the country as a whole, down to areas like school districts, voting districts, census block groups, and many others.\nsurvey=: this defines the produce that you’re using of the American Community Survey. Responses can include “acs1”, “acs3”, or (the default) “acs5”.\nsummary_var=: often the variable that you want would be made more meaningful as a ratio or with a demonminator. For example, the number of children in poverty could be useful on its own, but you’re likely to want to see that series as a percent of the total children. With the summary_var argument, you can tell the function which secondary variable you want to grab in the same API call.\nouput=wide: related to the above, I wanted to look at child poverty in a way that would require multiple summary variables (e.g. the percent of girls and boys in poverty). Since you can only have one summary variable, output='wide' allows you to grab all of the series that you may need in the same call.\ngeometry=TRUE: this argument returns the geospatial data in tidy format to create quick ggplot-based maps using geom_sf().\n\n\ndf &lt;- get_acs(geography = 'state',\n        variables = c(male_u5_pop = 'B01001_003', \n                      female_u5_pop = 'B01001_027', \n                      male_u5_poverty = 'B17001_004', \n                      female_u5_poverty = 'B17001_018'),\n        survey = 'acs5',\n        year = 2022,\n        output = 'wide')\n\nGetting data from the 2018-2022 5-year ACS\n\n\nNext, I’ll create some fields to combine gender-based poverty estimates and calculate a percent of the child population measure.\n\ndf &lt;- df %&gt;% \n  rename(state = NAME) %&gt;% \n  mutate(total_u5_popE = male_u5_popE + female_u5_popE,\n         total_u5_povertyE = male_u5_povertyE + male_u5_povertyE,\n         perc_u5_in_poverty = total_u5_povertyE / total_u5_popE)\n\nYou can combine point estimates for gender-based poverty by simply adding them, but you can’t do the same for margins of error. The estimated margins of error for each estimate are based on their respective samples and the uncertainty in them. To re-weight the margins of error, you take the square root of the sum of squared margins of error:\n\nnew_MOE_calc = sqrt(male_u5_povertyM^2 + female_u5_povertyM^2)\n\nThe tidycensus::moe_sum() function will calculate the new margin of error for you!\n\ndf &lt;- df %&gt;% \n  group_by(state) %&gt;% \n  mutate(total_u5_poverty_MOEcalc = moe_sum(male_u5_povertyM, female_u5_povertyM),\n         perc_u5_in_poverty_MOEcalc = total_u5_poverty_MOEcalc / total_u5_popE)\n\nNow, I can visualize the new point estimates and margins of error for child poverty (not specific to gender).\n\ndf %&gt;% \n7  filter(state != 'Puerto Rico') %&gt;%\n  ggplot(\n    aes(x=perc_u5_in_poverty, \n        y=reorder(state, perc_u5_in_poverty))) + \n  geom_point(size=3) +\n  geom_errorbarh(aes(xmin=perc_u5_in_poverty - perc_u5_in_poverty_MOEcalc, \n                     xmax=perc_u5_in_poverty + perc_u5_in_poverty_MOEcalc),\n                 height = 0.4) + \n  labs(y='',\n       x='',\n       caption=caption_text) + \n  ggtitle(title_text,\n          subtitle=subtitle_text) + \n  scale_x_continuous(labels = percent,\n                     position = 'top') + \n  my.theme\n\n\n7\n\nRemoving Puerto Rico because it’s not within a U.S. Census region."
  },
  {
    "objectID": "posts/2024-08-13-tidycensus-exploration/index.html#setup",
    "href": "posts/2024-08-13-tidycensus-exploration/index.html#setup",
    "title": "Part One: Exploring child poverty data with the tidycensus R package",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidycensus)\n1library(scales)\n2library(janitor)\n3library(glue)\n4library(gt)\nlibrary(usmap)\n\n5# census_api_key('INSERT KEY HERE', install = TRUE)\n\n\n1\n\nLoading the scales:: package to transform ggplot scales simply (some people choose to explicitly define scales:: in their code rather than loading the library).\n\n2\n\nThe janitor::clean_names() function tidies the column names of your dataset to use the snake case convention. Very handy!\n\n3\n\nThe glue:: package allows for simple addition of HTML to ggplot graphics.\n\n4\n\nThe gt:: library provides functionality for creating ggplot-esque tables.\n\n5\n\nThe first time that you’re working with the tidycensus:: package, you need to request an API key at https://api.census.gov/data/key_signup.html. The install= argument will install your personal key to the .Renviron file, and you won’t need to use the census_api_key() function again."
  },
  {
    "objectID": "posts/2024-08-13-tidycensus-exploration/index.html#data",
    "href": "posts/2024-08-13-tidycensus-exploration/index.html#data",
    "title": "Part One: Exploring child poverty data with the tidycensus R package",
    "section": "",
    "text": "For this analysis, I’m interested in looking at the most recent state-level child poverty data available from the U.S. Census Bureau. The tidycensus:: package allows API access to the decennial Census, as well as the more frequent American Community Survey (ACS), which I’ll use in this project.\nIf you’ve worked with ACS data before, you may know that there are a few survey products offered in the ACS suite. Most commonly, the choice of data is between the 1-year estimates and the 5-year estimates.\nWhat’s the difference between these, and how do you choose which survey product to use for your purposes?1\n\n\n\n\n\n\n\n\nFeature\nACS 1-Year Estimates\nACS 5-Year Estimates\n\n\n\n\nData Collection Period\n12 months\n60 months\n\n\nPopulation Coverage\nAreas with 65,000 or more people\nAll geographic areas, including those with fewer than 65,000 people\n\n\nSample Size\nSmallest\nLargest\n\n\nReliability\nLess reliable due to smaller sample size\nMore reliable due to larger sample size\n\n\nCurrency\nMost current data\nLess current, includes older data\n\n\nRelease Frequency\nAnnually\nAnnually\n\n\nBest Used For\nAnalyzing large populations, when currency is more important than precision\nAnalyzing small populations, where precision is more important than currency\n\n\nExample Usage\nExamining recent economic changes\nExamining trends in small geographic areas or small population subgroups\n\n\n\n\nTo start, I am interested in reviewing the most stable, geographically-available data on child poverty. Given that I’m less concerned with recency and more interested in broad availability, the ACS 5-year estimates are what I’ll use here."
  },
  {
    "objectID": "posts/2024-08-13-tidycensus-exploration/index.html#initial-tour-of-key-tidycensusget_acs-function",
    "href": "posts/2024-08-13-tidycensus-exploration/index.html#initial-tour-of-key-tidycensusget_acs-function",
    "title": "Part One: Exploring child poverty data with the tidycensus R package",
    "section": "",
    "text": "The tidycensus:: package has so much to offer (and I still have plenty to learn!). The tidycensus::load_variables() function provides a simple way to query the available data within each survey. Combining this with stringr::str_detect() is a nice way to search through the tens of thousands of data series that are available through the U.S. Census API.\n\nload_variables(2022, \"acs5\", cache = TRUE) %&gt;% \n6  filter(str_detect(label, \"Under 5 years\"))\n\n\n6\n\nThis searches all variable labels for “Under 5 years” to help identify data of interest.\n\n\n\n\n# A tibble: 240 × 4\n   name        label                                    concept        geography\n   &lt;chr&gt;       &lt;chr&gt;                                    &lt;chr&gt;          &lt;chr&gt;    \n 1 B01001A_003 Estimate!!Total:!!Male:!!Under 5 years   Sex by Age (W… tract    \n 2 B01001A_018 Estimate!!Total:!!Female:!!Under 5 years Sex by Age (W… tract    \n 3 B01001B_003 Estimate!!Total:!!Male:!!Under 5 years   Sex by Age (B… tract    \n 4 B01001B_018 Estimate!!Total:!!Female:!!Under 5 years Sex by Age (B… tract    \n 5 B01001C_003 Estimate!!Total:!!Male:!!Under 5 years   Sex by Age (A… tract    \n 6 B01001C_018 Estimate!!Total:!!Female:!!Under 5 years Sex by Age (A… tract    \n 7 B01001D_003 Estimate!!Total:!!Male:!!Under 5 years   Sex by Age (A… tract    \n 8 B01001D_018 Estimate!!Total:!!Female:!!Under 5 years Sex by Age (A… tract    \n 9 B01001E_003 Estimate!!Total:!!Male:!!Under 5 years   Sex by Age (N… tract    \n10 B01001E_018 Estimate!!Total:!!Female:!!Under 5 years Sex by Age (N… tract    \n# ℹ 230 more rows\n\n\nFor this demo, I’ll use the following series:\n\nB01001_003: Estimate!!Total:!!Male:!!Under 5 years (all racial groups)\nB01001_027: Estimate!!Total:!!Female:!!Under 5 years (all racial groups)\nB17001_004: Estimate!!Total:!!Income in the past 12 months below poverty level:!!Male:!!Under 5 years\nB17001_018: Estimate!!Total:!!Income in the past 12 months below poverty level:!!Female:!!Under 5 years\n\nThere are a bunch of useful helper functions/arguments to assist in fetching data from the Census API. Some noteworthy ones include:\n\nEach variable returns the geography, an estimate, and the margin of error (“moe”). Geographies can span from states, regions and the country as a whole, down to areas like school districts, voting districts, census block groups, and many others.\nsurvey=: this defines the produce that you’re using of the American Community Survey. Responses can include “acs1”, “acs3”, or (the default) “acs5”.\nsummary_var=: often the variable that you want would be made more meaningful as a ratio or with a demonminator. For example, the number of children in poverty could be useful on its own, but you’re likely to want to see that series as a percent of the total children. With the summary_var argument, you can tell the function which secondary variable you want to grab in the same API call.\nouput=wide: related to the above, I wanted to look at child poverty in a way that would require multiple summary variables (e.g. the percent of girls and boys in poverty). Since you can only have one summary variable, output='wide' allows you to grab all of the series that you may need in the same call.\ngeometry=TRUE: this argument returns the geospatial data in tidy format to create quick ggplot-based maps using geom_sf().\n\n\ndf &lt;- get_acs(geography = 'state',\n        variables = c(male_u5_pop = 'B01001_003', \n                      female_u5_pop = 'B01001_027', \n                      male_u5_poverty = 'B17001_004', \n                      female_u5_poverty = 'B17001_018'),\n        survey = 'acs5',\n        year = 2022,\n        output = 'wide')\n\nGetting data from the 2018-2022 5-year ACS\n\n\nNext, I’ll create some fields to combine gender-based poverty estimates and calculate a percent of the child population measure.\n\ndf &lt;- df %&gt;% \n  rename(state = NAME) %&gt;% \n  mutate(total_u5_popE = male_u5_popE + female_u5_popE,\n         total_u5_povertyE = male_u5_povertyE + male_u5_povertyE,\n         perc_u5_in_poverty = total_u5_povertyE / total_u5_popE)\n\nYou can combine point estimates for gender-based poverty by simply adding them, but you can’t do the same for margins of error. The estimated margins of error for each estimate are based on their respective samples and the uncertainty in them. To re-weight the margins of error, you take the square root of the sum of squared margins of error:\n\nnew_MOE_calc = sqrt(male_u5_povertyM^2 + female_u5_povertyM^2)\n\nThe tidycensus::moe_sum() function will calculate the new margin of error for you!\n\ndf &lt;- df %&gt;% \n  group_by(state) %&gt;% \n  mutate(total_u5_poverty_MOEcalc = moe_sum(male_u5_povertyM, female_u5_povertyM),\n         perc_u5_in_poverty_MOEcalc = total_u5_poverty_MOEcalc / total_u5_popE)\n\nNow, I can visualize the new point estimates and margins of error for child poverty (not specific to gender).\n\ndf %&gt;% \n7  filter(state != 'Puerto Rico') %&gt;%\n  ggplot(\n    aes(x=perc_u5_in_poverty, \n        y=reorder(state, perc_u5_in_poverty))) + \n  geom_point(size=3) +\n  geom_errorbarh(aes(xmin=perc_u5_in_poverty - perc_u5_in_poverty_MOEcalc, \n                     xmax=perc_u5_in_poverty + perc_u5_in_poverty_MOEcalc),\n                 height = 0.4) + \n  labs(y='',\n       x='',\n       caption=caption_text) + \n  ggtitle(title_text,\n          subtitle=subtitle_text) + \n  scale_x_continuous(labels = percent,\n                     position = 'top') + \n  my.theme\n\n\n7\n\nRemoving Puerto Rico because it’s not within a U.S. Census region."
  },
  {
    "objectID": "posts/2024-08-13-tidycensus-exploration/index.html#footnotes",
    "href": "posts/2024-08-13-tidycensus-exploration/index.html#footnotes",
    "title": "Part One: Exploring child poverty data with the tidycensus R package",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee the Census handbook for great detail: https://www.census.gov/programs-surveys/acs/library/handbooks/general.html.↩︎"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Portfolio",
    "section": "",
    "text": "Analyzing CPI with Python and R\n\n\n\nPython\n\n\npandas\n\n\nAPI\n\n\ndata-viz\n\n\nplotnine\n\n\nR\n\n\nQuarto\n\n\nregression\n\n\nmatplotlib\n\n\n\nAccessing, analyzing, and visualizing data from the Federal Reserve\n\n\n\nMickey Rafa\n\n\nSep 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart Three: Exploring even smaller geographies with tidycensus\n\n\n\nR\n\n\ntidyverse\n\n\ntidycensus\n\n\nmapping\n\n\napi\n\n\n\nThis post shows how to fetch data at smaller scales for analysis of Census data\n\n\n\nMickey Rafa\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart Two: Wrangling Census data for longitudinal analysis of child poverty\n\n\n\nR\n\n\ndata-viz\n\n\ntidyverse\n\n\ntidycensus\n\n\napi\n\n\njavascript\n\n\npurrr\n\n\n\nThis post shows how to fetch many years of data simply, unlocking longitudinal analysis of Census data\n\n\n\nMickey Rafa\n\n\nAug 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up an EPA air quality database using DuckDB\n\n\n\nR\n\n\ntidyverse\n\n\nduckdb\n\n\nenvironment\n\n\nsql\n\n\napi\n\n\n\nThis post uses the RAQSAPI package to access the EPA API and query the results with DuckDB\n\n\n\nMickey Rafa\n\n\nAug 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart One: Exploring child poverty data with the tidycensus R package\n\n\n\nR\n\n\ntidyverse\n\n\nmapping\n\n\ngt\n\n\ntidycensus\n\n\napi\n\n\n\nThis post uses the tidycensus package to access the Census API and visualize data on child poverty in the U.S.\n\n\n\nMickey Rafa\n\n\nAug 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring interactive mapping with mapboxgl\n\n\n\nR\n\n\ntidyverse\n\n\nmapping\n\n\n\nThis post explores some mapboxgl features\n\n\n\nMickey Rafa\n\n\nAug 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBrainTree SQL challenge\n\n\n\nSQL\n\n\nQuarto\n\n\n\nThis post works through the SQL challenges used by PayPal\n\n\n\nMickey Rafa\n\n\nJul 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuery comparison: SQL, R, and Python using Chicago Employees dataset\n\n\n\nR\n\n\nSQL\n\n\nPython\n\n\nQuarto\n\n\ngenerative-AI\n\n\n\nThis post demonstrates how to run these languages in Quarto\n\n\n\nMickey Rafa\n\n\nJul 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Science Workflows in Javascript: Part Three\n\n\n\nQuarto\n\n\nJavascript\n\n\ndata-viz\n\n\npca\n\n\nenvironment\n\n\n\nThis post is from the third session led by Observable HQ\n\n\n\nMickey Rafa\n\n\nJul 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Science Workflows in Javascript: Part Two\n\n\n\nQuarto\n\n\nJavascript\n\n\ndata-viz\n\n\ncluster-analysis\n\n\nenvironment\n\n\n\nThis post is from the second session led by Observable HQ\n\n\n\nMickey Rafa\n\n\nJul 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Science Workflows in Javascript: Part One\n\n\n\nQuarto\n\n\nJavascript\n\n\ndata-viz\n\n\nregression\n\n\nenvironment\n\n\n\nThis post is from the first session led by Observable HQ\n\n\n\nMickey Rafa\n\n\nJul 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding beautiful tables with the gt:: package in R\n\n\n\nR\n\n\nweb-scraping\n\n\ngt\n\n\nsports\n\n\n\nThis post outlines how to quickly scrape and clean data from Wikipedia and build a beautiful table in R\n\n\n\nMickey Rafa\n\n\nDec 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPART TWO: Mapping of African Health Clusters\n\n\n\nR\n\n\ndata-viz\n\n\ntidyverse\n\n\ninternational-development\n\n\ncluster-analysis\n\n\nmapping\n\n\n\nCreating maps of cluster results using ggplot\n\n\n\nMickey Rafa\n\n\nJun 18, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPART ONE: Cluster Analysis of African Health Outcomes\n\n\n\nR\n\n\ndata-viz\n\n\ntidyverse\n\n\ninternational-development\n\n\ncluster-analysis\n\n\n\nExploring how African countries cluster based on their health outcomes\n\n\n\nMickey Rafa\n\n\nJun 18, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExperimenting with distance as a concept for SDG achievement\n\n\n\nR\n\n\ndata-viz\n\n\ntidyverse\n\n\ninternational-development\n\n\ncluster-analysis\n\n\nmapping\n\n\n\nExploring how to conceptualize and visualize distance from Sustainable Development Goal targets\n\n\n\nMickey Rafa\n\n\nApr 15, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nAnimating the Gapminder dataset using gganimate\n\n\n\nR\n\n\ndata-viz\n\n\ntidyverse\n\n\ngganimate\n\n\n\nThis post outlines how to easily add animation to ggplot graphics\n\n\n\nMickey Rafa\n\n\nOct 15, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Gapminder data using nested models in R\n\n\n\nR\n\n\nregression\n\n\ndata-viz\n\n\ntidyverse\n\n\ninternational-development\n\n\n\nThis post outlines the value in building nested models using the purrr and broom packages\n\n\n\nMickey Rafa\n\n\nOct 1, 2017\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "workshops.html",
    "href": "workshops.html",
    "title": "Workshops",
    "section": "",
    "text": "PART TWO: Data Manipulation and Visualization Basics using R\n\n\n\nR\n\n\ntidyverse\n\n\nggplot\n\n\ndata-viz\n\n\ndomestic-politics\n\n\n\nThis skills workshop introduced R and the tidyverse to Master’s students at the University of Denver\n\n\n\nMickey Rafa\n\n\nMay 15, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPART ONE: Data Manipulation and Visualization Basics using R\n\n\n\nR\n\n\ntidyverse\n\n\nggplot\n\n\ndata-viz\n\n\ninternational-development\n\n\n\nThis skills workshop introduced R and the tidyverse to Master’s students at the University of Denver\n\n\n\nMickey Rafa\n\n\nMay 15, 2018\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-07-24-chicago-employee-query-comparison/index.html",
    "href": "posts/2024-07-24-chicago-employee-query-comparison/index.html",
    "title": "Query comparison: SQL, R, and Python using Chicago Employees dataset",
    "section": "",
    "text": "This project has three purposes:\n\nTo show how to run R, SQL, and Python all interchangeably in a Quarto document\nTo compare the ease of writing code using dplyr (R), SQL, and pandas (Python)\nTo include some demonstration of SQL in my portfolio (which is often not included but remains a critical skill)\n\n\n\n\nRequired packages:\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(httr)           #to fetch the data\nlibrary(janitor)        #for the clean_names() function for data cleaning\nlibrary(reticulate)     #to enable Python within R\nlibrary(DBI)            #to establish in-memory database of R dataframe\nlibrary(RSQLite)        #for SQLite engine\nlibrary(lubridate)      #for functions to handle dates\n\nAbout the dataset:\nThis dataset is from data.world, and includes information from Chicago’s Department of Human Resources for city employees in 2017. It’s a simple dataset to allow for comparisons across languages.\n\n# reading as a temporary file, then saving as df\nGET(\"https://query.data.world/s/hu5dkviuxd6k2ipuhpxjuyuds7aplu?dws=00000\", write_disk(tf &lt;- tempfile(fileext = \".xls\")))\n\nResponse [https://download.data.world/file_download/wbezchicago/chicago-employee-positions-and-salaries-for-2017/Employee%20Salary%20Data%20as%20of%20Sept.%202017.xls?auth=eyJhbGciOiJIUzUxMiJ9.eyJzdWIiOiJwcm9kLXVzZXItY2xpZW50Om1yYWZhMyIsImlzcyI6ImFnZW50Om1yYWZhMzo6ODg0MjQ2ZDItMTgzMy00NmZjLTk2YTMtZjQ2MWMzMDJjOTZiIiwiaWF0IjoxNzIxODc0NzgyLCJyb2xlIjpbInVzZXIiLCJ1c2VyX2FwaV9hZG1pbiIsInVzZXJfYXBpX2VudGVycHJpc2VfYWRtaW4iLCJ1c2VyX2FwaV9yZWFkIiwidXNlcl9hcGlfd3JpdGUiXSwiZ2VuZXJhbC1wdXJwb3NlIjpmYWxzZSwidXJsIjoiMjM2ZDZlZGQ3NjdkZmVmOGRjYzM0Mzg3YTExMDQ1N2EzMmU1OGY3ZSJ9.oTr9PF0rrsLdVhewmy2v1vRgYvT0jy_PBmqhmrufaRSB25PEm48ZPxvswiSrZw8bNaACxDWpxwBiJVCVSaM-Zg]\n  Date: 2024-07-26 20:16\n  Status: 200\n  Content-Type: application/vnd.ms-excel\n  Size: 4.72 MB\n&lt;ON DISK&gt;  /var/folders/ck/dmgn8lbx6vl8sl89vlhr0sv00000gn/T//Rtmp44XQRB/filed76165bf9508.xls\n\n\n\ndf &lt;- read_excel(tf) %&gt;% \n  #clean_names() to make all column names lowercase\n  clean_names()\n\n\n\nRows: 32,806\nColumns: 8\n$ name                           &lt;chr&gt; \"AARON,  JEFFERY M\", \"AARON,  KARINA\", …\n$ title                          &lt;chr&gt; \"SERGEANT\", \"POLICE OFFICER (ASSIGNED A…\n$ department                     &lt;chr&gt; \"POLICE\", \"POLICE\", \"FLEET AND FACILITY…\n$ salary_annual                  &lt;dbl&gt; 101442.0, 94122.0, 101592.0, 110064.0, …\n$ original_hire_date             &lt;dttm&gt; 2005-09-26, 2005-09-26, 1991-08-01, 19…\n$ start_date_in_present_position &lt;dttm&gt; 2016-06-01, 2017-04-16, 2000-05-01, 20…\n$ salary_basis                   &lt;chr&gt; \"SALARY\", \"SALARY\", \"SALARY\", \"SALARY\",…\n$ employment_category            &lt;chr&gt; \"Fulltime-Regular\", \"Fulltime-Regular\",…\n\n\n\n\n\nThe DBI:: package allows you to create an in-memory database to query against. The DBI project site is a great place to learn more about it. I’ll start by doing some initial setup and establishing the connection between the R dataframe and the SQL table name that I’ll query.\n\ncon &lt;- DBI::dbConnect(SQLite(), \":memory:\")\nDBI::dbWriteTable(conn = con, name = \"df\", value = df, field.types = c(\"original_hire_date\" = \"Date\"), row.names = FALSE)\n\nThe reticulate:: package allows for executing Python code in an R environment. The reticulate project site includes useful examples for getting up and running with Python in R. This package includes an r_to_py() function that is needed to convert an R dataframe into a pandas dataframe.\n\npy$df &lt;- r_to_py(df)\n\nWhen inserting a code chunk to your Markdown file, it originally defaults to ‘{r}’. You can simply change this to ‘python’ or ‘sql’ and, with the above set up, the code works beautifully in a Quarto document!\nIn the sections that follow, I used ChatGPT to generate prompts as querying exercises. For the initial code chunks,"
  },
  {
    "objectID": "posts/2024-07-24-chicago-employee-query-comparison/index.html#purpose",
    "href": "posts/2024-07-24-chicago-employee-query-comparison/index.html#purpose",
    "title": "Query comparison: SQL, R, and Python using Chicago Employees dataset",
    "section": "",
    "text": "This project has three purposes:\n\nTo show how to run R, SQL, and Python all interchangeably in a Quarto document\nTo compare the ease of writing code using dplyr (R), SQL, and pandas (Python)\nTo include some demonstration of SQL in my portfolio (which is often not included but remains a critical skill)"
  },
  {
    "objectID": "posts/2024-07-24-chicago-employee-query-comparison/index.html#setup-and-data-preparation",
    "href": "posts/2024-07-24-chicago-employee-query-comparison/index.html#setup-and-data-preparation",
    "title": "Query comparison: SQL, R, and Python using Chicago Employees dataset",
    "section": "",
    "text": "Required packages:\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(httr)           #to fetch the data\nlibrary(janitor)        #for the clean_names() function for data cleaning\nlibrary(reticulate)     #to enable Python within R\nlibrary(DBI)            #to establish in-memory database of R dataframe\nlibrary(RSQLite)        #for SQLite engine\nlibrary(lubridate)      #for functions to handle dates\n\nAbout the dataset:\nThis dataset is from data.world, and includes information from Chicago’s Department of Human Resources for city employees in 2017. It’s a simple dataset to allow for comparisons across languages.\n\n# reading as a temporary file, then saving as df\nGET(\"https://query.data.world/s/hu5dkviuxd6k2ipuhpxjuyuds7aplu?dws=00000\", write_disk(tf &lt;- tempfile(fileext = \".xls\")))\n\nResponse [https://download.data.world/file_download/wbezchicago/chicago-employee-positions-and-salaries-for-2017/Employee%20Salary%20Data%20as%20of%20Sept.%202017.xls?auth=eyJhbGciOiJIUzUxMiJ9.eyJzdWIiOiJwcm9kLXVzZXItY2xpZW50Om1yYWZhMyIsImlzcyI6ImFnZW50Om1yYWZhMzo6ODg0MjQ2ZDItMTgzMy00NmZjLTk2YTMtZjQ2MWMzMDJjOTZiIiwiaWF0IjoxNzIxODc0NzgyLCJyb2xlIjpbInVzZXIiLCJ1c2VyX2FwaV9hZG1pbiIsInVzZXJfYXBpX2VudGVycHJpc2VfYWRtaW4iLCJ1c2VyX2FwaV9yZWFkIiwidXNlcl9hcGlfd3JpdGUiXSwiZ2VuZXJhbC1wdXJwb3NlIjpmYWxzZSwidXJsIjoiMjM2ZDZlZGQ3NjdkZmVmOGRjYzM0Mzg3YTExMDQ1N2EzMmU1OGY3ZSJ9.oTr9PF0rrsLdVhewmy2v1vRgYvT0jy_PBmqhmrufaRSB25PEm48ZPxvswiSrZw8bNaACxDWpxwBiJVCVSaM-Zg]\n  Date: 2024-07-26 20:16\n  Status: 200\n  Content-Type: application/vnd.ms-excel\n  Size: 4.72 MB\n&lt;ON DISK&gt;  /var/folders/ck/dmgn8lbx6vl8sl89vlhr0sv00000gn/T//Rtmp44XQRB/filed76165bf9508.xls\n\n\n\ndf &lt;- read_excel(tf) %&gt;% \n  #clean_names() to make all column names lowercase\n  clean_names()\n\n\n\nRows: 32,806\nColumns: 8\n$ name                           &lt;chr&gt; \"AARON,  JEFFERY M\", \"AARON,  KARINA\", …\n$ title                          &lt;chr&gt; \"SERGEANT\", \"POLICE OFFICER (ASSIGNED A…\n$ department                     &lt;chr&gt; \"POLICE\", \"POLICE\", \"FLEET AND FACILITY…\n$ salary_annual                  &lt;dbl&gt; 101442.0, 94122.0, 101592.0, 110064.0, …\n$ original_hire_date             &lt;dttm&gt; 2005-09-26, 2005-09-26, 1991-08-01, 19…\n$ start_date_in_present_position &lt;dttm&gt; 2016-06-01, 2017-04-16, 2000-05-01, 20…\n$ salary_basis                   &lt;chr&gt; \"SALARY\", \"SALARY\", \"SALARY\", \"SALARY\",…\n$ employment_category            &lt;chr&gt; \"Fulltime-Regular\", \"Fulltime-Regular\",…"
  },
  {
    "objectID": "posts/2024-07-24-chicago-employee-query-comparison/index.html#setting-up-python-and-sql-to-execute",
    "href": "posts/2024-07-24-chicago-employee-query-comparison/index.html#setting-up-python-and-sql-to-execute",
    "title": "Query comparison: SQL, R, and Python using Chicago Employees dataset",
    "section": "",
    "text": "The DBI:: package allows you to create an in-memory database to query against. The DBI project site is a great place to learn more about it. I’ll start by doing some initial setup and establishing the connection between the R dataframe and the SQL table name that I’ll query.\n\ncon &lt;- DBI::dbConnect(SQLite(), \":memory:\")\nDBI::dbWriteTable(conn = con, name = \"df\", value = df, field.types = c(\"original_hire_date\" = \"Date\"), row.names = FALSE)\n\nThe reticulate:: package allows for executing Python code in an R environment. The reticulate project site includes useful examples for getting up and running with Python in R. This package includes an r_to_py() function that is needed to convert an R dataframe into a pandas dataframe.\n\npy$df &lt;- r_to_py(df)\n\nWhen inserting a code chunk to your Markdown file, it originally defaults to ‘{r}’. You can simply change this to ‘python’ or ‘sql’ and, with the above set up, the code works beautifully in a Quarto document!\nIn the sections that follow, I used ChatGPT to generate prompts as querying exercises. For the initial code chunks,"
  },
  {
    "objectID": "posts/2024-08-20-tidycensus-exploration-pt2/index.html",
    "href": "posts/2024-08-20-tidycensus-exploration-pt2/index.html",
    "title": "Part Two: Wrangling Census data for longitudinal analysis of child poverty",
    "section": "",
    "text": "In Part One, I demonstrated how to fetch data and do some basic analysis of U.S. Census data. Each API call with the tidycensus:: package can only be for one year of data, so to do longitudinal analysis requires some additional wrangling.\nIn this post, I’ll build a script that iterates through the available years, fetches the data, then combines the data into a single dataframe. Then, I’ll unpack some of the trends seen in child poverty in Colorado.\n\n\n\nlibrary(tidyverse)\nlibrary(tidycensus)\n1library(scales)\n2library(gt)\n3library(glue)\n\n4# census_api_key('INSERT KEY HERE', install = TRUE)\n\n\n1\n\nLoading the scales:: package to transform ggplot scales simply (some people choose to explicitly define scales:: in their code rather than loading the library).\n\n2\n\nThe gt:: library provides functionality for creating ggplot-esque tables.\n\n3\n\nThe glue:: package allows for simple addition of HTML to ggplot graphics.\n\n4\n\nThe first time that you’re working with the tidycensus:: package, you need to request an API key at https://api.census.gov/data/key_signup.html. The install= argument will install your personal key to the .Renviron file, and you won’t need to use the census_api_key() function again."
  },
  {
    "objectID": "posts/2024-08-20-tidycensus-exploration-pt2/index.html#setup",
    "href": "posts/2024-08-20-tidycensus-exploration-pt2/index.html#setup",
    "title": "Part Two: Wrangling Census data for longitudinal analysis of child poverty",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidycensus)\n1library(scales)\n2library(gt)\n3library(glue)\n\n4# census_api_key('INSERT KEY HERE', install = TRUE)\n\n\n1\n\nLoading the scales:: package to transform ggplot scales simply (some people choose to explicitly define scales:: in their code rather than loading the library).\n\n2\n\nThe gt:: library provides functionality for creating ggplot-esque tables.\n\n3\n\nThe glue:: package allows for simple addition of HTML to ggplot graphics.\n\n4\n\nThe first time that you’re working with the tidycensus:: package, you need to request an API key at https://api.census.gov/data/key_signup.html. The install= argument will install your personal key to the .Renviron file, and you won’t need to use the census_api_key() function again."
  },
  {
    "objectID": "posts/2024-08-20-tidycensus-exploration-pt2/index.html#fetching-from-the-tidycensus-api",
    "href": "posts/2024-08-20-tidycensus-exploration-pt2/index.html#fetching-from-the-tidycensus-api",
    "title": "Part Two: Wrangling Census data for longitudinal analysis of child poverty",
    "section": "Fetching from the tidycensus:: API",
    "text": "Fetching from the tidycensus:: API\n\n5years &lt;- seq(2005, 2022) %&gt;%\n6  setdiff(2020)\n\n\n5\n\nData for the ACS-1 product begins in 2005.\n\n6\n\nDue to the COVID-19 pandemic, the U.S. Census Bureau does not have standard ACS products available for 2020. The setdiff() function removes 2020 from the vector.\n\n\n\n\nNext, I’ll define a function to fetch the ACS data for each year in the vector.\n\nfetch_acs_data &lt;- function(year) {\n  get_acs(geography = \"county\", \n          state = \"Colorado\",\n          survey = 'acs1',\n          variables = c(male_u5_pop = 'B01001_003', \n                        female_u5_pop = 'B01001_027', \n                        male_u5_poverty = 'B17001_004', \n                        female_u5_poverty = 'B17001_018'),\n          year = year,\n          output = 'wide') %&gt;% \n    mutate(year = year)\n}\n\nThen, I’ll use purrr::map_df() to apply each year to the fetch_acs_data() function that I created, which will result in a single dataframe of all years.\n\ncombined_acs_data &lt;- map_df(years, fetch_acs_data)"
  },
  {
    "objectID": "posts/2024-08-20-tidycensus-exploration-pt2/index.html#data-wrangling",
    "href": "posts/2024-08-20-tidycensus-exploration-pt2/index.html#data-wrangling",
    "title": "Part Two: Wrangling Census data for longitudinal analysis of child poverty",
    "section": "Data wrangling",
    "text": "Data wrangling\n\nCreating some fields to combine gender-based poverty estimates and calculate a percent of the child population measure\n\n\ncombined_acs_data &lt;- combined_acs_data %&gt;% \n  mutate(total_u5_popE = male_u5_popE + female_u5_popE,\n         total_u5_povertyE = male_u5_povertyE + female_u5_povertyE,\n         perc_u5_poverty = total_u5_povertyE / total_u5_popE)\n\n\nCreating a county field that cleans the county_state field to only include the county name\n\n\ncombined_acs_data &lt;- combined_acs_data %&gt;% \n  mutate(county = str_remove(NAME, \" County.*\")) %&gt;% \n  select(county, everything())\n\n\nReading in a table that maps Colorado’s counties to a region of the state\n\n\ncolorado_regions &lt;- read_csv('.//data/colorado_regions.csv', show_col_types = FALSE) %&gt;%\n  mutate(region = as.factor(region))\n\n\n(Finally) creating the combined_acs_data dataframe for longitudinal analysis\n\n\ncombined_acs_data &lt;- combined_acs_data %&gt;% \n  left_join(x=.,\n            y=colorado_regions,\n            by='county')"
  },
  {
    "objectID": "posts/2024-08-20-tidycensus-exploration-pt2/index.html#footnotes",
    "href": "posts/2024-08-20-tidycensus-exploration-pt2/index.html#footnotes",
    "title": "Part Two: Wrangling Census data for longitudinal analysis of child poverty",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nScreenshot taken from the ACS 2018 Handbook, found here: https://www.census.gov/content/dam/Census/library/publications/2018/acs/acs_general_handbook_2018_ch03.pdf.↩︎\nBy default the tidycensus:: API provides the margin of error at the 90% confidence interval, but you can change this with the moe_level argument within get_acs().↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "In progress"
  },
  {
    "objectID": "posts/2024-09-15-pandas-datareader/index.html",
    "href": "posts/2024-09-15-pandas-datareader/index.html",
    "title": "Analyzing CPI with Python and R",
    "section": "",
    "text": "The pandas-datareader package is a powerful tool for easily accessing financial and economic data through various APIs. In this post, we’ll explore how to use it to fetch data from FRED (Federal Reserve Economic Data), the World Bank, and Yahoo Finance for S&P500 data.\n\n\nFirst, import the packages:\n\n1import pandas as pd\nfrom matplotlib import pyplot as plt\nimport pandas_datareader as pdr\nfrom datetime import datetime\n\n\n1\n\nIf it’s your first time running code with these libraries, you’ll need to first use the pip install command. Since these are already installed for me locally, I can just import.\n\n\n\n\nNext, I’ll set a variable for the time frame that I’d like to use for this demonstration.\n\nstart = datetime(2016, 1, 1)\nend = datetime.now()"
  },
  {
    "objectID": "posts/2024-09-15-pandas-datareader/index.html#setup",
    "href": "posts/2024-09-15-pandas-datareader/index.html#setup",
    "title": "Analyzing CPI with Python and R",
    "section": "",
    "text": "First, import the packages:\n\n1import pandas as pd\nfrom matplotlib import pyplot as plt\nimport pandas_datareader as pdr\nfrom datetime import datetime\n\n\n1\n\nIf it’s your first time running code with these libraries, you’ll need to first use the pip install command. Since these are already installed for me locally, I can just import.\n\n\n\n\nNext, I’ll set a variable for the time frame that I’d like to use for this demonstration.\n\nstart = datetime(2016, 1, 1)\nend = datetime.now()"
  },
  {
    "objectID": "posts/2024-09-15-pandas-datareader/index.html#footnotes",
    "href": "posts/2024-09-15-pandas-datareader/index.html#footnotes",
    "title": "Analyzing CPI with Python and R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe Federal Reserve website is a great place to comb through to see all the available data (https://fred.stlouisfed.org/).↩︎"
  }
]