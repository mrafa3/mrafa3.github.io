[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Portfolio",
    "section": "",
    "text": "Creating great U.S. county maps with advanced ggplot styling\n\n\n\nR\n\n\ndata-viz\n\n\nmapping\n\n\ntidyverse\n\n\ncdcplaces\n\n\napi\n\n\n\nThis post shows how to make a beautiful U.S. map and create your own ggplot theme\n\n\n\nMickey Rafa\n\n\nOct 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMapping depression and food insecurity in the U.S. with the CDC PLACES project\n\n\n\nR\n\n\ndata-viz\n\n\nmapping\n\n\ntidyverse\n\n\ncdcplaces\n\n\napi\n\n\n\nThis post shows how to fetch data from the CDC and analyze and map it simply\n\n\n\nMickey Rafa\n\n\nSep 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing CPI with Python and R\n\n\n\nPython\n\n\npandas\n\n\napi\n\n\ndata-viz\n\n\nplotnine\n\n\nR\n\n\nQuarto\n\n\nregression\n\n\nmatplotlib\n\n\n\nAccessing, analyzing, and visualizing data from the Federal Reserve\n\n\n\nMickey Rafa\n\n\nSep 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart Three: Exploring even smaller geographies with tidycensus\n\n\n\nR\n\n\ntidyverse\n\n\ntidycensus\n\n\nmapping\n\n\napi\n\n\n\nThis post shows how to fetch data at smaller scales for analysis of Census data\n\n\n\nMickey Rafa\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart Two: Wrangling Census data for longitudinal analysis of child poverty\n\n\n\nR\n\n\ndata-viz\n\n\ntidyverse\n\n\ntidycensus\n\n\napi\n\n\njavascript\n\n\npurrr\n\n\n\nThis post shows how to fetch many years of data simply, unlocking longitudinal analysis of Census data\n\n\n\nMickey Rafa\n\n\nAug 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up an EPA air quality database using DuckDB\n\n\n\nR\n\n\ntidyverse\n\n\nduckdb\n\n\nenvironment\n\n\nsql\n\n\napi\n\n\n\nThis post uses the RAQSAPI package to access the EPA API and query the results with DuckDB\n\n\n\nMickey Rafa\n\n\nAug 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart One: Exploring child poverty data with the tidycensus R package\n\n\n\nR\n\n\ntidyverse\n\n\nmapping\n\n\ngt\n\n\ntidycensus\n\n\napi\n\n\n\nThis post uses the tidycensus package to access the Census API and visualize data on child poverty in the U.S.\n\n\n\nMickey Rafa\n\n\nAug 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring interactive mapping with mapboxgl\n\n\n\nR\n\n\ntidyverse\n\n\nmapping\n\n\n\nThis post explores some mapboxgl features\n\n\n\nMickey Rafa\n\n\nAug 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBrainTree SQL challenge\n\n\n\nSQL\n\n\nQuarto\n\n\n\nThis post works through the SQL challenges used by PayPal\n\n\n\nMickey Rafa\n\n\nJul 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuery comparison: SQL, R, and Python using Chicago Employees dataset\n\n\n\nR\n\n\nSQL\n\n\nPython\n\n\nQuarto\n\n\ngenerative-AI\n\n\n\nThis post demonstrates how to run these languages in Quarto\n\n\n\nMickey Rafa\n\n\nJul 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Science Workflows in Javascript: Part Three\n\n\n\nQuarto\n\n\nJavascript\n\n\ndata-viz\n\n\npca\n\n\nenvironment\n\n\n\nThis post is from the third session led by Observable HQ\n\n\n\nMickey Rafa\n\n\nJul 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Science Workflows in Javascript: Part Two\n\n\n\nQuarto\n\n\nJavascript\n\n\ndata-viz\n\n\ncluster-analysis\n\n\nenvironment\n\n\n\nThis post is from the second session led by Observable HQ\n\n\n\nMickey Rafa\n\n\nJul 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Science Workflows in Javascript: Part One\n\n\n\nQuarto\n\n\nJavascript\n\n\ndata-viz\n\n\nregression\n\n\nenvironment\n\n\n\nThis post is from the first session led by Observable HQ\n\n\n\nMickey Rafa\n\n\nJul 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding beautiful tables with the gt:: package in R\n\n\n\nR\n\n\nweb-scraping\n\n\ngt\n\n\nsports\n\n\n\nThis post outlines how to quickly scrape and clean data from Wikipedia and build a beautiful table in R\n\n\n\nMickey Rafa\n\n\nDec 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPART TWO: Mapping of African Health Clusters\n\n\n\nR\n\n\ndata-viz\n\n\ntidyverse\n\n\ninternational-development\n\n\ncluster-analysis\n\n\nmapping\n\n\n\nCreating maps of cluster results using ggplot\n\n\n\nMickey Rafa\n\n\nJun 18, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPART ONE: Cluster Analysis of African Health Outcomes\n\n\n\nR\n\n\ndata-viz\n\n\ntidyverse\n\n\ninternational-development\n\n\ncluster-analysis\n\n\n\nExploring how African countries cluster based on their health outcomes\n\n\n\nMickey Rafa\n\n\nJun 18, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExperimenting with distance as a concept for SDG achievement\n\n\n\nR\n\n\ndata-viz\n\n\ntidyverse\n\n\ninternational-development\n\n\ncluster-analysis\n\n\nmapping\n\n\n\nExploring how to conceptualize and visualize distance from Sustainable Development Goal targets\n\n\n\nMickey Rafa\n\n\nApr 15, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nAnimating the Gapminder dataset using gganimate\n\n\n\nR\n\n\ndata-viz\n\n\ntidyverse\n\n\ngganimate\n\n\n\nThis post outlines how to easily add animation to ggplot graphics\n\n\n\nMickey Rafa\n\n\nOct 15, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Gapminder data using nested models in R\n\n\n\nR\n\n\nregression\n\n\ndata-viz\n\n\ntidyverse\n\n\ninternational-development\n\n\n\nThis post outlines the value in building nested models using the purrr and broom packages\n\n\n\nMickey Rafa\n\n\nOct 1, 2017\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mickey Rafa",
    "section": "",
    "text": "My name is Mickey – welcome to my portfolio website! I’m an analytics leader with nearly a decade of experience in high-growth environments, in both non-profit and for-profit organizations. I specialize in turning data into insights, improving the accessibility of dashboards and reports, and creating compelling data stories.\nOn this site, I include a selection of data science and viz projects that showcase the breadth of work that I’ve completed.\nLet’s connect! Feel free to reach out to me on LinkedIn or send me an email.\n\nEducation\n\nMS in Business Analytics | University of Denver\nMA in International Studies | University of Denver\nBA in Political Science | West Virginia University\n\nWork Experience\n\nSr. Manager, Employer Analytics | Guild Education\nAssistant Director of Research | Pardee Center for International Futures, University of Denver\n\nPeer-Reviewed Publications\n\nMoyer, J., Matthews, A., Rafa, M., Xiong, Y. “Identifying Patterns in the Structural Drivers of Intrastate Conflict.” British Journal of Political Science. (2022).\nWang, X., Rafa, M., Moyer, J., Li, J., Scheer, J., Sutton, P. “Estimation and Mapping of Sub-National GDP in Uganda Using NPP-VIIRS Imagery.” Remote Sensing. (2019)."
  },
  {
    "objectID": "posts/2024-07-24-chicago-employee-query-comparison/index.html",
    "href": "posts/2024-07-24-chicago-employee-query-comparison/index.html",
    "title": "Query comparison: SQL, R, and Python using Chicago Employees dataset",
    "section": "",
    "text": "This project has three purposes:\n\nTo show how to run R, SQL, and Python all interchangeably in a Quarto document\nTo compare the ease of writing code using dplyr (R), SQL, and pandas (Python)\nTo include some demonstration of SQL in my portfolio (which is often not included but remains a critical skill)\n\n\n\n\nRequired packages:\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(httr)           #to fetch the data\nlibrary(janitor)        #for the clean_names() function for data cleaning\nlibrary(reticulate)     #to enable Python within R\nlibrary(DBI)            #to establish in-memory database of R dataframe\nlibrary(RSQLite)        #for SQLite engine\nlibrary(lubridate)      #for functions to handle dates\n\nAbout the dataset:\nThis dataset is from data.world, and includes information from Chicago’s Department of Human Resources for city employees in 2017. It’s a simple dataset to allow for comparisons across languages.\n\n# reading as a temporary file, then saving as df\nGET(\"https://query.data.world/s/hu5dkviuxd6k2ipuhpxjuyuds7aplu?dws=00000\", write_disk(tf &lt;- tempfile(fileext = \".xls\")))\n\nResponse [https://download.data.world/file_download/wbezchicago/chicago-employee-positions-and-salaries-for-2017/Employee%20Salary%20Data%20as%20of%20Sept.%202017.xls?auth=eyJhbGciOiJIUzUxMiJ9.eyJzdWIiOiJwcm9kLXVzZXItY2xpZW50Om1yYWZhMyIsImlzcyI6ImFnZW50Om1yYWZhMzo6ODg0MjQ2ZDItMTgzMy00NmZjLTk2YTMtZjQ2MWMzMDJjOTZiIiwiaWF0IjoxNzIxODc0NzgyLCJyb2xlIjpbInVzZXIiLCJ1c2VyX2FwaV9hZG1pbiIsInVzZXJfYXBpX2VudGVycHJpc2VfYWRtaW4iLCJ1c2VyX2FwaV9yZWFkIiwidXNlcl9hcGlfd3JpdGUiXSwiZ2VuZXJhbC1wdXJwb3NlIjpmYWxzZSwidXJsIjoiMjM2ZDZlZGQ3NjdkZmVmOGRjYzM0Mzg3YTExMDQ1N2EzMmU1OGY3ZSJ9.oTr9PF0rrsLdVhewmy2v1vRgYvT0jy_PBmqhmrufaRSB25PEm48ZPxvswiSrZw8bNaACxDWpxwBiJVCVSaM-Zg]\n  Date: 2024-07-26 20:16\n  Status: 200\n  Content-Type: application/vnd.ms-excel\n  Size: 4.72 MB\n&lt;ON DISK&gt;  /var/folders/ck/dmgn8lbx6vl8sl89vlhr0sv00000gn/T//Rtmp44XQRB/filed76165bf9508.xls\n\n\n\ndf &lt;- read_excel(tf) %&gt;% \n  #clean_names() to make all column names lowercase\n  clean_names()\n\n\n\nRows: 32,806\nColumns: 8\n$ name                           &lt;chr&gt; \"AARON,  JEFFERY M\", \"AARON,  KARINA\", …\n$ title                          &lt;chr&gt; \"SERGEANT\", \"POLICE OFFICER (ASSIGNED A…\n$ department                     &lt;chr&gt; \"POLICE\", \"POLICE\", \"FLEET AND FACILITY…\n$ salary_annual                  &lt;dbl&gt; 101442.0, 94122.0, 101592.0, 110064.0, …\n$ original_hire_date             &lt;dttm&gt; 2005-09-26, 2005-09-26, 1991-08-01, 19…\n$ start_date_in_present_position &lt;dttm&gt; 2016-06-01, 2017-04-16, 2000-05-01, 20…\n$ salary_basis                   &lt;chr&gt; \"SALARY\", \"SALARY\", \"SALARY\", \"SALARY\",…\n$ employment_category            &lt;chr&gt; \"Fulltime-Regular\", \"Fulltime-Regular\",…\n\n\n\n\n\nThe DBI:: package allows you to create an in-memory database to query against. The DBI project site is a great place to learn more about it. I’ll start by doing some initial setup and establishing the connection between the R dataframe and the SQL table name that I’ll query.\n\ncon &lt;- DBI::dbConnect(SQLite(), \":memory:\")\nDBI::dbWriteTable(conn = con, name = \"df\", value = df, field.types = c(\"original_hire_date\" = \"Date\"), row.names = FALSE)\n\nThe reticulate:: package allows for executing Python code in an R environment. The reticulate project site includes useful examples for getting up and running with Python in R. This package includes an r_to_py() function that is needed to convert an R dataframe into a pandas dataframe.\n\npy$df &lt;- r_to_py(df)\n\nWhen inserting a code chunk to your Markdown file, it originally defaults to ‘{r}’. You can simply change this to ‘python’ or ‘sql’ and, with the above set up, the code works beautifully in a Quarto document!\nIn the sections that follow, I used ChatGPT to generate prompts as querying exercises. For the initial code chunks,"
  },
  {
    "objectID": "posts/2024-07-24-chicago-employee-query-comparison/index.html#purpose",
    "href": "posts/2024-07-24-chicago-employee-query-comparison/index.html#purpose",
    "title": "Query comparison: SQL, R, and Python using Chicago Employees dataset",
    "section": "",
    "text": "This project has three purposes:\n\nTo show how to run R, SQL, and Python all interchangeably in a Quarto document\nTo compare the ease of writing code using dplyr (R), SQL, and pandas (Python)\nTo include some demonstration of SQL in my portfolio (which is often not included but remains a critical skill)"
  },
  {
    "objectID": "posts/2024-07-24-chicago-employee-query-comparison/index.html#setup-and-data-preparation",
    "href": "posts/2024-07-24-chicago-employee-query-comparison/index.html#setup-and-data-preparation",
    "title": "Query comparison: SQL, R, and Python using Chicago Employees dataset",
    "section": "",
    "text": "Required packages:\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(httr)           #to fetch the data\nlibrary(janitor)        #for the clean_names() function for data cleaning\nlibrary(reticulate)     #to enable Python within R\nlibrary(DBI)            #to establish in-memory database of R dataframe\nlibrary(RSQLite)        #for SQLite engine\nlibrary(lubridate)      #for functions to handle dates\n\nAbout the dataset:\nThis dataset is from data.world, and includes information from Chicago’s Department of Human Resources for city employees in 2017. It’s a simple dataset to allow for comparisons across languages.\n\n# reading as a temporary file, then saving as df\nGET(\"https://query.data.world/s/hu5dkviuxd6k2ipuhpxjuyuds7aplu?dws=00000\", write_disk(tf &lt;- tempfile(fileext = \".xls\")))\n\nResponse [https://download.data.world/file_download/wbezchicago/chicago-employee-positions-and-salaries-for-2017/Employee%20Salary%20Data%20as%20of%20Sept.%202017.xls?auth=eyJhbGciOiJIUzUxMiJ9.eyJzdWIiOiJwcm9kLXVzZXItY2xpZW50Om1yYWZhMyIsImlzcyI6ImFnZW50Om1yYWZhMzo6ODg0MjQ2ZDItMTgzMy00NmZjLTk2YTMtZjQ2MWMzMDJjOTZiIiwiaWF0IjoxNzIxODc0NzgyLCJyb2xlIjpbInVzZXIiLCJ1c2VyX2FwaV9hZG1pbiIsInVzZXJfYXBpX2VudGVycHJpc2VfYWRtaW4iLCJ1c2VyX2FwaV9yZWFkIiwidXNlcl9hcGlfd3JpdGUiXSwiZ2VuZXJhbC1wdXJwb3NlIjpmYWxzZSwidXJsIjoiMjM2ZDZlZGQ3NjdkZmVmOGRjYzM0Mzg3YTExMDQ1N2EzMmU1OGY3ZSJ9.oTr9PF0rrsLdVhewmy2v1vRgYvT0jy_PBmqhmrufaRSB25PEm48ZPxvswiSrZw8bNaACxDWpxwBiJVCVSaM-Zg]\n  Date: 2024-07-26 20:16\n  Status: 200\n  Content-Type: application/vnd.ms-excel\n  Size: 4.72 MB\n&lt;ON DISK&gt;  /var/folders/ck/dmgn8lbx6vl8sl89vlhr0sv00000gn/T//Rtmp44XQRB/filed76165bf9508.xls\n\n\n\ndf &lt;- read_excel(tf) %&gt;% \n  #clean_names() to make all column names lowercase\n  clean_names()\n\n\n\nRows: 32,806\nColumns: 8\n$ name                           &lt;chr&gt; \"AARON,  JEFFERY M\", \"AARON,  KARINA\", …\n$ title                          &lt;chr&gt; \"SERGEANT\", \"POLICE OFFICER (ASSIGNED A…\n$ department                     &lt;chr&gt; \"POLICE\", \"POLICE\", \"FLEET AND FACILITY…\n$ salary_annual                  &lt;dbl&gt; 101442.0, 94122.0, 101592.0, 110064.0, …\n$ original_hire_date             &lt;dttm&gt; 2005-09-26, 2005-09-26, 1991-08-01, 19…\n$ start_date_in_present_position &lt;dttm&gt; 2016-06-01, 2017-04-16, 2000-05-01, 20…\n$ salary_basis                   &lt;chr&gt; \"SALARY\", \"SALARY\", \"SALARY\", \"SALARY\",…\n$ employment_category            &lt;chr&gt; \"Fulltime-Regular\", \"Fulltime-Regular\",…"
  },
  {
    "objectID": "posts/2024-07-24-chicago-employee-query-comparison/index.html#setting-up-python-and-sql-to-execute",
    "href": "posts/2024-07-24-chicago-employee-query-comparison/index.html#setting-up-python-and-sql-to-execute",
    "title": "Query comparison: SQL, R, and Python using Chicago Employees dataset",
    "section": "",
    "text": "The DBI:: package allows you to create an in-memory database to query against. The DBI project site is a great place to learn more about it. I’ll start by doing some initial setup and establishing the connection between the R dataframe and the SQL table name that I’ll query.\n\ncon &lt;- DBI::dbConnect(SQLite(), \":memory:\")\nDBI::dbWriteTable(conn = con, name = \"df\", value = df, field.types = c(\"original_hire_date\" = \"Date\"), row.names = FALSE)\n\nThe reticulate:: package allows for executing Python code in an R environment. The reticulate project site includes useful examples for getting up and running with Python in R. This package includes an r_to_py() function that is needed to convert an R dataframe into a pandas dataframe.\n\npy$df &lt;- r_to_py(df)\n\nWhen inserting a code chunk to your Markdown file, it originally defaults to ‘{r}’. You can simply change this to ‘python’ or ‘sql’ and, with the above set up, the code works beautifully in a Quarto document!\nIn the sections that follow, I used ChatGPT to generate prompts as querying exercises. For the initial code chunks,"
  },
  {
    "objectID": "posts/2017-10-01-gapminder-nested-models/index.html",
    "href": "posts/2017-10-01-gapminder-nested-models/index.html",
    "title": "Analysis of Gapminder data using nested models in R",
    "section": "",
    "text": "Working on nesting models following the workflow from R for Data Science.\n\n\n        country        continent        year         lifeExp     \n Afghanistan:  12   Africa  :624   Min.   :1952   Min.   :23.60  \n Albania    :  12   Americas:300   1st Qu.:1966   1st Qu.:48.20  \n Algeria    :  12   Asia    :396   Median :1980   Median :60.71  \n Angola     :  12   Europe  :360   Mean   :1980   Mean   :59.47  \n Argentina  :  12   Oceania : 24   3rd Qu.:1993   3rd Qu.:70.85  \n Australia  :  12                  Max.   :2007   Max.   :82.60  \n (Other)    :1632                                                \n      pop              gdpPercap       \n Min.   :6.001e+04   Min.   :   241.2  \n 1st Qu.:2.794e+06   1st Qu.:  1202.1  \n Median :7.024e+06   Median :  3531.8  \n Mean   :2.960e+07   Mean   :  7215.3  \n 3rd Qu.:1.959e+07   3rd Qu.:  9325.5  \n Max.   :1.319e+09   Max.   :113523.1  \n                                       \n\n\n\n#create year_since_1950\ngapminder &lt;- gapminder %&gt;% \n  mutate(year_since_1950 = year - 1950)\n#group by data that you want to produce multiple models by\n#then nest to create data column of remaining columns\nnest_gapminder &lt;- gapminder %&gt;% \n  group_by(continent, country) %&gt;% \n  nest()\n\n#create function for model that you want to build\n#df will be the only parametre\ncontinent_year_model &lt;- function(df) {\n  lm(lifeExp ~ year_since_1950, data = df)\n}\n\n#take nested df and map function to the data column\n#model will keep nested model statistics by country/year\nnest_gapminder &lt;- nest_gapminder %&gt;% \n  mutate(model = map(data, continent_year_model),\n         glance = map(model, broom::glance))\n\nglance_gapminder_model &lt;- nest_gapminder %&gt;% \n  unnest(glance) %&gt;% \n  arrange(desc(adj.r.squared))\n\n\nglance_gapminder_model %&gt;% \n  ggplot(.) + \n  geom_jitter(aes(x=continent,\n                 y=r.squared,\n                 color=continent)) + \n  ggtitle('Distribution of model fit by continent',\n          subtitle = 'Each dot is a country model') + \n  labs(x='Continent',\n       y='R-squared',\n       caption='LifeExp ~ year') + \n  my.theme\n\n\n\n\n\n\n\n\n\nglance_gapminder_model %&gt;% \n  ggplot(.) + \n  geom_density_ridges(aes(x=r.squared, \n                          y=continent,\n                          fill=continent)) + \n  ggtitle('Distribution of model fits by continent',\n          subtitle = 'Time does not explain variance in African life expectancy as strongly as others') + \n  labs(x='R-squared',\n       y='Continent',\n       caption='LifeExp ~ year') + \n  my.theme\n\nPicking joint bandwidth of 0.0392\n\n\n\n\n\n\n\n\n\n\n#which countries have the poor fits?\nbad_fit &lt;- glance_gapminder_model %&gt;% filter(r.squared &lt; 0.4)\n\ngapminder %&gt;% \n  semi_join(bad_fit, by = \"country\") %&gt;% \n  ggplot(.) + \n    geom_line(aes(x=year, \n                  y=lifeExp, \n                  color=country), linewidth=1.1) + \n  ggtitle('Countries in which time does not adequately explain life expectancy',\n          subtitle = 'Visualizing countries with model fits less than .4') + \n  labs(x='Year',\n       y='Life expectancy',\n       caption='LifeExp ~ year') + \n  my.theme\n\n\n\n\n\n\n\n\nThis demonstrates that:\n\nThe 10 countries with the poorest model fits are all in Africa.\n\nThe fit seems to be due to a dramatic decline in life expectancy that begins around 1990. This is the HIV/AIDS epidemic.\nFrom R for Data Science (http://r4ds.had.co.nz/many-models.html#making-tidy-data-with-broom):\n\nMaking tidy data with broom:\nThe broom package provides three general tools for turning models into tidy data frames:\n\nbroom::glance(model) returns a row for each model. Each column gives a model summary: either a measure of model quality, or complexity, or a combination of the two.\n\nbroom::tidy(model) returns a row for each coefficient in the model. Each column gives information about the estimate or its variability.\n\nbroom::augment(model, data) returns a row for each row in data, adding extra values like residuals, and influence statistics."
  },
  {
    "objectID": "posts/2024-07-15-wikipedia-international-mens-soccer/index.html",
    "href": "posts/2024-07-15-wikipedia-international-mens-soccer/index.html",
    "title": "Building beautiful tables with the gt:: package in R",
    "section": "",
    "text": "In this project, I wanted to experiment with the gt:: package to create a beautiful table using R. This code will walk through scraping a table from Wikipedia of the top international goal scorers in men’s soccer history.\n\n\n\nRequired packages:\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(scales)\nlibrary(htmltools)\nlibrary(rvest)\nlibrary(gt)\nlibrary(countrycode)\n\nData.world has a dataset that consists of country names and URLs associated with .svg images of their national flags, which I’d like to include within the table for extra style.\n\nflag_db &lt;- read.csv(\"https://query.data.world/s/cnheo22w5mwowz2tfxrbb3z2i3mzya?dws=00000\", header=TRUE, stringsAsFactors=FALSE) %&gt;% \n  #Convert country names into 3-letter country codes\n  mutate(Code = countrycode(sourcevar = Country, origin = \"country.name\", destination = \"iso3c\", warn = FALSE)) %&gt;% \n  select(Country, flag_URL = ImageURL)\n\n\n\n\n\nurl_goals &lt;- 'https://en.wikipedia.org/wiki/List_of_men%27s_footballers_with_50_or_more_international_goals'\n\n\n(raw &lt;- url_goals %&gt;%\n  rvest::read_html() %&gt;%\n  rvest::html_nodes(xpath='//*[@id=\"mw-content-text\"]/div[1]/table[2]') %&gt;% \n  html_table() %&gt;% \n  .[[1]] %&gt;% \n  clean_names())\n\n# A tibble: 80 × 10\n    rank player      nation confederation goals caps  goalsper_match career_span\n   &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;          &lt;chr&gt;      \n 1     1 Cristiano … Portu… UEFA          130   212   0.61           2003–      \n 2     2 Lionel Mes… Argen… CONMEBOL      109   187   0.58           2005–      \n 3     3 Ali Daei    Iran   AFC           108[… 148[… 0.73           1993–2006  \n 4     4 Sunil Chhe… India  AFC           94    151[… 0.62           2005–2024  \n 5     5 Mokhtar Da… Malay… AFC           89    142   0.63           1972–1985  \n 6     6 Ali Mabkho… Unite… AFC           85    115   0.74           2009–      \n 7     6 Romelu Luk… Belgi… UEFA          85    119   0.71           2010–      \n 8     8 Ferenc Pus… Hunga… UEFA          84    89    0.94           1945–1962  \n 9     9 Robert Lew… Poland UEFA          83    152   0.55           2008–      \n10    10 Godfrey Ch… Zambia CAF           79    111   0.71           1968–1980  \n# ℹ 70 more rows\n# ℹ 2 more variables: date_of_50th_goal &lt;chr&gt;, ref &lt;chr&gt;\n\n\nFor data cleaning of this Wiki table, we need to: * Remove all footnotes within the table * Convert all chr columns to numeric values * Re-code one of the confederation values so that it’s clean for the by confederation table\n\n(raw &lt;- raw %&gt;% \n  #Remove extra spaces in nation to clean this column\n  mutate(nation = str_replace_all(str_trim(nation), \"\\\\s+\", \" \")) %&gt;%\n  mutate(nation = case_when(\n    str_detect(nation, \"^Hungary\\\\s+Spain$\") ~ \"Hungary\",\n    #re-coding this as United Kingdom to bring in flag\n    str_detect(nation, \"^England\\\\s+England\\\\s+amateurs$\") ~ \"United Kingdom\",\n    TRUE ~ nation\n  )) %&gt;%\n  mutate(confederation = case_when(\n    confederation == \"AFC / OFC[h]\" ~ \"AFC\",\n    TRUE ~ confederation)) %&gt;% \n  #remove footnotes and transform columns to numeric values\n  mutate_at(vars(player, career_span, goals, caps, goalsper_match), \n            ~ str_remove(., \"\\\\[.*\\\\]\")) %&gt;% \n  mutate_at(vars(goals, caps, goalsper_match), \n            as.numeric) %&gt;% \n  left_join(flag_db, by = c('nation' = 'Country')) %&gt;% \n  select(flag_URL, everything()))\n\n# A tibble: 80 × 11\n   flag_URL          rank player nation confederation goals  caps goalsper_match\n   &lt;chr&gt;            &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;\n 1 https://upload.…     1 Crist… Portu… UEFA            130   212           0.61\n 2 https://upload.…     2 Lione… Argen… CONMEBOL        109   187           0.58\n 3 https://upload.…     3 Ali D… Iran   AFC             108   148           0.73\n 4 https://upload.…     4 Sunil… India  AFC              94   151           0.62\n 5 https://upload.…     5 Mokht… Malay… AFC              89   142           0.63\n 6 https://upload.…     6 Ali M… Unite… AFC              85   115           0.74\n 7 https://upload.…     6 Romel… Belgi… UEFA             85   119           0.71\n 8 https://upload.…     8 Feren… Hunga… UEFA             84    89           0.94\n 9 https://upload.…     9 Rober… Poland UEFA             83   152           0.55\n10 https://upload.…    10 Godfr… Zambia CAF              79   111           0.71\n# ℹ 70 more rows\n# ℹ 3 more variables: career_span &lt;chr&gt;, date_of_50th_goal &lt;chr&gt;, ref &lt;chr&gt;\n\n\nNow that I have the data scraped and cleaned, I’m interested in making some tables using the ::gt() package. Tables are great for communicating summary information, so I’ll first build a graphic of the top 12 international goal scorers of all time.\n\ndf_top_scorers &lt;- raw %&gt;% \n  slice(1:12)\n\nThen, I’ll extract the min and max values from the table for conditional formatting of the table.\n\nmin_goals &lt;- df_top_scorers$goals %&gt;% min()\nmax_goals &lt;- df_top_scorers$goals %&gt;% max()\n\ngoals_palette &lt;- col_numeric(c(\"lightgreen\", \"darkgreen\"), \n                             domain = c(min_goals, max_goals), \n                             alpha = .75)\n\nFor the first table, I’ll put the columns and styling in place first. I’m going to build a table that includes the flag_url column – which is messy at first – and then I’ll use a function to render the flag graphics from those URLs. This blog post is excellent – it showed me how to do this step-by-step.\n\n(tbl_top_scorers &lt;- df_top_scorers %&gt;% \n  select(rank, player, career_span, flag_URL, nation, goals, caps, goalsper_match) %&gt;% \n  gt() %&gt;% \n  #rename columns\n  cols_label(rank = 'Rank',\n             player = 'Name',\n             career_span = 'Career Span',\n             nation = 'Country',\n             goals = 'Total Goals Scored',\n             caps = 'Matches',\n             goalsper_match = 'Goals per Match') %&gt;% \n  #add table title\n  tab_header(title = md(\"**Total Goals Scored in Men's International Soccer Matches**\")) %&gt;% \n  tab_source_note(source_note = \"Data from Wikipedia\") %&gt;% \n  #apply new style to all column headers\n  tab_style(\n    locations = cells_column_labels(columns = everything()),\n    style = list(\n      #thick border\n      cell_borders(sides = \"bottom\", weight = px(3)),\n      #make text bold\n      cell_text(weight = \"bold\")\n    )\n  ) %&gt;% \n  #apply different style to title\n  tab_style(locations = cells_title(groups = \"title\"),\n            style = list(\n              cell_text(weight = \"bold\", size = 24)\n            )) %&gt;% \n  data_color(columns = c(goals),\n             colors = goals_palette) %&gt;% \n  opt_all_caps() %&gt;% \n  opt_table_font(\n    font = list(\n      google_font(\"Chivo\"),\n      default_fonts()\n    )\n  ) %&gt;% \n  tab_options(\n    #remove border between column headers and title\n    column_labels.border.top.width = px(3),\n    column_labels.border.top.color = \"transparent\",\n    #remove border around the table\n    table.border.top.color = \"transparent\",\n    table.border.bottom.color = \"transparent\",\n    #adjust font sizes and alignment\n    source_notes.font.size = 12,\n    heading.align = \"left\"\n  ))\n\n\n\n\n\n\n\nTotal Goals Scored in Men’s International Soccer Matches\n\n\nRank\nName\nCareer Span\nflag_URL\nCountry\nTotal Goals Scored\nMatches\nGoals per Match\n\n\n\n\n1\nCristiano Ronaldo\n2003–\nhttps://upload.wikimedia.org/wikipedia/commons/5/5c/Flag_of_Portugal.svg\nPortugal\n130\n212\n0.61\n\n\n2\nLionel Messi\n2005–\nhttps://upload.wikimedia.org/wikipedia/commons/1/1a/Flag_of_Argentina.svg\nArgentina\n109\n187\n0.58\n\n\n3\nAli Daei\n1993–2006\nhttps://upload.wikimedia.org/wikipedia/commons/c/ca/Flag_of_Iran.svg\nIran\n108\n148\n0.73\n\n\n4\nSunil Chhetri\n2005–2024\nhttps://upload.wikimedia.org/wikipedia/en/4/41/Flag_of_India.svg\nIndia\n94\n151\n0.62\n\n\n5\nMokhtar Dahari\n1972–1985\nhttps://upload.wikimedia.org/wikipedia/commons/6/66/Flag_of_Malaysia.svg\nMalaysia\n89\n142\n0.63\n\n\n6\nAli Mabkhout\n2009–\nhttps://upload.wikimedia.org/wikipedia/commons/c/cb/Flag_of_the_United_Arab_Emirates.svg\nUnited Arab Emirates\n85\n115\n0.74\n\n\n6\nRomelu Lukaku\n2010–\nhttps://upload.wikimedia.org/wikipedia/commons/9/92/Flag_of_Belgium_%28civil%29.svg\nBelgium\n85\n119\n0.71\n\n\n8\nFerenc Puskás\n1945–1962\nhttps://upload.wikimedia.org/wikipedia/commons/c/c1/Flag_of_Hungary.svg\nHungary\n84\n89\n0.94\n\n\n9\nRobert Lewandowski\n2008–\nhttps://upload.wikimedia.org/wikipedia/en/1/12/Flag_of_Poland.svg\nPoland\n83\n152\n0.55\n\n\n10\nGodfrey Chitalu\n1968–1980\nhttps://upload.wikimedia.org/wikipedia/commons/0/06/Flag_of_Zambia.svg\nZambia\n79\n111\n0.71\n\n\n10\nNeymar\n2010–\nhttps://upload.wikimedia.org/wikipedia/en/0/05/Flag_of_Brazil.svg\nBrazil\n79\n128\n0.62\n\n\n12\nHussein Saeed\n1977–1990\nhttps://upload.wikimedia.org/wikipedia/commons/f/f6/Flag_of_Iraq.svg\nIraq\n78\n137\n0.57\n\n\n\nData from Wikipedia\n\n\n\n\n\n\n\n\n\n(tbl_top_scorers &lt;- tbl_top_scorers %&gt;% \n    text_transform(\n    #Apply a function to a column\n    locations = cells_body(c(flag_URL)),\n    fn = function(x) {\n      #Return an image of set dimensions\n      web_image(\n        url = x,\n        height = 12\n      )\n    }\n  ) %&gt;% \n  #Hide column header flag_URL and reduce width\n  cols_width(c(flag_URL) ~ px(30)) %&gt;% \n  cols_label(flag_URL = \"\"))\n\n\n\n\n\n\n\nTotal Goals Scored in Men’s International Soccer Matches\n\n\nRank\nName\nCareer Span\n\nCountry\nTotal Goals Scored\nMatches\nGoals per Match\n\n\n\n\n1\nCristiano Ronaldo\n2003–\n\nPortugal\n130\n212\n0.61\n\n\n2\nLionel Messi\n2005–\n\nArgentina\n109\n187\n0.58\n\n\n3\nAli Daei\n1993–2006\n\nIran\n108\n148\n0.73\n\n\n4\nSunil Chhetri\n2005–2024\n\nIndia\n94\n151\n0.62\n\n\n5\nMokhtar Dahari\n1972–1985\n\nMalaysia\n89\n142\n0.63\n\n\n6\nAli Mabkhout\n2009–\n\nUnited Arab Emirates\n85\n115\n0.74\n\n\n6\nRomelu Lukaku\n2010–\n\nBelgium\n85\n119\n0.71\n\n\n8\nFerenc Puskás\n1945–1962\n\nHungary\n84\n89\n0.94\n\n\n9\nRobert Lewandowski\n2008–\n\nPoland\n83\n152\n0.55\n\n\n10\nGodfrey Chitalu\n1968–1980\n\nZambia\n79\n111\n0.71\n\n\n10\nNeymar\n2010–\n\nBrazil\n79\n128\n0.62\n\n\n12\nHussein Saeed\n1977–1990\n\nIraq\n78\n137\n0.57\n\n\n\nData from Wikipedia"
  },
  {
    "objectID": "posts/2024-07-15-wikipedia-international-mens-soccer/index.html#purpose",
    "href": "posts/2024-07-15-wikipedia-international-mens-soccer/index.html#purpose",
    "title": "Building beautiful tables with the gt:: package in R",
    "section": "",
    "text": "In this project, I wanted to experiment with the gt:: package to create a beautiful table using R. This code will walk through scraping a table from Wikipedia of the top international goal scorers in men’s soccer history."
  },
  {
    "objectID": "posts/2024-07-15-wikipedia-international-mens-soccer/index.html#setup-and-data-preparation",
    "href": "posts/2024-07-15-wikipedia-international-mens-soccer/index.html#setup-and-data-preparation",
    "title": "Building beautiful tables with the gt:: package in R",
    "section": "",
    "text": "Required packages:\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(scales)\nlibrary(htmltools)\nlibrary(rvest)\nlibrary(gt)\nlibrary(countrycode)\n\nData.world has a dataset that consists of country names and URLs associated with .svg images of their national flags, which I’d like to include within the table for extra style.\n\nflag_db &lt;- read.csv(\"https://query.data.world/s/cnheo22w5mwowz2tfxrbb3z2i3mzya?dws=00000\", header=TRUE, stringsAsFactors=FALSE) %&gt;% \n  #Convert country names into 3-letter country codes\n  mutate(Code = countrycode(sourcevar = Country, origin = \"country.name\", destination = \"iso3c\", warn = FALSE)) %&gt;% \n  select(Country, flag_URL = ImageURL)"
  },
  {
    "objectID": "posts/2024-07-15-wikipedia-international-mens-soccer/index.html#scrape-wikipedia-data",
    "href": "posts/2024-07-15-wikipedia-international-mens-soccer/index.html#scrape-wikipedia-data",
    "title": "Building beautiful tables with the gt:: package in R",
    "section": "",
    "text": "url_goals &lt;- 'https://en.wikipedia.org/wiki/List_of_men%27s_footballers_with_50_or_more_international_goals'\n\n\n(raw &lt;- url_goals %&gt;%\n  rvest::read_html() %&gt;%\n  rvest::html_nodes(xpath='//*[@id=\"mw-content-text\"]/div[1]/table[2]') %&gt;% \n  html_table() %&gt;% \n  .[[1]] %&gt;% \n  clean_names())\n\n# A tibble: 80 × 10\n    rank player      nation confederation goals caps  goalsper_match career_span\n   &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;          &lt;chr&gt;      \n 1     1 Cristiano … Portu… UEFA          130   212   0.61           2003–      \n 2     2 Lionel Mes… Argen… CONMEBOL      109   187   0.58           2005–      \n 3     3 Ali Daei    Iran   AFC           108[… 148[… 0.73           1993–2006  \n 4     4 Sunil Chhe… India  AFC           94    151[… 0.62           2005–2024  \n 5     5 Mokhtar Da… Malay… AFC           89    142   0.63           1972–1985  \n 6     6 Ali Mabkho… Unite… AFC           85    115   0.74           2009–      \n 7     6 Romelu Luk… Belgi… UEFA          85    119   0.71           2010–      \n 8     8 Ferenc Pus… Hunga… UEFA          84    89    0.94           1945–1962  \n 9     9 Robert Lew… Poland UEFA          83    152   0.55           2008–      \n10    10 Godfrey Ch… Zambia CAF           79    111   0.71           1968–1980  \n# ℹ 70 more rows\n# ℹ 2 more variables: date_of_50th_goal &lt;chr&gt;, ref &lt;chr&gt;\n\n\nFor data cleaning of this Wiki table, we need to: * Remove all footnotes within the table * Convert all chr columns to numeric values * Re-code one of the confederation values so that it’s clean for the by confederation table\n\n(raw &lt;- raw %&gt;% \n  #Remove extra spaces in nation to clean this column\n  mutate(nation = str_replace_all(str_trim(nation), \"\\\\s+\", \" \")) %&gt;%\n  mutate(nation = case_when(\n    str_detect(nation, \"^Hungary\\\\s+Spain$\") ~ \"Hungary\",\n    #re-coding this as United Kingdom to bring in flag\n    str_detect(nation, \"^England\\\\s+England\\\\s+amateurs$\") ~ \"United Kingdom\",\n    TRUE ~ nation\n  )) %&gt;%\n  mutate(confederation = case_when(\n    confederation == \"AFC / OFC[h]\" ~ \"AFC\",\n    TRUE ~ confederation)) %&gt;% \n  #remove footnotes and transform columns to numeric values\n  mutate_at(vars(player, career_span, goals, caps, goalsper_match), \n            ~ str_remove(., \"\\\\[.*\\\\]\")) %&gt;% \n  mutate_at(vars(goals, caps, goalsper_match), \n            as.numeric) %&gt;% \n  left_join(flag_db, by = c('nation' = 'Country')) %&gt;% \n  select(flag_URL, everything()))\n\n# A tibble: 80 × 11\n   flag_URL          rank player nation confederation goals  caps goalsper_match\n   &lt;chr&gt;            &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;\n 1 https://upload.…     1 Crist… Portu… UEFA            130   212           0.61\n 2 https://upload.…     2 Lione… Argen… CONMEBOL        109   187           0.58\n 3 https://upload.…     3 Ali D… Iran   AFC             108   148           0.73\n 4 https://upload.…     4 Sunil… India  AFC              94   151           0.62\n 5 https://upload.…     5 Mokht… Malay… AFC              89   142           0.63\n 6 https://upload.…     6 Ali M… Unite… AFC              85   115           0.74\n 7 https://upload.…     6 Romel… Belgi… UEFA             85   119           0.71\n 8 https://upload.…     8 Feren… Hunga… UEFA             84    89           0.94\n 9 https://upload.…     9 Rober… Poland UEFA             83   152           0.55\n10 https://upload.…    10 Godfr… Zambia CAF              79   111           0.71\n# ℹ 70 more rows\n# ℹ 3 more variables: career_span &lt;chr&gt;, date_of_50th_goal &lt;chr&gt;, ref &lt;chr&gt;\n\n\nNow that I have the data scraped and cleaned, I’m interested in making some tables using the ::gt() package. Tables are great for communicating summary information, so I’ll first build a graphic of the top 12 international goal scorers of all time.\n\ndf_top_scorers &lt;- raw %&gt;% \n  slice(1:12)\n\nThen, I’ll extract the min and max values from the table for conditional formatting of the table.\n\nmin_goals &lt;- df_top_scorers$goals %&gt;% min()\nmax_goals &lt;- df_top_scorers$goals %&gt;% max()\n\ngoals_palette &lt;- col_numeric(c(\"lightgreen\", \"darkgreen\"), \n                             domain = c(min_goals, max_goals), \n                             alpha = .75)\n\nFor the first table, I’ll put the columns and styling in place first. I’m going to build a table that includes the flag_url column – which is messy at first – and then I’ll use a function to render the flag graphics from those URLs. This blog post is excellent – it showed me how to do this step-by-step.\n\n(tbl_top_scorers &lt;- df_top_scorers %&gt;% \n  select(rank, player, career_span, flag_URL, nation, goals, caps, goalsper_match) %&gt;% \n  gt() %&gt;% \n  #rename columns\n  cols_label(rank = 'Rank',\n             player = 'Name',\n             career_span = 'Career Span',\n             nation = 'Country',\n             goals = 'Total Goals Scored',\n             caps = 'Matches',\n             goalsper_match = 'Goals per Match') %&gt;% \n  #add table title\n  tab_header(title = md(\"**Total Goals Scored in Men's International Soccer Matches**\")) %&gt;% \n  tab_source_note(source_note = \"Data from Wikipedia\") %&gt;% \n  #apply new style to all column headers\n  tab_style(\n    locations = cells_column_labels(columns = everything()),\n    style = list(\n      #thick border\n      cell_borders(sides = \"bottom\", weight = px(3)),\n      #make text bold\n      cell_text(weight = \"bold\")\n    )\n  ) %&gt;% \n  #apply different style to title\n  tab_style(locations = cells_title(groups = \"title\"),\n            style = list(\n              cell_text(weight = \"bold\", size = 24)\n            )) %&gt;% \n  data_color(columns = c(goals),\n             colors = goals_palette) %&gt;% \n  opt_all_caps() %&gt;% \n  opt_table_font(\n    font = list(\n      google_font(\"Chivo\"),\n      default_fonts()\n    )\n  ) %&gt;% \n  tab_options(\n    #remove border between column headers and title\n    column_labels.border.top.width = px(3),\n    column_labels.border.top.color = \"transparent\",\n    #remove border around the table\n    table.border.top.color = \"transparent\",\n    table.border.bottom.color = \"transparent\",\n    #adjust font sizes and alignment\n    source_notes.font.size = 12,\n    heading.align = \"left\"\n  ))\n\n\n\n\n\n\n\nTotal Goals Scored in Men’s International Soccer Matches\n\n\nRank\nName\nCareer Span\nflag_URL\nCountry\nTotal Goals Scored\nMatches\nGoals per Match\n\n\n\n\n1\nCristiano Ronaldo\n2003–\nhttps://upload.wikimedia.org/wikipedia/commons/5/5c/Flag_of_Portugal.svg\nPortugal\n130\n212\n0.61\n\n\n2\nLionel Messi\n2005–\nhttps://upload.wikimedia.org/wikipedia/commons/1/1a/Flag_of_Argentina.svg\nArgentina\n109\n187\n0.58\n\n\n3\nAli Daei\n1993–2006\nhttps://upload.wikimedia.org/wikipedia/commons/c/ca/Flag_of_Iran.svg\nIran\n108\n148\n0.73\n\n\n4\nSunil Chhetri\n2005–2024\nhttps://upload.wikimedia.org/wikipedia/en/4/41/Flag_of_India.svg\nIndia\n94\n151\n0.62\n\n\n5\nMokhtar Dahari\n1972–1985\nhttps://upload.wikimedia.org/wikipedia/commons/6/66/Flag_of_Malaysia.svg\nMalaysia\n89\n142\n0.63\n\n\n6\nAli Mabkhout\n2009–\nhttps://upload.wikimedia.org/wikipedia/commons/c/cb/Flag_of_the_United_Arab_Emirates.svg\nUnited Arab Emirates\n85\n115\n0.74\n\n\n6\nRomelu Lukaku\n2010–\nhttps://upload.wikimedia.org/wikipedia/commons/9/92/Flag_of_Belgium_%28civil%29.svg\nBelgium\n85\n119\n0.71\n\n\n8\nFerenc Puskás\n1945–1962\nhttps://upload.wikimedia.org/wikipedia/commons/c/c1/Flag_of_Hungary.svg\nHungary\n84\n89\n0.94\n\n\n9\nRobert Lewandowski\n2008–\nhttps://upload.wikimedia.org/wikipedia/en/1/12/Flag_of_Poland.svg\nPoland\n83\n152\n0.55\n\n\n10\nGodfrey Chitalu\n1968–1980\nhttps://upload.wikimedia.org/wikipedia/commons/0/06/Flag_of_Zambia.svg\nZambia\n79\n111\n0.71\n\n\n10\nNeymar\n2010–\nhttps://upload.wikimedia.org/wikipedia/en/0/05/Flag_of_Brazil.svg\nBrazil\n79\n128\n0.62\n\n\n12\nHussein Saeed\n1977–1990\nhttps://upload.wikimedia.org/wikipedia/commons/f/f6/Flag_of_Iraq.svg\nIraq\n78\n137\n0.57\n\n\n\nData from Wikipedia\n\n\n\n\n\n\n\n\n\n(tbl_top_scorers &lt;- tbl_top_scorers %&gt;% \n    text_transform(\n    #Apply a function to a column\n    locations = cells_body(c(flag_URL)),\n    fn = function(x) {\n      #Return an image of set dimensions\n      web_image(\n        url = x,\n        height = 12\n      )\n    }\n  ) %&gt;% \n  #Hide column header flag_URL and reduce width\n  cols_width(c(flag_URL) ~ px(30)) %&gt;% \n  cols_label(flag_URL = \"\"))\n\n\n\n\n\n\n\nTotal Goals Scored in Men’s International Soccer Matches\n\n\nRank\nName\nCareer Span\n\nCountry\nTotal Goals Scored\nMatches\nGoals per Match\n\n\n\n\n1\nCristiano Ronaldo\n2003–\n\nPortugal\n130\n212\n0.61\n\n\n2\nLionel Messi\n2005–\n\nArgentina\n109\n187\n0.58\n\n\n3\nAli Daei\n1993–2006\n\nIran\n108\n148\n0.73\n\n\n4\nSunil Chhetri\n2005–2024\n\nIndia\n94\n151\n0.62\n\n\n5\nMokhtar Dahari\n1972–1985\n\nMalaysia\n89\n142\n0.63\n\n\n6\nAli Mabkhout\n2009–\n\nUnited Arab Emirates\n85\n115\n0.74\n\n\n6\nRomelu Lukaku\n2010–\n\nBelgium\n85\n119\n0.71\n\n\n8\nFerenc Puskás\n1945–1962\n\nHungary\n84\n89\n0.94\n\n\n9\nRobert Lewandowski\n2008–\n\nPoland\n83\n152\n0.55\n\n\n10\nGodfrey Chitalu\n1968–1980\n\nZambia\n79\n111\n0.71\n\n\n10\nNeymar\n2010–\n\nBrazil\n79\n128\n0.62\n\n\n12\nHussein Saeed\n1977–1990\n\nIraq\n78\n137\n0.57\n\n\n\nData from Wikipedia"
  },
  {
    "objectID": "posts/2019-04-15-distance-sustainable-development-health/index.html",
    "href": "posts/2019-04-15-distance-sustainable-development-health/index.html",
    "title": "Experimenting with distance as a concept for SDG achievement",
    "section": "",
    "text": "The United Nations created the Sustainable Development Goals (SDGs) to set an ambitious global development agenda to work toward by 2030. A continental development organization in Africa asked, how could we think about SDG 3 (the health goals) in a holistic and aggregate sense?\nIn this analysis, I explore measuring a country’s distance to target achievement. I build a composite score of standardized distance to SDG 3 achievement, and find that Nigeria is furthest from achieving SDG 3 of all African countries.\n\n\n\nThis analysis uses results from the International Futures global forecasting model and its Current Path scenario. The results span many of the measurable targets of SDG3, including:\n\nMaternal mortality ratio\nNeonatal mortality ratio\nUnder-5 mortality rate\nNon-communicable disease death rate\nTraffic accident death rate\nAIDS death rate\nMalaria death rate"
  },
  {
    "objectID": "posts/2019-04-15-distance-sustainable-development-health/index.html#purpose",
    "href": "posts/2019-04-15-distance-sustainable-development-health/index.html#purpose",
    "title": "Experimenting with distance as a concept for SDG achievement",
    "section": "",
    "text": "The United Nations created the Sustainable Development Goals (SDGs) to set an ambitious global development agenda to work toward by 2030. A continental development organization in Africa asked, how could we think about SDG 3 (the health goals) in a holistic and aggregate sense?\nIn this analysis, I explore measuring a country’s distance to target achievement. I build a composite score of standardized distance to SDG 3 achievement, and find that Nigeria is furthest from achieving SDG 3 of all African countries."
  },
  {
    "objectID": "posts/2019-04-15-distance-sustainable-development-health/index.html#dataset",
    "href": "posts/2019-04-15-distance-sustainable-development-health/index.html#dataset",
    "title": "Experimenting with distance as a concept for SDG achievement",
    "section": "",
    "text": "This analysis uses results from the International Futures global forecasting model and its Current Path scenario. The results span many of the measurable targets of SDG3, including:\n\nMaternal mortality ratio\nNeonatal mortality ratio\nUnder-5 mortality rate\nNon-communicable disease death rate\nTraffic accident death rate\nAIDS death rate\nMalaria death rate"
  },
  {
    "objectID": "posts/2019-04-15-distance-sustainable-development-health/index.html#setting-order-by-distance-from-achievement",
    "href": "posts/2019-04-15-distance-sustainable-development-health/index.html#setting-order-by-distance-from-achievement",
    "title": "Experimenting with distance as a concept for SDG achievement",
    "section": "Setting order by distance from achievement",
    "text": "Setting order by distance from achievement\n\n\n# A tibble: 54 × 2\n   country               avg_val\n   &lt;chr&gt;                   &lt;dbl&gt;\n 1 Libya                  -0.703\n 2 Sao Tome and Principe  -0.677\n 3 Cape Verde             -0.661\n 4 Seychelles             -0.635\n 5 Egypt                  -0.630\n 6 Tunisia                -0.552\n 7 Senegal                -0.453\n 8 Niger                  -0.387\n 9 Eritrea                -0.373\n10 Algeria                -0.363\n# ℹ 44 more rows\n\n\n\n(dist_plot_df &lt;- df_scaled_dist_results %&gt;% \n  filter(country != 'TARGET') %&gt;% \n  left_join(x=.,\n            y=df_scaled_dist_results_rel_targets,\n            by='country') %&gt;% \n  left_join(x=.,\n            y=cntry_order,\n            by='country') %&gt;% \n  arrange(-avg_val) %&gt;% \n  gather(var, val, 2:8))\n\nWarning: attributes are not identical across measure variables; they will be\ndropped\n\n\n# A tibble: 378 × 4\n   country     avg_val var        val\n   &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1 Nigeria       1.15  AIDS  -0.00515\n 2 Somalia       0.922 AIDS  -0.711  \n 3 Equa Guinea   0.843 AIDS   0.902  \n 4 Chad          0.766 AIDS  -0.558  \n 5 Central AfR   0.741 AIDS   0.569  \n 6 SierraLeo     0.610 AIDS  -0.372  \n 7 Lesotho       0.610 AIDS   3.90   \n 8 Angola        0.551 AIDS  -0.340  \n 9 Sudan South   0.527 AIDS   1.02   \n10 Cameroon      0.449 AIDS   0.432  \n# ℹ 368 more rows\n\n\n\ncntry_order$country &lt;- as.factor(cntry_order$country)\n\n\n(df_target_scaled_vals &lt;- df_scaled_dist_results %&gt;% \n  filter(country == 'TARGET') %&gt;% \n  left_join(x=.,\n            y=df_scaled_dist_results_rel_targets %&gt;% filter(country == 'TARGET'),\n            by='country') %&gt;% \n  gather(var, target, 2:8) %&gt;% \n  select(2:3))\n\nWarning: attributes are not identical across measure variables; they will be\ndropped\n\n\n# A tibble: 7 × 2\n  var          target\n  &lt;chr&gt;         &lt;dbl&gt;\n1 AIDS         -0.821\n2 CHILDDTHR    -0.868\n3 Malaria      -0.782\n4 MATMORTRATIO -1.08 \n5 NEONATMOR    -0.516\n6 NonCommun    -3.05 \n7 TrafficAcc   -6.07 \n\n\n\ncbPalette &lt;- c(\"#999999\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\ntarg_cols &lt;- c('AIDS' = \"#E69F00\", \n               'CHILDDTHR' = \"#56B4E9\",\n               'Malaria' = \"#009E73\",\n               'MATMORTRATIO' = \"#F0E442\",\n               'NEONATMOR' = \"#0072B2\",\n               'NonCommun' = \"#D55E00\",\n               'TrafficAcc' = \"#CC79A7\")\n\n\n(dist_plot_df_2 &lt;- dist_plot_df %&gt;% \n  left_join(x=.,\n            y=df_target_scaled_vals,\n            by='var'))\n\n# A tibble: 378 × 5\n   country     avg_val var        val target\n   &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n 1 Nigeria       1.15  AIDS  -0.00515 -0.821\n 2 Somalia       0.922 AIDS  -0.711   -0.821\n 3 Equa Guinea   0.843 AIDS   0.902   -0.821\n 4 Chad          0.766 AIDS  -0.558   -0.821\n 5 Central AfR   0.741 AIDS   0.569   -0.821\n 6 SierraLeo     0.610 AIDS  -0.372   -0.821\n 7 Lesotho       0.610 AIDS   3.90    -0.821\n 8 Angola        0.551 AIDS  -0.340   -0.821\n 9 Sudan South   0.527 AIDS   1.02    -0.821\n10 Cameroon      0.449 AIDS   0.432   -0.821\n# ℹ 368 more rows\n\n\n\n(seg_length &lt;- dist_plot_df_2 %&gt;% \n  mutate(achieve = ifelse(val &lt;= target, 1, 0)) %&gt;% \n  filter(achieve == 0) %&gt;% \n  group_by(country) %&gt;% \n  summarise(min_val = min(val),\n            max_val = max(val)))\n\n# A tibble: 54 × 3\n   country      min_val max_val\n   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n 1 Algeria       -1.04    1.52 \n 2 Angola        -0.570   3.07 \n 3 Benin         -0.553   0.513\n 4 Botswana      -0.766   1.66 \n 5 Burkina Faso  -0.648   1.16 \n 6 Burundi       -1.03    1.11 \n 7 Cameroon      -0.761   1.29 \n 8 Cape Verde    -0.388   0.539\n 9 Central AfR   -0.497   1.73 \n10 Chad          -0.974   2.78 \n# ℹ 44 more rows\n\n\n\ndist_plot_df_2 %&gt;% \n  mutate(achieve = ifelse(val &lt;= target, 1, 0)) %&gt;% \n  filter(achieve == 0) %&gt;% \n  ggplot(.,\n         aes(x=val, \n             y=factor(country, levels=cntry_order$country))) + \n  geom_point(aes(color=var), \n             #height=.25, width=0,\n             alpha=.5, size=8) + \n  geom_segment(data=seg_length,\n               aes(x=min_val, xend=max_val, yend=country),\n               color='gray60', linewidth=1) + \n  labs(x='',\n       y='') + \n  my.theme.minimal + \n  theme(legend.position = 'top') + \n  scale_color_manual(values = targ_cols,\n                     labels = c('MATMORTRATIO' = 'Maternal Mortality',\n                               'NEONATMOR' = 'Neonatal Mortality',\n                               'CHILDDTHR' = '&lt; 5 Mortality',\n                               'NonCommun' = 'NCDs',\n                               'TrafficAcc' = 'Traffic Mortality')) + \n  scale_y_discrete(labels = function(x) str_wrap(x, width = 10)) + \n  scale_x_continuous(position = 'top')"
  },
  {
    "objectID": "posts/2023-07-22-data-science-workflows-js-2/index.html",
    "href": "posts/2023-07-22-data-science-workflows-js-2/index.html",
    "title": "Data Science Workflows in Javascript: Part Two",
    "section": "",
    "text": "Introduction\nLearning objectives After following along in Session #2, participants will be able to:\nAccess remote data from an online repository with d3.csv Merge and wrangle data from multiple files Create interactive exploratory data viz with Observable Plot and Inputs Reuse community examples content using imports Perform and visualize cluster analysis by k-means Publish outcomes with embeds Background In this session, we’ll recreate the curated penguins dataset from scratch by accessing the raw data from the Environmental Data Initiative Data Portal, wrangling it to match the curated version, exploring the data in interactive data visualizations, then doing some cluster analysis by k-means and hierarchical clustering.\nThe penguins data contains size measurements and blood isotope analysis for nesting pairs of three penguin species (Adélie, gentoo, and chinstrap), collected on islands near Palmer Archipelago, Antarctica, from 2007 - 2009 by Dr. Kristen B. Gorman and colleagues (Gorman et al. 2016).\nGorman et al (2014). Ecological Sexual Dimorphism and Environmental Variability within a Community of Antarctic Penguins (Genus pygoscelis). PLoS ONE, 9(3): e90081. https://dx.plos.org/10.1371/journal.pone.0090081.\nStep 0: Fork this notebook! Fork this follow-along notebook to make your own copy, in your own account. Working in your own fork will ensure that changes you make during the session are saved.\nNote: You can work without an account by making changes in tinker mode (not recommended), but your changes will not be saved, and refreshing the page will erase any changes.\nStep 1: Get the raw data from Environmental Data Initiative using d3.csv()\nFirst, we’ll access the penguins data for each of the three species from the Environmental Data Initiative using d3.csv(). The links for the three species are:\nGentoo penguins: https://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-pal.220.7&entityid=e03b43c924f226486f2f0ab6709d2381 Adélie penguins: https://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-pal.219.5&entityid=002f3893385f710df69eeebe893144ff Chinstrap penguins: https://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-pal.221.8&entityid=fe853aa8f7a59aa84cdd3197619ef462 Let’s store these in our notebook as gentoo, adelie, and chinstrap (note that they are stored as arrays of objects, with all properties as characters). We will use d3.autoType to parse the data (assign types), otherwise they will be automatically interpreted as characters.\nLearn more about accessing data from remote files and APIs.\nNote: d3.csv() was not working, so I’ve switched this to FileAttachment().\n\ngentoo = FileAttachment(\".//data/gentoo.csv\").csv({ typed: true })\n\nadelie = FileAttachment(\".//data/adelie.csv\").csv({ typed: true })\n\nchinstrap = FileAttachment(\".//data/chinstrap.csv\").csv({ typed: true })\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 2: Merge & wrangle the data\nOur first step will be to get these all into a single array of objects. Since they all have the same properties, in the same order, we can use the concat() method to combine them.\n\n// Make combined version, penguinsCombo, here\npenguinsCombo = chinstrap.concat(gentoo, adelie)\n\n\n\n\n\n\nRecreate the curated penguins data in JS Let’s wrangle the data in JavaScript to recreate the curated penguins data, updating to only keep and rename the following:\n\nSpecies (renamed species, and limited to only the first word)\nIsland (renamed island)\nSex (renamed sex, and converted to lowercase)\nCulmen Length (mm) (renamed bill_length_mm)\nCulmen Depth (mm) (renamed bill_depth_mm)\nBody Mass (g) (renamed body_mass_g)\nFlipper Length (mm) (renamed flipper_length_mm)\n\n\n// Create the wrangled version of penguins here: \npenguins = penguinsCombo.map((d) =&gt; ({\n  species: d.Species.split(\" \")[0],\n  island: d.Island,\n  sex: d.Sex == null || d.Sex == \".\" ? null : d.Sex.toLowerCase(),\n  bill_length_mm: d[\"Culmen Length (mm)\"],\n  bill_depth_mm: d[\"Culmen Depth (mm)\"],\n  body_mass_g: d[\"Body Mass (g)\"],\n  flipper_length_mm: d[\"Flipper Length (mm)\"]}))\n\n\n\n\n\n\nMeet Arquero Although we could continue do all of our wrangling in plain JavaScript, let’s explore a different tool for working with data in JavaScript - the Arquero library. Arquero may feel more familiar for data scientists who have been working in pandas (Python users) or dplyr / tidyr (R users).\nIn the last session, we used require to load an external library. Here, we’ll use import to access library components from another Observable notebook. This highlights another cool way we can reuse content (functions, data, text, really anything we want…) from the Observable ecosystem.\nThe following line imports all aq and op module functions from the notebook https://observablehq.com/@uwdata/arquero (notice that the slug after observable.com in the URL is what’s used to import from the notebook):\n\nimport {aq, op} from \"@uwdata/arquero\"\n\n\n\n\n\n\nTo use the Arquero verbs, we need to convert our array of objects to an Arquero table:\n\n// Convert your array of objects to an Arquero table here:\npenguinsTable = aq.from(penguins).view\n// you can append .view() at the end to show the table\n\n\n\n\n\n\nAnd from there, we can chain together Arquero methods much like we would in dplyr or pandas. For example, let’s write a sequence where we:\nFilter to create a subset only of female penguins Select columns for species, bill depth and bill length Add a new column with the ratio of bill length to bill depth Group by species, and find the mean bill ratio by group\n\n// Write Arquero code to perform the steps above here: \npenguinsTable\n  .filter((d) =&gt; d.sex == \"female\")\n  .select('species', 'bill_depth_mm', 'bill_length_mm')\n  .derive({bill_ratio: d =&gt; d.bill_length_mm / d.bill_depth_mm}) //like mutate() in dplyr\n  .groupby('species')\n  .rollup({ mean_bill_ratio: d =&gt; op.mean(d.bill_ratio)})\n  .view()\n//need to review more of the map function and arrow function process\n\n\n\n\n\n\nStep 3: Exploratory data visualization First, let’s quickly refresh how we can make some quick plots using Observable Plot.\nChart cell Use the Chart cell to get started with charts, without code. You can always eject to JavaScript to keep customizing! Let’s use the Chart cell to make a scatterplot of bill dimensions by species. Then we’ll eject to Plot code to keep customizing.\n\nPlot.plot({\n  color: { legend: true },\n  marks: [\n    Plot.dot(penguins, {\n      x: \"bill_length_mm\",\n      y: \"bill_depth_mm\",\n      fill: \"species\",\n      tip: true,\n      r: \"body_mass_g\",\n      opacity: 0.5\n    }),\n    Plot.frame()\n  ],\n  color: {range: [\"teal\", \"darkorange\", \"orchid\"]},\n  r: {domain: d3.extent(penguins.map(d =&gt; d.body_mass_g)), range: [1, 20]},\n  grid: true\n})\n\n\n\n\n\n\nImport an visualize a pairplot What if we want to make a pair plot so that we can simultaneously explore different relationships and distributions? Well, we could build that ourselves…or we can use something that’s already built in another notebook!\nObservable imports let us access content (functions, charts, any content really…) from another notebook. Here, we’ll import the PlotMatrix function (to create an interactive chart) from the @observablehq/autoplot-matrix notebook.\n\nimport {PlotMatrix} with {data} from \"@observablehq/autoplot-matrix\"\n\n\n\n\n\n\nThat function expects the data to be stored as data, so we’ll make a copy of penguins named data:\n\ndata = penguins\n\n\n\n\n\n\nThen we can feed data into the PlotMatrix function to create a pairplot:\n\nPlotMatrix(data)\n\n\n\n\n\n\nInteractivity with Inputs In Session 1, we briefly saw how we can add Inputs and connect their values to chart elements for interactivity. Let’s practice a few more that can help us explore different slices of the data.\nStep 4: k-means and hierarchical clustering Now, we will see how clustering by k-means and hierarchical clustering do. Both are an unsupervised learning method, meaning that we don’t try to fit our data or estimates based on a known outcome.\nPrep the data Our first step is to make a subset with only the variables we’ll include in our cluster analysis, then scale the data (we’ll use Christoph Pahmeyer’s scale function, imported below, which will scale all properties to have a mean value of 0 and standard deviation of 1).\nDisclaimer: there are different ways to scale data, which we do not cover here. For your own projects, it is important to consider appropriate scaling and analysis beyond what we cover in this brief introduction to cluster analysis in JavaScript.\n\nimport {scale} from \"@chrispahm/hierarchical-clustering\"\n\n\n\n\n\n\n\n// Make a subset of penguins with complete cases (filter out values where bill length is null):\npenguinsComplete = penguins.filter((d) =&gt; d.bill_length_mm !== null)// && d.sex == \"female\")\n\n\n\n\n\n\n\n// Create a scaled version of the values (non-numeric will be NaN, which is fine..): \npenguinsScale = scale(penguinsComplete)\n\n\n\n\n\n\n\n// Convert the array of objects to an array of arrays: \npenguinsArray = penguinsScale.map(\n  (d) =&gt; [d.bill_length_mm, d.bill_depth_mm, d.body_mass_g, d.flipper_length_mm]\n)\n\n\n\n\n\n\n\n\nk-means with ml.js KMeans()\nAs in Session 1, we’ll use the ml.js library, which contains methods for k-means and hierarchical clustering. Because ml.js isn’t recommended automatically as part of Observable’s standard library, it is loaded in the appendix of this notebook.\n\nML = require(\"https://www.lactame.com/lib/ml/6.0.0/ml.min.js\")\n\n\n\n\n\n\n\n// Use ml.js KMeans() method to perform k-means clustering for k centroids: \npenguinsClusters = ML.KMeans(penguinsArray, 3)\n\n\n\n\n\n\nLet’s see how these clusters map onto the three different penguin species.\n\n// Combine the cluster values for each element with the original female penguins data:\npenguinsKmeans = penguinsComplete.map((d,i) =&gt; ({...d, clusterNo: penguinsClusters.clusters[i]}))\n\n\n\n\n\n\nFinally, we’ll visualize the output of our k-means clustering in Observable Plot. Add a snippet to create a scatterplot, then we’ll customize to show our clusters that we can explore across different variables.\n\nmyChart = Plot.plot({ // Remember to name if you want to use embeds! \n  marks: [\n    Plot.text(penguinsKmeans, {\n      x: \"body_mass_g\",\n      y: \"flipper_length_mm\",\n      text: \"clusterNo\",\n      fontSize: \"15px\",\n      fontWeight: 500,\n      fill: \"species\",\n      tip: true\n    })\n  ],\n  color: { legend: true }\n})\n\n\n\n\n\n\nCitations A. Horst, A. Hill, and K.B. Gorman. Palmer Archipelago Penguins Data in the palmerpenguins R Package - An Alternative to Anderson’s Irises. The R Journal, 2022. https://journal.r-project.org/articles/RJ-2022-020/)\nPalmer Station Antarctica LTER and K. Gorman, 2020. Structural size measurements and isotopic signatures of foraging among adult male and female Adélie penguins (Pygoscelis adeliae) nesting along the Palmer Archipelago near Palmer Station, 2007-2009 ver 5. Environmental Data Initiative. https://doi.org/10.6073/pasta/98b16d7d563f265cb52372c8ca99e60f\nPalmer Station Antarctica LTER and K. Gorman, 2020. Structural size measurements and isotopic signatures of foraging among adult male and female Gentoo penguin (Pygoscelis papua) nesting along the Palmer Archipelago near Palmer Station, 2007-2009 ver 5. Environmental Data Initiative. https://doi.org/10.6073/pasta/7fca67fb28d56ee2ffa3d9370ebda689\nPalmer Station Antarctica LTER and K. Gorman, 2020. Structural size measurements and isotopic signatures of foraging among adult male and female Chinstrap penguin (Pygoscelis antarcticus) nesting along the Palmer Archipelago near Palmer Station, 2007-2009 ver 6. Environmental Data Initiative. https://doi.org/10.6073/pasta/c14dfcfada8ea13a17536e73eb6fbe9e"
  },
  {
    "objectID": "posts/2024-08-13-tidycensus-exploration/index.html",
    "href": "posts/2024-08-13-tidycensus-exploration/index.html",
    "title": "Part One: Exploring child poverty data with the tidycensus R package",
    "section": "",
    "text": "I’ve used data from the U.S. Census Bureau several times, and for this project, I wanted to reacquaint myself with the tidycensus:: package to gather and wrangle data. I also wanted to use the usmap:: package to generate a simple U.S. map, and the gt:: package to display the data in a nice table format.\n\n\n\nlibrary(tidyverse)\nlibrary(tidycensus)\n1library(scales)\n2library(janitor)\n3library(glue)\n4library(gt)\nlibrary(usmap)\n\n5# census_api_key('INSERT KEY HERE', install = TRUE)\n\n\n1\n\nLoading the scales:: package to transform ggplot scales simply (some people choose to explicitly define scales:: in their code rather than loading the library).\n\n2\n\nThe janitor::clean_names() function tidies the column names of your dataset to use the snake case convention. Very handy!\n\n3\n\nThe glue:: package allows for simple addition of HTML to ggplot graphics.\n\n4\n\nThe gt:: library provides functionality for creating ggplot-esque tables.\n\n5\n\nThe first time that you’re working with the tidycensus:: package, you need to request an API key at https://api.census.gov/data/key_signup.html. The install= argument will install your personal key to the .Renviron file, and you won’t need to use the census_api_key() function again.\n\n\n\n\n\n\n\nFor this analysis, I’m interested in looking at the most recent state-level child poverty data available from the U.S. Census Bureau. The tidycensus:: package allows API access to the decennial Census, as well as the more frequent American Community Survey (ACS), which I’ll use in this project.\nIf you’ve worked with ACS data before, you may know that there are a few survey products offered in the ACS suite. Most commonly, the choice of data is between the 1-year estimates and the 5-year estimates.\nWhat’s the difference between these, and how do you choose which survey product to use for your purposes?1\n\n\n\n\n\n\n\n\nFeature\nACS 1-Year Estimates\nACS 5-Year Estimates\n\n\n\n\nData Collection Period\n12 months\n60 months\n\n\nPopulation Coverage\nAreas with 65,000 or more people\nAll geographic areas, including those with fewer than 65,000 people\n\n\nSample Size\nSmallest\nLargest\n\n\nReliability\nLess reliable due to smaller sample size\nMore reliable due to larger sample size\n\n\nCurrency\nMost current data\nLess current, includes older data\n\n\nRelease Frequency\nAnnually\nAnnually\n\n\nBest Used For\nAnalyzing large populations, when currency is more important than precision\nAnalyzing small populations, where precision is more important than currency\n\n\nExample Usage\nExamining recent economic changes\nExamining trends in small geographic areas or small population subgroups\n\n\n\n\nTo start, I am interested in reviewing the most stable, geographically-available data on child poverty. Given that I’m less concerned with recency and more interested in broad availability, the ACS 5-year estimates are what I’ll use here.\n\n\n\nThe tidycensus:: package has so much to offer (and I still have plenty to learn!). The tidycensus::load_variables() function provides a simple way to query the available data within each survey. Combining this with stringr::str_detect() is a nice way to search through the tens of thousands of data series that are available through the U.S. Census API.\n\nload_variables(2022, \"acs5\", cache = TRUE) %&gt;% \n6  filter(str_detect(label, \"Under 5 years\"))\n\n\n6\n\nThis searches all variable labels for “Under 5 years” to help identify data of interest.\n\n\n\n\n# A tibble: 240 × 4\n   name        label                                    concept        geography\n   &lt;chr&gt;       &lt;chr&gt;                                    &lt;chr&gt;          &lt;chr&gt;    \n 1 B01001A_003 Estimate!!Total:!!Male:!!Under 5 years   Sex by Age (W… tract    \n 2 B01001A_018 Estimate!!Total:!!Female:!!Under 5 years Sex by Age (W… tract    \n 3 B01001B_003 Estimate!!Total:!!Male:!!Under 5 years   Sex by Age (B… tract    \n 4 B01001B_018 Estimate!!Total:!!Female:!!Under 5 years Sex by Age (B… tract    \n 5 B01001C_003 Estimate!!Total:!!Male:!!Under 5 years   Sex by Age (A… tract    \n 6 B01001C_018 Estimate!!Total:!!Female:!!Under 5 years Sex by Age (A… tract    \n 7 B01001D_003 Estimate!!Total:!!Male:!!Under 5 years   Sex by Age (A… tract    \n 8 B01001D_018 Estimate!!Total:!!Female:!!Under 5 years Sex by Age (A… tract    \n 9 B01001E_003 Estimate!!Total:!!Male:!!Under 5 years   Sex by Age (N… tract    \n10 B01001E_018 Estimate!!Total:!!Female:!!Under 5 years Sex by Age (N… tract    \n# ℹ 230 more rows\n\n\nFor this demo, I’ll use the following series:\n\nB01001_003: Estimate!!Total:!!Male:!!Under 5 years (all racial groups)\nB01001_027: Estimate!!Total:!!Female:!!Under 5 years (all racial groups)\nB17001_004: Estimate!!Total:!!Income in the past 12 months below poverty level:!!Male:!!Under 5 years\nB17001_018: Estimate!!Total:!!Income in the past 12 months below poverty level:!!Female:!!Under 5 years\n\nThere are a bunch of useful helper functions/arguments to assist in fetching data from the Census API. Some noteworthy ones include:\n\nEach variable returns the geography, an estimate, and the margin of error (“moe”). Geographies can span from states, regions and the country as a whole, down to areas like school districts, voting districts, census block groups, and many others.\nsurvey=: this defines the produce that you’re using of the American Community Survey. Responses can include “acs1”, “acs3”, or (the default) “acs5”.\nsummary_var=: often the variable that you want would be made more meaningful as a ratio or with a demonminator. For example, the number of children in poverty could be useful on its own, but you’re likely to want to see that series as a percent of the total children. With the summary_var argument, you can tell the function which secondary variable you want to grab in the same API call.\nouput=wide: related to the above, I wanted to look at child poverty in a way that would require multiple summary variables (e.g. the percent of girls and boys in poverty). Since you can only have one summary variable, output='wide' allows you to grab all of the series that you may need in the same call.\ngeometry=TRUE: this argument returns the geospatial data in tidy format to create quick ggplot-based maps using geom_sf().\n\n\ndf &lt;- get_acs(geography = 'state',\n        variables = c(male_u5_pop = 'B01001_003', \n                      female_u5_pop = 'B01001_027', \n                      male_u5_poverty = 'B17001_004', \n                      female_u5_poverty = 'B17001_018'),\n        survey = 'acs5',\n        year = 2022,\n        output = 'wide')\n\nGetting data from the 2018-2022 5-year ACS\n\n\nNext, I’ll create some fields to combine gender-based poverty estimates and calculate a percent of the child population measure.\n\ndf &lt;- df %&gt;% \n  rename(state = NAME) %&gt;% \n  mutate(total_u5_popE = male_u5_popE + female_u5_popE,\n         total_u5_povertyE = male_u5_povertyE + male_u5_povertyE,\n         perc_u5_in_poverty = total_u5_povertyE / total_u5_popE)\n\nYou can combine point estimates for gender-based poverty by simply adding them, but you can’t do the same for margins of error. The estimated margins of error for each estimate are based on their respective samples and the uncertainty in them. To re-weight the margins of error, you take the square root of the sum of squared margins of error:\n\nnew_MOE_calc = sqrt(male_u5_povertyM^2 + female_u5_povertyM^2)\n\nThe tidycensus::moe_sum() function will calculate the new margin of error for you!\n\ndf &lt;- df %&gt;% \n  group_by(state) %&gt;% \n  mutate(total_u5_poverty_MOEcalc = moe_sum(male_u5_povertyM, female_u5_povertyM),\n         perc_u5_in_poverty_MOEcalc = total_u5_poverty_MOEcalc / total_u5_popE)\n\nNow, I can visualize the new point estimates and margins of error for child poverty (not specific to gender).\n\ndf %&gt;% \n7  filter(state != 'Puerto Rico') %&gt;%\n  ggplot(\n    aes(x=perc_u5_in_poverty, \n        y=reorder(state, perc_u5_in_poverty))) + \n  geom_point(size=3) +\n  geom_errorbarh(aes(xmin=perc_u5_in_poverty - perc_u5_in_poverty_MOEcalc, \n                     xmax=perc_u5_in_poverty + perc_u5_in_poverty_MOEcalc),\n                 height = 0.4) + \n  labs(y='',\n       x='',\n       caption=caption_text) + \n  ggtitle(title_text,\n          subtitle=subtitle_text) + \n  scale_x_continuous(labels = percent,\n                     position = 'top') + \n  my.theme\n\n\n7\n\nRemoving Puerto Rico because it’s not within a U.S. Census region."
  },
  {
    "objectID": "posts/2024-08-13-tidycensus-exploration/index.html#setup",
    "href": "posts/2024-08-13-tidycensus-exploration/index.html#setup",
    "title": "Part One: Exploring child poverty data with the tidycensus R package",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidycensus)\n1library(scales)\n2library(janitor)\n3library(glue)\n4library(gt)\nlibrary(usmap)\n\n5# census_api_key('INSERT KEY HERE', install = TRUE)\n\n\n1\n\nLoading the scales:: package to transform ggplot scales simply (some people choose to explicitly define scales:: in their code rather than loading the library).\n\n2\n\nThe janitor::clean_names() function tidies the column names of your dataset to use the snake case convention. Very handy!\n\n3\n\nThe glue:: package allows for simple addition of HTML to ggplot graphics.\n\n4\n\nThe gt:: library provides functionality for creating ggplot-esque tables.\n\n5\n\nThe first time that you’re working with the tidycensus:: package, you need to request an API key at https://api.census.gov/data/key_signup.html. The install= argument will install your personal key to the .Renviron file, and you won’t need to use the census_api_key() function again."
  },
  {
    "objectID": "posts/2024-08-13-tidycensus-exploration/index.html#data",
    "href": "posts/2024-08-13-tidycensus-exploration/index.html#data",
    "title": "Part One: Exploring child poverty data with the tidycensus R package",
    "section": "",
    "text": "For this analysis, I’m interested in looking at the most recent state-level child poverty data available from the U.S. Census Bureau. The tidycensus:: package allows API access to the decennial Census, as well as the more frequent American Community Survey (ACS), which I’ll use in this project.\nIf you’ve worked with ACS data before, you may know that there are a few survey products offered in the ACS suite. Most commonly, the choice of data is between the 1-year estimates and the 5-year estimates.\nWhat’s the difference between these, and how do you choose which survey product to use for your purposes?1\n\n\n\n\n\n\n\n\nFeature\nACS 1-Year Estimates\nACS 5-Year Estimates\n\n\n\n\nData Collection Period\n12 months\n60 months\n\n\nPopulation Coverage\nAreas with 65,000 or more people\nAll geographic areas, including those with fewer than 65,000 people\n\n\nSample Size\nSmallest\nLargest\n\n\nReliability\nLess reliable due to smaller sample size\nMore reliable due to larger sample size\n\n\nCurrency\nMost current data\nLess current, includes older data\n\n\nRelease Frequency\nAnnually\nAnnually\n\n\nBest Used For\nAnalyzing large populations, when currency is more important than precision\nAnalyzing small populations, where precision is more important than currency\n\n\nExample Usage\nExamining recent economic changes\nExamining trends in small geographic areas or small population subgroups\n\n\n\n\nTo start, I am interested in reviewing the most stable, geographically-available data on child poverty. Given that I’m less concerned with recency and more interested in broad availability, the ACS 5-year estimates are what I’ll use here."
  },
  {
    "objectID": "posts/2024-08-13-tidycensus-exploration/index.html#initial-tour-of-key-tidycensusget_acs-function",
    "href": "posts/2024-08-13-tidycensus-exploration/index.html#initial-tour-of-key-tidycensusget_acs-function",
    "title": "Part One: Exploring child poverty data with the tidycensus R package",
    "section": "",
    "text": "The tidycensus:: package has so much to offer (and I still have plenty to learn!). The tidycensus::load_variables() function provides a simple way to query the available data within each survey. Combining this with stringr::str_detect() is a nice way to search through the tens of thousands of data series that are available through the U.S. Census API.\n\nload_variables(2022, \"acs5\", cache = TRUE) %&gt;% \n6  filter(str_detect(label, \"Under 5 years\"))\n\n\n6\n\nThis searches all variable labels for “Under 5 years” to help identify data of interest.\n\n\n\n\n# A tibble: 240 × 4\n   name        label                                    concept        geography\n   &lt;chr&gt;       &lt;chr&gt;                                    &lt;chr&gt;          &lt;chr&gt;    \n 1 B01001A_003 Estimate!!Total:!!Male:!!Under 5 years   Sex by Age (W… tract    \n 2 B01001A_018 Estimate!!Total:!!Female:!!Under 5 years Sex by Age (W… tract    \n 3 B01001B_003 Estimate!!Total:!!Male:!!Under 5 years   Sex by Age (B… tract    \n 4 B01001B_018 Estimate!!Total:!!Female:!!Under 5 years Sex by Age (B… tract    \n 5 B01001C_003 Estimate!!Total:!!Male:!!Under 5 years   Sex by Age (A… tract    \n 6 B01001C_018 Estimate!!Total:!!Female:!!Under 5 years Sex by Age (A… tract    \n 7 B01001D_003 Estimate!!Total:!!Male:!!Under 5 years   Sex by Age (A… tract    \n 8 B01001D_018 Estimate!!Total:!!Female:!!Under 5 years Sex by Age (A… tract    \n 9 B01001E_003 Estimate!!Total:!!Male:!!Under 5 years   Sex by Age (N… tract    \n10 B01001E_018 Estimate!!Total:!!Female:!!Under 5 years Sex by Age (N… tract    \n# ℹ 230 more rows\n\n\nFor this demo, I’ll use the following series:\n\nB01001_003: Estimate!!Total:!!Male:!!Under 5 years (all racial groups)\nB01001_027: Estimate!!Total:!!Female:!!Under 5 years (all racial groups)\nB17001_004: Estimate!!Total:!!Income in the past 12 months below poverty level:!!Male:!!Under 5 years\nB17001_018: Estimate!!Total:!!Income in the past 12 months below poverty level:!!Female:!!Under 5 years\n\nThere are a bunch of useful helper functions/arguments to assist in fetching data from the Census API. Some noteworthy ones include:\n\nEach variable returns the geography, an estimate, and the margin of error (“moe”). Geographies can span from states, regions and the country as a whole, down to areas like school districts, voting districts, census block groups, and many others.\nsurvey=: this defines the produce that you’re using of the American Community Survey. Responses can include “acs1”, “acs3”, or (the default) “acs5”.\nsummary_var=: often the variable that you want would be made more meaningful as a ratio or with a demonminator. For example, the number of children in poverty could be useful on its own, but you’re likely to want to see that series as a percent of the total children. With the summary_var argument, you can tell the function which secondary variable you want to grab in the same API call.\nouput=wide: related to the above, I wanted to look at child poverty in a way that would require multiple summary variables (e.g. the percent of girls and boys in poverty). Since you can only have one summary variable, output='wide' allows you to grab all of the series that you may need in the same call.\ngeometry=TRUE: this argument returns the geospatial data in tidy format to create quick ggplot-based maps using geom_sf().\n\n\ndf &lt;- get_acs(geography = 'state',\n        variables = c(male_u5_pop = 'B01001_003', \n                      female_u5_pop = 'B01001_027', \n                      male_u5_poverty = 'B17001_004', \n                      female_u5_poverty = 'B17001_018'),\n        survey = 'acs5',\n        year = 2022,\n        output = 'wide')\n\nGetting data from the 2018-2022 5-year ACS\n\n\nNext, I’ll create some fields to combine gender-based poverty estimates and calculate a percent of the child population measure.\n\ndf &lt;- df %&gt;% \n  rename(state = NAME) %&gt;% \n  mutate(total_u5_popE = male_u5_popE + female_u5_popE,\n         total_u5_povertyE = male_u5_povertyE + male_u5_povertyE,\n         perc_u5_in_poverty = total_u5_povertyE / total_u5_popE)\n\nYou can combine point estimates for gender-based poverty by simply adding them, but you can’t do the same for margins of error. The estimated margins of error for each estimate are based on their respective samples and the uncertainty in them. To re-weight the margins of error, you take the square root of the sum of squared margins of error:\n\nnew_MOE_calc = sqrt(male_u5_povertyM^2 + female_u5_povertyM^2)\n\nThe tidycensus::moe_sum() function will calculate the new margin of error for you!\n\ndf &lt;- df %&gt;% \n  group_by(state) %&gt;% \n  mutate(total_u5_poverty_MOEcalc = moe_sum(male_u5_povertyM, female_u5_povertyM),\n         perc_u5_in_poverty_MOEcalc = total_u5_poverty_MOEcalc / total_u5_popE)\n\nNow, I can visualize the new point estimates and margins of error for child poverty (not specific to gender).\n\ndf %&gt;% \n7  filter(state != 'Puerto Rico') %&gt;%\n  ggplot(\n    aes(x=perc_u5_in_poverty, \n        y=reorder(state, perc_u5_in_poverty))) + \n  geom_point(size=3) +\n  geom_errorbarh(aes(xmin=perc_u5_in_poverty - perc_u5_in_poverty_MOEcalc, \n                     xmax=perc_u5_in_poverty + perc_u5_in_poverty_MOEcalc),\n                 height = 0.4) + \n  labs(y='',\n       x='',\n       caption=caption_text) + \n  ggtitle(title_text,\n          subtitle=subtitle_text) + \n  scale_x_continuous(labels = percent,\n                     position = 'top') + \n  my.theme\n\n\n7\n\nRemoving Puerto Rico because it’s not within a U.S. Census region."
  },
  {
    "objectID": "posts/2024-08-13-tidycensus-exploration/index.html#footnotes",
    "href": "posts/2024-08-13-tidycensus-exploration/index.html#footnotes",
    "title": "Part One: Exploring child poverty data with the tidycensus R package",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee the Census handbook for great detail: https://www.census.gov/programs-surveys/acs/library/handbooks/general.html.↩︎"
  },
  {
    "objectID": "posts/2019-06-25-mapping-african-health-clusters/index.html",
    "href": "posts/2019-06-25-mapping-african-health-clusters/index.html",
    "title": "PART TWO: Mapping of African Health Clusters",
    "section": "",
    "text": "library(tidyverse)\nlibrary(sf)\nlibrary(maps)\nlibrary(ggsci)\nlibrary(patchwork)\n\nFirst, I’ll read in the cluster analysis results from Part One.\n\nkmeans_viz_data &lt;- read_csv('.//data/kmeans_viz_data.csv')\n\nGet the country polygons from the sf:: package.\n\nworld1 &lt;- sf::st_as_sf(map('world', plot = FALSE, fill = TRUE))\n\nPlot these polygons using ggplot().\n\nggplot() + \n  geom_sf(data = world1) + \n  coord_sf(xlim = c(-30, 60),\n            ylim = c(39, -37))\n\n\n\n\n\n\n\n\nCreate custom map themes.\n\nmap.theme &lt;- theme(text = element_text(family = \"Arial\", color = \"black\"), \n                   panel.background = element_rect(fill = \"white\"),\n                   plot.background = element_rect(fill = \"white\"),\n                   #panel.grid = element_blank(),\n                   #panel.border = element_blank(),\n                   plot.title = element_text(size = 25, hjust = 0),\n                   plot.subtitle = element_text(size = 25),\n                   plot.caption = element_text(size = 20), \n                   axis.text = element_blank(),\n                   axis.title = element_blank(),\n                   axis.ticks = element_blank(),\n                   legend.position = \"inside\", \n                   legend.key.size = unit(.7, \"cm\"),\n                   legend.direction = \"horizontal\",\n                   legend.background = element_rect(fill=\"white\"))\n\nmap.theme.small &lt;- theme(text = element_text(family = \"Arial\", color = \"black\"), \n                   panel.background = element_rect(fill = \"white\"),\n                   plot.background = element_rect(fill = \"white\"),\n                   panel.grid = element_blank(),\n                   plot.title = element_text(size = 8, face = 'bold', hjust = 0.5),\n                   plot.subtitle = element_text(size = 8),\n                   axis.text = element_blank(),\n                   axis.title = element_blank(),\n                   axis.ticks = element_blank(),\n                   legend.position = \"none\", \n                   legend.background = element_rect(fill=\"white\"),\n                   legend.title = element_blank())\n\n\nMapping\nRecode some country names to make sf:: and the cluster results compatible.\n\nmap.world &lt;- map_data(\"world\") %&gt;% \n  mutate(region = recode(region, \n                         \"Central African Republic\" = \"Central AfR\",\n                         'Democratic Republic of the Congo' = 'Congo, Democratic Republic of',\n                         'Republic of Congo' = 'Congo, Republic of',\n                         'Equatorial Guinea' = 'Equa Guinea',\n                         'Guinea-Bissau' = 'GuineaBiss',\n                         'Ivory Coast' = \"Cote d'Ivoire\",\n                         'Sierra Leone' = 'SierraLeo', \n                         'South Sudan' = 'Sudan South'))\n\n\n\nCluster maps\n\njoined.world.map &lt;- left_join(x=map.world,\n                             y=kmeans_viz_data,\n                             by=c('region' = 'country'))\n\nUsing the coordinates from above to focus on Africa, I’ll map the cluster results.\n\n(l_map_clust &lt;- joined.world.map %&gt;% \n  filter(africa == '1') %&gt;% \n  ggplot(.) +\n  geom_polygon(aes(x = long, y = lat, group = group, fill = clust_name), color=\"gray90\") +\n  geom_polygon(data=. %&gt;% filter(region == 'Western Sahara'),\n               aes(x = long, y = lat, group = group), fill = \"gray90\", color=\"gray90\") +\n  coord_map(xlim = c(-30, 60),\n            ylim = c(39, -37)) + \n  labs(fill = '') + \n  scale_fill_jco() + \n  map.theme + \n  guides(fill=guide_legend(nrow=2, byrow=TRUE)) + \n  theme(legend.position.inside = c(.2, .3)))\n\n\n\n\n\n\n\n\nI want to plot each of the island nations alongside the continental map, so that you can see their results more clearly. I’ll generate a zoomed in map for each, and then use patchwork:: to bring them all together.\n\n(s_map_1 &lt;- joined.world.map %&gt;% \n  filter(africa == 1) %&gt;% \n  ggplot(.) +\n  geom_polygon(aes(x = long, y = lat, group = group, fill = clust_name), color=\"gray90\") +\n  scale_fill_jco() + \n  ggtitle('Cabo Verde') + \n  coord_map(xlim = c(-25.5, -22.2), ylim = c(14.5, 17.8)) + \n  map.theme.small)\n\n\n\n\n\n\n\n\n\n(s_map_2 &lt;- joined.world.map %&gt;% \n  filter(africa == 1) %&gt;% \n  ggplot(.) +\n  geom_polygon(aes(x = long, y = lat, group = group, fill = clust_name), color=\"gray90\") +\n  scale_fill_jco() + \n  ggtitle('Comoros') + \n  coord_map(xlim = c(42.1, 45.4), ylim = c(-13.4, -10.1)) + \n  map.theme.small)\n\n\n\n\n\n\n\n\n\n(s_map_3 &lt;- joined.world.map %&gt;% \n  filter(africa == 1) %&gt;% \n  ggplot(.) +\n  geom_polygon(aes(x = long, y = lat, group = group, fill = clust_name), color=\"gray90\") +\n  scale_fill_jco() + \n  ggtitle('Mauritius') + \n  coord_map(xlim = c(56, 59.3), ylim = c(-22.3, -19)) + \n  map.theme.small)\n\n\n\n\n\n\n\n\n\n(s_map_4 &lt;- joined.world.map %&gt;% \n  filter(africa == 1) %&gt;% \n  ggplot(.) +\n  geom_polygon(aes(x = long, y = lat, group = group, fill = clust_name), color=\"gray90\") +\n  scale_fill_jco() + \n  ggtitle('Sao Tome and Principe') + \n  coord_map(xlim = c(5, 8.3), ylim = c(-.5, 2.8)) + \n  map.theme.small)\n\n\n\n\n\n\n\n\n\n(s_map_5 &lt;- joined.world.map %&gt;% \n  filter(africa == 1) %&gt;% \n  ggplot(.) +\n  geom_polygon(aes(x = long, y = lat, group = group, fill = clust_name), color=\"gray90\") +\n  scale_fill_jco() + \n  ggtitle('Seychelles') + \n  coord_map(xlim = c(55.2, 55.8), ylim = c(-4.9, -4.4)) + \n  map.theme.small)\n\n\n\n\n\n\n\n\n\n(s_map_plot_grid &lt;- s_map_1 / s_map_2 / s_map_3 / s_map_4 / s_map_5)\n\n\n\n\n\n\n\n\nNow, the final plot that brings them all together.\n\n(l_map_clust + s_map_plot_grid + plot_layout(widths = c(8, 1, 1)))\n\n\n\n\n\n\n\n\nThese results demonstrate that, while there are some discernible regional patterns in health outcomes, some countries in West, Central, and East Africa do not necessarily look like their neighbors."
  },
  {
    "objectID": "posts/2017-10-15-gapminder-gganimate/index.html",
    "href": "posts/2017-10-15-gapminder-gganimate/index.html",
    "title": "Animating the Gapminder dataset using gganimate",
    "section": "",
    "text": "In this project, I wanted to experiment with gganimate:: to reproduce the classic animated visualization by Hans Rosling of Gapminder data.\n\n\n\nThe Gapminder dataset, compiled by the Gapminder Foundation, provides data on global development trends, including indicators like life expectancy, GDP per capita, and population. The dataset is known for its use in dynamic visualizations that illustrate changes in global indicators over time.\n\n\n\n\n\n        country        continent        year         lifeExp     \n Afghanistan:  12   Africa  :624   Min.   :1952   Min.   :23.60  \n Albania    :  12   Americas:300   1st Qu.:1966   1st Qu.:48.20  \n Algeria    :  12   Asia    :396   Median :1980   Median :60.71  \n Angola     :  12   Europe  :360   Mean   :1980   Mean   :59.47  \n Argentina  :  12   Oceania : 24   3rd Qu.:1993   3rd Qu.:70.85  \n Australia  :  12                  Max.   :2007   Max.   :82.60  \n (Other)    :1632                                                \n      pop              gdpPercap       \n Min.   :6.001e+04   Min.   :   241.2  \n 1st Qu.:2.794e+06   1st Qu.:  1202.1  \n Median :7.024e+06   Median :  3531.8  \n Mean   :2.960e+07   Mean   :  7215.3  \n 3rd Qu.:1.959e+07   3rd Qu.:  9325.5  \n Max.   :1.319e+09   Max.   :113523.1  \n                                       \n\n\nIn this graphic, I want to spotlight China’s development path, and also show the story of differences by continent. First, I’ll show all the data, which makes clear the need for animation of this data.\n\ngapminder %&gt;% \n  ggplot(.,\n         aes(x=gdpPercap,\n             y=lifeExp,\n             size=pop,\n             color=continent)) +\n  geom_point() + \n  geom_text(data=gapminder %&gt;% filter(country == 'China'),\n                           aes(label=country), \n                           family='Arial', size= 3, color='black', nudge_y = 2) + \n  scale_x_log10() + \n  ggtitle('Change in life expectancy and income over time',\n          subtitle = 'Bubble size indicates population size') + \n  labs(x='GDP per capita (logged)\\n',\n       y='Life expectancy (years)',\n       caption='Source: Gapminder dataset') + \n  theme(legend.position = 'top',\n        legend.title = element_blank()) + \n  guides(size='none')\n\n\n\n\n\n\n\n\n\n\n\n\ngapminder %&gt;% \n  ggplot(.,\n         aes(x=gdpPercap,\n             y=lifeExp,\n             size=pop,\n             color=continent)) +\n  geom_point() + \n  geom_text(data=gapminder %&gt;% filter(country == 'China'),\n                           aes(label=country), \n                           family='Arial', size= 3, color='black', nudge_y = 2) + \n  geom_text(aes(x=min(gdpPercap), \n                y=min(lifeExp),\n                label=as.factor(year)), \n            hjust=-3.5, vjust=-0.2, alpha=0.2, color='gray70', size=20) +\n  scale_x_log10() + \n  ggtitle('Change in life expectancy and income over time',\n          subtitle = 'Bubble size indicates population size') + \n  labs(x='GDP per capita (logged)\\n',\n       y='Life expectancy (years)',\n       caption='Source: Gapminder dataset') + \n  theme(legend.position = 'top') + \n  guides(size='none') + \n  my.theme + \n  transition_states(\n    year,\n    transition_length = 2,\n    state_length = 0,\n    wrap = TRUE\n  ) + \n  enter_fade() + \n  exit_shrink() +\n  ease_aes()\n\n\n\n\n\n\n\n\n\n\n\n\ngapminder %&gt;% \n  ggplot(.,\n         aes(x=gdpPercap,\n             y=lifeExp,\n             size=pop,\n             color=continent)) +\n  geom_point() + \n  geom_text(data=gapminder %&gt;% filter(country == 'China'),\n                           aes(label=country), \n                           family='Arial', size= 3, color='black', nudge_y = 2) + \n  geom_text(aes(x=min(gdpPercap), \n                y=min(lifeExp),\n                label=as.factor(year)), \n            hjust=-3.5, vjust=-0.2, alpha=0.2, color='gray70', size=20) +\n  scale_x_log10() + \n  ggtitle('Change in life expectancy and income over time',\n          subtitle = 'Bubble size indicates population size') + \n  labs(x='GDP per capita (logged)\\n',\n       y='Life expectancy (years)',\n       caption='Source: Gapminder dataset') + \n  theme(legend.position = 'top') + \n  guides(size='none') + \n  my.theme + \n  transition_states(\n    year,\n    transition_length = 2,\n    state_length = 0,\n    wrap = TRUE\n  ) + \n  shadow_wake(wake_length = 0.1, alpha = FALSE, exclude_layer = c(2,3)) + \n  enter_fade() + \n  exit_shrink() +\n  ease_aes()"
  },
  {
    "objectID": "posts/2017-10-15-gapminder-gganimate/index.html#purpose",
    "href": "posts/2017-10-15-gapminder-gganimate/index.html#purpose",
    "title": "Animating the Gapminder dataset using gganimate",
    "section": "",
    "text": "In this project, I wanted to experiment with gganimate:: to reproduce the classic animated visualization by Hans Rosling of Gapminder data."
  },
  {
    "objectID": "posts/2017-10-15-gapminder-gganimate/index.html#dataset",
    "href": "posts/2017-10-15-gapminder-gganimate/index.html#dataset",
    "title": "Animating the Gapminder dataset using gganimate",
    "section": "",
    "text": "The Gapminder dataset, compiled by the Gapminder Foundation, provides data on global development trends, including indicators like life expectancy, GDP per capita, and population. The dataset is known for its use in dynamic visualizations that illustrate changes in global indicators over time."
  },
  {
    "objectID": "posts/2017-10-15-gapminder-gganimate/index.html#setup",
    "href": "posts/2017-10-15-gapminder-gganimate/index.html#setup",
    "title": "Animating the Gapminder dataset using gganimate",
    "section": "",
    "text": "country        continent        year         lifeExp     \n Afghanistan:  12   Africa  :624   Min.   :1952   Min.   :23.60  \n Albania    :  12   Americas:300   1st Qu.:1966   1st Qu.:48.20  \n Algeria    :  12   Asia    :396   Median :1980   Median :60.71  \n Angola     :  12   Europe  :360   Mean   :1980   Mean   :59.47  \n Argentina  :  12   Oceania : 24   3rd Qu.:1993   3rd Qu.:70.85  \n Australia  :  12                  Max.   :2007   Max.   :82.60  \n (Other)    :1632                                                \n      pop              gdpPercap       \n Min.   :6.001e+04   Min.   :   241.2  \n 1st Qu.:2.794e+06   1st Qu.:  1202.1  \n Median :7.024e+06   Median :  3531.8  \n Mean   :2.960e+07   Mean   :  7215.3  \n 3rd Qu.:1.959e+07   3rd Qu.:  9325.5  \n Max.   :1.319e+09   Max.   :113523.1  \n                                       \n\n\nIn this graphic, I want to spotlight China’s development path, and also show the story of differences by continent. First, I’ll show all the data, which makes clear the need for animation of this data.\n\ngapminder %&gt;% \n  ggplot(.,\n         aes(x=gdpPercap,\n             y=lifeExp,\n             size=pop,\n             color=continent)) +\n  geom_point() + \n  geom_text(data=gapminder %&gt;% filter(country == 'China'),\n                           aes(label=country), \n                           family='Arial', size= 3, color='black', nudge_y = 2) + \n  scale_x_log10() + \n  ggtitle('Change in life expectancy and income over time',\n          subtitle = 'Bubble size indicates population size') + \n  labs(x='GDP per capita (logged)\\n',\n       y='Life expectancy (years)',\n       caption='Source: Gapminder dataset') + \n  theme(legend.position = 'top',\n        legend.title = element_blank()) + \n  guides(size='none')"
  },
  {
    "objectID": "posts/2017-10-15-gapminder-gganimate/index.html#gganimate",
    "href": "posts/2017-10-15-gapminder-gganimate/index.html#gganimate",
    "title": "Animating the Gapminder dataset using gganimate",
    "section": "",
    "text": "gapminder %&gt;% \n  ggplot(.,\n         aes(x=gdpPercap,\n             y=lifeExp,\n             size=pop,\n             color=continent)) +\n  geom_point() + \n  geom_text(data=gapminder %&gt;% filter(country == 'China'),\n                           aes(label=country), \n                           family='Arial', size= 3, color='black', nudge_y = 2) + \n  geom_text(aes(x=min(gdpPercap), \n                y=min(lifeExp),\n                label=as.factor(year)), \n            hjust=-3.5, vjust=-0.2, alpha=0.2, color='gray70', size=20) +\n  scale_x_log10() + \n  ggtitle('Change in life expectancy and income over time',\n          subtitle = 'Bubble size indicates population size') + \n  labs(x='GDP per capita (logged)\\n',\n       y='Life expectancy (years)',\n       caption='Source: Gapminder dataset') + \n  theme(legend.position = 'top') + \n  guides(size='none') + \n  my.theme + \n  transition_states(\n    year,\n    transition_length = 2,\n    state_length = 0,\n    wrap = TRUE\n  ) + \n  enter_fade() + \n  exit_shrink() +\n  ease_aes()"
  },
  {
    "objectID": "posts/2017-10-15-gapminder-gganimate/index.html#using-the-gganimateshadow_wake-function-to-leave-a-trail",
    "href": "posts/2017-10-15-gapminder-gganimate/index.html#using-the-gganimateshadow_wake-function-to-leave-a-trail",
    "title": "Animating the Gapminder dataset using gganimate",
    "section": "",
    "text": "gapminder %&gt;% \n  ggplot(.,\n         aes(x=gdpPercap,\n             y=lifeExp,\n             size=pop,\n             color=continent)) +\n  geom_point() + \n  geom_text(data=gapminder %&gt;% filter(country == 'China'),\n                           aes(label=country), \n                           family='Arial', size= 3, color='black', nudge_y = 2) + \n  geom_text(aes(x=min(gdpPercap), \n                y=min(lifeExp),\n                label=as.factor(year)), \n            hjust=-3.5, vjust=-0.2, alpha=0.2, color='gray70', size=20) +\n  scale_x_log10() + \n  ggtitle('Change in life expectancy and income over time',\n          subtitle = 'Bubble size indicates population size') + \n  labs(x='GDP per capita (logged)\\n',\n       y='Life expectancy (years)',\n       caption='Source: Gapminder dataset') + \n  theme(legend.position = 'top') + \n  guides(size='none') + \n  my.theme + \n  transition_states(\n    year,\n    transition_length = 2,\n    state_length = 0,\n    wrap = TRUE\n  ) + \n  shadow_wake(wake_length = 0.1, alpha = FALSE, exclude_layer = c(2,3)) + \n  enter_fade() + \n  exit_shrink() +\n  ease_aes()"
  },
  {
    "objectID": "posts/2024-09-30-cdcplaces/index.html",
    "href": "posts/2024-09-30-cdcplaces/index.html",
    "title": "Mapping depression and food insecurity in the U.S. with the CDC PLACES project",
    "section": "",
    "text": "I saw a post recently about the Centers for Disease Control’s Population Level Analysis and Community Estimates (or “CDC PLACES”) project and its latest data release. The details of this project caught my attention, because it’s a nation-wide project that measures and estimates key population health indicators, including metrics around prevention and the social determinants of health.1 Here is how the CDC describes it on their project website:\n\nPLACES is a collaboration between CDC, the Robert Wood Johnson Foundation, and the CDC Foundation. PLACES provides health data for small areas across the country. This allows local health departments and jurisdictions, regardless of population size and rurality, to better understand the burden and geographic distribution of health measures in their areas and assist them in planning public health interventions.\n\n\n\nI found that there is a CDCPLACES:: R package that facilitates API access to the project’s data, so I wanted to explore what’s available, and what it’s like to fetch, analyze, and visualize data from the PLACES project.\nIn future posts, I plan to do a more formal analysis, but for now – I wanted to get to know this package and the available data.\n\n\n\n\nlibrary(tidyverse)\nlibrary(CDCPLACES)\n1library(ggtext)\nlibrary(glue)\n\n\n1\n\nI use the ggtext::element_textbox_simple() function to bold some (but not all) text in a plot title. The ggtext:: package has lots of neat functionality for ggplot graphics.\n\n\n\n\nThe CDCPLACES:: package includes only two functions:\n\nget_dictionary(): shows indicators and some metadata for each indicator (measure name, release year, etc.)\nget_places(): fetches the data from the CDC PLACES API\n\n\nget_dictionary() %&gt;% select(1:2) %&gt;% head(10)\n\n    measureid                                         measure_full_name\n1   ARTHRITIS                                    Arthritis among adults\n2      BPHIGH                          High blood pressure among adults\n3      CANCER                Cancer (non-skin) or melanoma among adults\n4     CASTHMA                               Current asthma among adults\n5         CHD                       Coronary heart disease among adults\n6        COPD        Chronic obstructive pulmonary disease among adults\n7  DEPRESSION                                   Depression among adults\n8    DIABETES                           Diagnosed diabetes among adults\n9    HIGHCHOL High cholesterol among adults who have ever been screened\n10     KIDNEY       Chronic kidney disease among adults aged &gt;=18 years\n\n\nFrom the get_dictionary() function, you can see that there are 44 records (representing 44 different measures) from health outcomes, risk behaviors, status, prevention, disability, and health-related social needs (to preserve space, I’m only showing the top 10 records). It’s a really exciting set of metrics for evaluating patterns of morbidity and health-related metrics (such as preventative metrics or the social determinants of health).\nWithin the get_places() function, there are the following arguments:\n\ngeography=: argument that takes county, census, or zcta\nstate=: takes in single state abbreviations (two-letter abbreviations), as well as vectors of multiple states\nmeasure=: code for the field(s) that you want to fetch (use get_dictionary()) for a complete list of measures)\nrelease=: takes in string for year of data that you want (available for years 2020-2023)\ngeometry=: returns an sf:: field that facilitates easy mapping (FALSE by default)\ncat=: instead of using the measure= argument, you can fetch entire categories of measures.\nage_adjust=: age adjustment is typically important for cross-regional/country comparisons. Age is a key determinant for risk of health conditions (e.g. older populations are more likely to experience dimentia). Adjusting for age differences across geographies can make for a more accurate comparison of prevalence."
  },
  {
    "objectID": "posts/2024-09-30-cdcplaces/index.html#purpose",
    "href": "posts/2024-09-30-cdcplaces/index.html#purpose",
    "title": "Mapping depression and food insecurity in the U.S. with the CDC PLACES project",
    "section": "",
    "text": "I found that there is a CDCPLACES:: R package that facilitates API access to the project’s data, so I wanted to explore what’s available, and what it’s like to fetch, analyze, and visualize data from the PLACES project.\nIn future posts, I plan to do a more formal analysis, but for now – I wanted to get to know this package and the available data."
  },
  {
    "objectID": "posts/2024-09-30-cdcplaces/index.html#setup-and-the-cdcplaces-package",
    "href": "posts/2024-09-30-cdcplaces/index.html#setup-and-the-cdcplaces-package",
    "title": "Mapping depression and food insecurity in the U.S. with the CDC PLACES project",
    "section": "",
    "text": "library(tidyverse)\nlibrary(CDCPLACES)\n1library(ggtext)\nlibrary(glue)\n\n\n1\n\nI use the ggtext::element_textbox_simple() function to bold some (but not all) text in a plot title. The ggtext:: package has lots of neat functionality for ggplot graphics.\n\n\n\n\nThe CDCPLACES:: package includes only two functions:\n\nget_dictionary(): shows indicators and some metadata for each indicator (measure name, release year, etc.)\nget_places(): fetches the data from the CDC PLACES API\n\n\nget_dictionary() %&gt;% select(1:2) %&gt;% head(10)\n\n    measureid                                         measure_full_name\n1   ARTHRITIS                                    Arthritis among adults\n2      BPHIGH                          High blood pressure among adults\n3      CANCER                Cancer (non-skin) or melanoma among adults\n4     CASTHMA                               Current asthma among adults\n5         CHD                       Coronary heart disease among adults\n6        COPD        Chronic obstructive pulmonary disease among adults\n7  DEPRESSION                                   Depression among adults\n8    DIABETES                           Diagnosed diabetes among adults\n9    HIGHCHOL High cholesterol among adults who have ever been screened\n10     KIDNEY       Chronic kidney disease among adults aged &gt;=18 years\n\n\nFrom the get_dictionary() function, you can see that there are 44 records (representing 44 different measures) from health outcomes, risk behaviors, status, prevention, disability, and health-related social needs (to preserve space, I’m only showing the top 10 records). It’s a really exciting set of metrics for evaluating patterns of morbidity and health-related metrics (such as preventative metrics or the social determinants of health).\nWithin the get_places() function, there are the following arguments:\n\ngeography=: argument that takes county, census, or zcta\nstate=: takes in single state abbreviations (two-letter abbreviations), as well as vectors of multiple states\nmeasure=: code for the field(s) that you want to fetch (use get_dictionary()) for a complete list of measures)\nrelease=: takes in string for year of data that you want (available for years 2020-2023)\ngeometry=: returns an sf:: field that facilitates easy mapping (FALSE by default)\ncat=: instead of using the measure= argument, you can fetch entire categories of measures.\nage_adjust=: age adjustment is typically important for cross-regional/country comparisons. Age is a key determinant for risk of health conditions (e.g. older populations are more likely to experience dimentia). Adjusting for age differences across geographies can make for a more accurate comparison of prevalence."
  },
  {
    "objectID": "posts/2024-09-30-cdcplaces/index.html#social-needs-a-new-category-of-data-from-the-cdc-places-project",
    "href": "posts/2024-09-30-cdcplaces/index.html#social-needs-a-new-category-of-data-from-the-cdc-places-project",
    "title": "Mapping depression and food insecurity in the U.S. with the CDC PLACES project",
    "section": "Social Needs – a new category of data from the CDC PLACES project",
    "text": "Social Needs – a new category of data from the CDC PLACES project\nThe September 2024 release from CDC PLACES includes a new category of social needs measures, which include:\n\nSocial isolation\nFood stamps\nFood insecurity\nHousing insecurity\nUtilities services threat\nTransportation barriers\nLack of social/emotional support\n\nInstead of using the measure= argument (as I did above), I’ll use the cat= argument to pull in all social needs indicators from the PLACES project.\n\ndf_social_needs &lt;- get_places(geography = 'county',\n                 cat = 'SOCLNEED',\n                 release = '2024',\n                 geometry = TRUE)\n\nFor this first release of social needs indicators, there are only 39 states included.2\n\ndf_social_needs %&gt;% \n  filter(data_value_type == 'Crude prevalence',\n         measureid == 'FOODINSECU',\n         !stateabbr %in% c('AK', 'HI')) %&gt;%\n  ggplot(.) + \n  geom_sf(aes(fill=data_value/100)) + \n  coord_sf(datum = NA) +\n  ggtitle('Food insecurity in the past 12 months among adults') +\n  scale_fill_viridis_c(option = 'magma',\n                       labels=scales::percent,\n                       name='') +\n  theme_minimal()\n\n\n\n\n\n\n\n\nI also like to visualize the error bands associated with point estimates. The geom_errorborh() function comes in handy for that. I’ll visualize the top 25 food insecure counties, along with their confidence intervals.\n\ndf_social_needs %&gt;% \n  filter(data_value_type == 'Crude prevalence',\n         measureid == 'FOODINSECU') %&gt;% \n  top_n(25, data_value) %&gt;% \n  ggplot(.,\n         aes(y=reorder(paste(locationname, statedesc, sep=\", \"), data_value),\n             color=if_else(statedesc == 'Texas', 'Texas', 'Other'))) + \n  geom_point(aes(x=data_value/100)) + \n  geom_errorbarh(aes(xmin=low_confidence_limit/100, \n                     xmax=high_confidence_limit/100), \n                 height = 0.4) +\n  ggtitle('% of adults experiencing **food insecurity** in the past 12 months',\n          subtitle = glue(\"&lt;br&gt;25 counties with the highest rates in 2024&lt;br&gt;&lt;span style='color:#cd2626'&gt;**Texas**&lt;/span&gt; has nine counties in the top 25\")) + \n  \n  labs(x='',\n       y='',\n       caption='Source: Centers for Disease Control PLACES project\\nPoint estimates and confidence intervals shown') + \n  scale_x_continuous(labels = scales::percent,\n                     position = 'top') + \n  scale_color_manual(values = c('Texas' = 'firebrick3',\n                                'Other' = 'black')) + \n  theme_minimal() + \n  theme(plot.title = element_textbox_simple(size=16),\n        plot.subtitle = element_textbox_simple(),\n        legend.position = 'none')\n\n\n\n\n\n\n\n\nAnd I’ll visualize the food insecurity data from my home state, West Viriginia, where parts of central and southern West Virginia experience the highest rates of food insecurity in the state.\n\ndf_social_needs %&gt;% \n  filter(measureid == 'FOODINSECU',\n         statedesc == 'West Virginia') %&gt;%\n  ggplot(.) + \n  geom_sf(aes(fill=data_value/100)) + \n  coord_sf(datum = NA) + \n  ggtitle('% of adults experiencing food insecurity in the last 12 months') +\n  scale_fill_viridis_c(option = 'magma',\n                       labels=scales::percent,\n                       name='') +\n  guides(color='none') + \n  theme_minimal()"
  },
  {
    "objectID": "posts/2024-09-30-cdcplaces/index.html#footnotes",
    "href": "posts/2024-09-30-cdcplaces/index.html#footnotes",
    "title": "Mapping depression and food insecurity in the U.S. with the CDC PLACES project",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAt the time of this review, Arkansas, Colorado, Connecticut, Illinois, Louisiana, New York, North Dakota, Oregon, Pennsylvania, South Dakota, and Virginia are not included in the social needs indicators in CDCPLACES.↩︎\nAt the time of this review, Arkansas, Colorado, Connecticut, Illinois, Louisiana, New York, North Dakota, Oregon, Pennsylvania, South Dakota, and Virginia are not included in the social needs indicators in CDCPLACES.↩︎"
  },
  {
    "objectID": "workshops/2018-05-15-korbel-skills-tidyverse-part2/index.html",
    "href": "workshops/2018-05-15-korbel-skills-tidyverse-part2/index.html",
    "title": "PART TWO: Data Manipulation and Visualization Basics using R",
    "section": "",
    "text": "This is a continuation of the “Data Manipulation and Visualization Basics using R” workshop, which was originally delivered in 2018. This post assumes that you know the content from Part 1."
  },
  {
    "objectID": "workshops/2018-05-15-korbel-skills-tidyverse-part2/index.html#footnotes",
    "href": "workshops/2018-05-15-korbel-skills-tidyverse-part2/index.html#footnotes",
    "title": "PART TWO: Data Manipulation and Visualization Basics using R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.tidyverse.org/packages/↩︎\nhttps://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html↩︎\nhttp://scdb.wustl.edu/data.php↩︎"
  },
  {
    "objectID": "workshops/2018-05-15-korbel-skills-tidyverse/index.html",
    "href": "workshops/2018-05-15-korbel-skills-tidyverse/index.html",
    "title": "PART ONE: Data Manipulation and Visualization Basics using R",
    "section": "",
    "text": "This workshop session will help you familiarize with two of R’s most widely used packages, dplyr and ggplot2. These packages are part of what is called the tidyverse, which is a collection of packages that are designed to work together in R to simplify and data manipulation, discovery, visualization, analysis, and more.1\n\n\n\n\nEach tile is a package that makes up the “tidyverse”\n\n\n\ndplyr is designed to standardize the data management and manipulation process in R. dplyr has a small set of functions that help users navigate the most common challenges of data manipulation.2 Generally speaking, these include restructuring, filtering, grouping, and summarizing your data, as well as creating new variables for your analysis.\nggplot2 is a package for visualizing data. In this lesson, we will learn the building blocks for creating beautiful visualizations. We will learn how to produce histograms, scatter plots, and line plots, and we will visualize ungrouped and grouped data.\ndplyr and ggplot2 are packages that supply a grammar for data manipulation and data visualization, respectively. This workshop will only scratch the surface of what is possible, but learning the fundamentals of these packages can empower you to do a ton of exciting things in R."
  },
  {
    "objectID": "workshops/2018-05-15-korbel-skills-tidyverse/index.html#workshop-format-and-key-terms",
    "href": "workshops/2018-05-15-korbel-skills-tidyverse/index.html#workshop-format-and-key-terms",
    "title": "PART ONE: Data Manipulation and Visualization Basics using R",
    "section": "Workshop format and key terms",
    "text": "Workshop format and key terms\nTwo quick notes on the format of this workshop:\n\nThere are links to additional resources in the end notes of this document. I have personally used most of these resources to build my skills in R, so I hope you will find them useful, too.\n\nThroughout the workshop, there will be exercises to complete on your own or with your table mates. Do you best to complete these based on what you’ve learned. If you get stuck, you can click the “Code” button on the right to show the code that I would use to answer the question. Use that to test yourself and figure out why it works.\n\nThere are a few key terms that we’ll use throughout this session.\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nFunctions\nYou give R an input, it generates an output. A give-away that you’re working with a function is parentheses.\n\n\nArguments\nFunctions take arguments as their input. If you’re ever unsure of which arguments that a function takes, you can type args(“function name”), ?(“function name”), or help(“function name”).\n\n\nPackages\nA collection of code, functions, documentation, and occasionally data, created and curated by users of the R community.\n\n\nWorking directory\nThis is the location on your computer where all files relevant to your project are saved. Any files that are imported to or exported from your analysis should be here. Think of this as your project’s home base.\n\n\nObject\nR is an object-oriented programming language, meaning that you can assign meaning to a user-defined string that you can then manipulate or visualize. In R, you can assign objects with the “&lt;-” assignment operator. It can be a single value, list of values, a full data set, a function, etc. In this session, the primary object that we’ll use is the gapminder dataset."
  },
  {
    "objectID": "workshops/2018-05-15-korbel-skills-tidyverse/index.html#setting-up-your-r-project-environment",
    "href": "workshops/2018-05-15-korbel-skills-tidyverse/index.html#setting-up-your-r-project-environment",
    "title": "PART ONE: Data Manipulation and Visualization Basics using R",
    "section": "Setting up your R Project environment",
    "text": "Setting up your R Project environment\nTo get started, we need to set up our new project environment, with the following steps:\n\nOpen R Studio, and go to File –&gt; New Project.\n\n\n\n\n 2. Select New Directory.\n\n\n\n 3. Select New Project.\n\n\n\n 4. Type in a name for your project under “Directory name”. Choose the location where you want this R Project to live on your computer. Select Create Project.\n\n\n\n These steps have established a directory on your computer where your R project lives. This is a very good practice to keep when you are starting a new analysis. You will keep all related files in this project folder, and R will know that this is your working directory (where all files relevant to your project are saved).\n\nFinally, go to File –&gt; New File –&gt; R Notebook. R notebook files are an excellent way to perform an analysis, because they allow for R code and documentation to work side-by-side. This enables you to tell a story that moves alongside your quantitative analysis, and it can be output in all kinds of formats, including Word documents, .pdfs, and .html files.\n\n\n\n\n We could do a full session on R projects and notebooks, but this should be sufficient to get you started."
  },
  {
    "objectID": "workshops/2018-05-15-korbel-skills-tidyverse/index.html#distinct",
    "href": "workshops/2018-05-15-korbel-skills-tidyverse/index.html#distinct",
    "title": "PART ONE: Data Manipulation and Visualization Basics using R",
    "section": "distinct()",
    "text": "distinct()\nTo begin our anlaysis, let’s look at the years that are available in the gapminder dataset. There are many ways to do this, but I’ll use the distinct() function from dplyr.\nCreate a new code chunk (Ctrl + Alt + I), and type gapminder %&gt;% distinct(year). Then, execute the code chunk (with the green play button in the upper righthand corner of the chunk).\n\ngapminder %&gt;% \n  distinct(year)\n\n# A tibble: 12 × 1\n    year\n   &lt;int&gt;\n 1  1952\n 2  1957\n 3  1962\n 4  1967\n 5  1972\n 6  1977\n 7  1982\n 8  1987\n 9  1992\n10  1997\n11  2002\n12  2007\n\n\nThis shows us that we have data that are in five-year increments from 1952 to 2007.\nBefore we move on, let’s take a closer look at the code above, because it uses a style that we will be fluent in by the end of this session. This code uses “piping” (the %&gt;%), which is a coding style that makes code more readable and easier to troubleshoot.3 The “pipe” (%&gt;%) tells R to take the object to the left of the pipe and feed it to the first argument in the code following the pipe. The code above can be read as:\n\nTake the gapminder dataframe and feed it into the distinct() function, THEN\n\nReturn the distinct or unique values in the year column\n\nWhen reading your code, you can think of the pipe operator (%&gt;%) as “THEN”. To use the above example, take the gapminder dataframe, THEN return the distinct values in the year column.\nLet’s look at the distinct continents in the gapminder dataset. In your code chunk, relace distinct(year) with distinct(continent). Then, with your cursor on the same line as the code, use the keyboard shortcut for running a block of code (Ctrl + Enter).\n\ngapminder %&gt;% \n  distinct(continent)\n\n# A tibble: 5 × 1\n  continent\n  &lt;fct&gt;    \n1 Asia     \n2 Europe   \n3 Africa   \n4 Americas \n5 Oceania  \n\n\nThis is how the code looks without piping. Not too difficult (yet).\n\ndistinct(gapminder, continent)\n\nThis simple example does not convey the full value of piping, but that will be clear by the end of the session."
  },
  {
    "objectID": "workshops/2018-05-15-korbel-skills-tidyverse/index.html#filter",
    "href": "workshops/2018-05-15-korbel-skills-tidyverse/index.html#filter",
    "title": "PART ONE: Data Manipulation and Visualization Basics using R",
    "section": "filter()",
    "text": "filter()\n\n\n\nThe filter() function allows you to subset your data by values of the variables in your dataset.\nWe’re going to use piping a lot in this session, so let’s get comfortable with it. Try using the pipe operator keyboard shortcut (Ctrl + Shift + M) with your code from this point forward.\nLet’s filter the gapminder dataframe to look at only the values in the year 2007.\n\ngapminder %&gt;% \n  filter(year == 2007)\n\n# A tibble: 142 × 6\n   country     continent  year lifeExp       pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;     &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       2007    43.8  31889923      975.\n 2 Albania     Europe     2007    76.4   3600523     5937.\n 3 Algeria     Africa     2007    72.3  33333216     6223.\n 4 Angola      Africa     2007    42.7  12420476     4797.\n 5 Argentina   Americas   2007    75.3  40301927    12779.\n 6 Australia   Oceania    2007    81.2  20434176    34435.\n 7 Austria     Europe     2007    79.8   8199783    36126.\n 8 Bahrain     Asia       2007    75.6    708573    29796.\n 9 Bangladesh  Asia       2007    64.1 150448339     1391.\n10 Belgium     Europe     2007    79.4  10392226    33693.\n# ℹ 132 more rows\n\n\nThis returns the 142 records (or rows) that are in the year 2007.\nWhat’s the double equal sign (“==”) mean? This is a boolean equal, meaning that it is evaluating whether the equation returns a TRUE or FALSE. The above code can be read as “filter, return rows in which the variable year equals 2007”. All rows that would return a TRUE for “year equals 2007” are displayed.\nYou can also use the filter() command on multiple conditions, too. Just separate the conditions with a comma. The code below takes the gapminder dataframe, and filters the data in which the country is South Africa and the year is 2007.\n\ngapminder %&gt;% \n  filter(country == 'South Africa',\n         year == 2007)\n\n# A tibble: 1 × 6\n  country      continent  year lifeExp      pop gdpPercap\n  &lt;fct&gt;        &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n1 South Africa Africa     2007    49.3 43997828     9270.\n\n\nThis shows that the dataset has only one occurrence of South Africa in 2007. Note that in dplyr, separating arguments with a comma allows you to combine multiple logical expressions. How would you read the above code aloud?\nOne thing to keep in mind – your data object remains unchanged, unless you assign the filter() to a new object. To save the above code as a new object, you need to assign it to a new named object, such as:\n\n\nsa_2007 &lt;- gapminder %&gt;% filter(country == ‘South Africa’, year == 2007)"
  },
  {
    "objectID": "workshops/2018-05-15-korbel-skills-tidyverse/index.html#arrange",
    "href": "workshops/2018-05-15-korbel-skills-tidyverse/index.html#arrange",
    "title": "PART ONE: Data Manipulation and Visualization Basics using R",
    "section": "arrange()",
    "text": "arrange()\nThe arrange() function sorts your data – a very common thing to do in an exploratory data analysis. Let’s arrange the gapminder data by life expectancy (the variable is called lifeExp).\n\ngapminder %&gt;% \n  arrange(lifeExp)\n\n# A tibble: 1,704 × 6\n   country      continent  year lifeExp     pop gdpPercap\n   &lt;fct&gt;        &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;   &lt;int&gt;     &lt;dbl&gt;\n 1 Rwanda       Africa     1992    23.6 7290203      737.\n 2 Afghanistan  Asia       1952    28.8 8425333      779.\n 3 Gambia       Africa     1952    30    284320      485.\n 4 Angola       Africa     1952    30.0 4232095     3521.\n 5 Sierra Leone Africa     1952    30.3 2143249      880.\n 6 Afghanistan  Asia       1957    30.3 9240934      821.\n 7 Cambodia     Asia       1977    31.2 6978607      525.\n 8 Mozambique   Africa     1952    31.3 6446316      469.\n 9 Sierra Leone Africa     1957    31.6 2295678     1004.\n10 Burkina Faso Africa     1952    32.0 4469979      543.\n# ℹ 1,694 more rows\n\n\nThis shows us that Rwanda in 1992 is the lowest life expectancy in the dataset. By default, the arrange() function sorts your data in ascending order. What if we want to sort the data by the largest GDP per capita (variable is gdpPercap) in the dataset?\n\ngapminder %&gt;% \n  arrange(-gdpPercap)\n\n# A tibble: 1,704 × 6\n   country   continent  year lifeExp     pop gdpPercap\n   &lt;fct&gt;     &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;   &lt;int&gt;     &lt;dbl&gt;\n 1 Kuwait    Asia       1957    58.0  212846   113523.\n 2 Kuwait    Asia       1972    67.7  841934   109348.\n 3 Kuwait    Asia       1952    55.6  160000   108382.\n 4 Kuwait    Asia       1962    60.5  358266    95458.\n 5 Kuwait    Asia       1967    64.6  575003    80895.\n 6 Kuwait    Asia       1977    69.3 1140357    59265.\n 7 Norway    Europe     2007    80.2 4627926    49357.\n 8 Kuwait    Asia       2007    77.6 2505559    47307.\n 9 Singapore Asia       2007    80.0 4553009    47143.\n10 Norway    Europe     2002    79.0 4535591    44684.\n# ℹ 1,694 more rows\n\n#you could also do...\n#gapminder %&gt;% \n#  arrange(desc(gdpPercap))\n\n#or if you wanted to return the top X GDP per capita records\n#you could use the top_n() function from dplyr\n#gapminder %&gt;%\n#  arrange(-gdpPercap) %&gt;% \n#  top_n(15, gdpPercap)\n\n\nExercises:\n\nYou can also arrange by multiple variables with the arrange() function. Let’s say that you want to arrange the gapminder data by year and by life expectancy. Try using gapminder %&gt;% arrange(year, lifeExp). What do you see?\n\n\n\ngapminder %&gt;% \n  arrange(year, lifeExp)\n\n\nLet’s put the filter() and arrange() functions together. I’d like to see the life expectancy of all Asian countries from the year 2000 to the most recent year, sorted in ascending order. How might we do that?\n\n\n\ngapminder %&gt;% \n  filter(year &gt;= 2000,\n         continent == 'Asia') %&gt;% \n  arrange(lifeExp)\n\n\nWhen you are combining multiple functions, the power and clarity of piping becomes more obvious. This is what the previous code would look like without piping.\n\n\n\narrange(filter(gapminder, year &gt;= 2000, continent == ‘Asia’), lifeExp)\n\n\nWho would like to try to read this?\n\n\nWithout piping, you need to write this code inside-out! Does anyone actually find this easier?!"
  },
  {
    "objectID": "workshops/2018-05-15-korbel-skills-tidyverse/index.html#mutate",
    "href": "workshops/2018-05-15-korbel-skills-tidyverse/index.html#mutate",
    "title": "PART ONE: Data Manipulation and Visualization Basics using R",
    "section": "mutate()",
    "text": "mutate()\n\n\n\nIn data analysis, it’s common to want to create or derive new variables from your original dataset. This is done with the dplyr::mutate() function. For example, you might want to create a variable that is the distance in time from the beginning of the dataset. This is a typical derived variable used in regression modeling.\nFor these new variables, let’s save this as a new data object.\n\ngapminder &lt;- gapminder %&gt;% \n  mutate(time_dist = year - 1952)\n\nprint(gapminder)\n\n# A tibble: 1,704 × 7\n   country     continent  year lifeExp      pop gdpPercap time_dist\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.         0\n 2 Afghanistan Asia       1957    30.3  9240934      821.         5\n 3 Afghanistan Asia       1962    32.0 10267083      853.        10\n 4 Afghanistan Asia       1967    34.0 11537966      836.        15\n 5 Afghanistan Asia       1972    36.1 13079460      740.        20\n 6 Afghanistan Asia       1977    38.4 14880372      786.        25\n 7 Afghanistan Asia       1982    39.9 12881816      978.        30\n 8 Afghanistan Asia       1987    40.8 13867957      852.        35\n 9 Afghanistan Asia       1992    41.7 16317921      649.        40\n10 Afghanistan Asia       1997    41.8 22227415      635.        45\n# ℹ 1,694 more rows\n\n\n\nExercises:\n\nUse dplyr::mutate() to create two other variables – GDP and population (in millions). Note that just like the filter() function, which allows you to have multiple conditions, you can also create multiple new variables by using a comma within mutate().\n\n\ngapminder &lt;- gapminder %&gt;% \n  mutate(gdp = gdpPercap * pop,\n         pop = pop/1000000)\n\n\nWhat are the countries with the largest total GDP in 2007?\n\n\ngapminder %&gt;% \n  filter(year == 2007) %&gt;% \n  arrange(desc(gdp))"
  },
  {
    "objectID": "workshops/2018-05-15-korbel-skills-tidyverse/index.html#summarise",
    "href": "workshops/2018-05-15-korbel-skills-tidyverse/index.html#summarise",
    "title": "PART ONE: Data Manipulation and Visualization Basics using R",
    "section": "summarise()",
    "text": "summarise()\nIn your own anlayses, you’ve probably calculated summary stats for variables. Earlier, we used summary(gapminder) to get an overview of the gapminder dataset, based on some preset functions. summarise() can produce the same summary statistics as summary(), but you have total control over the output (it can do some neat things with grouped data, too, which you will see later). Think of summarise() as useful when you want to condense many rows of data down into a smaller number of rows with summary information.\n\n\n\nThe summarise() function follows the same syntax as mutate(), which is used for creating or deriving new variables, too. The difference is that summarise() is creating a new grouped variable with an aggregate function. Using the gapminder dataset as an example, what is the mean life expectancy from the dataset?\n\ngapminder %&gt;% \n  summarise(meanlifeExp = mean(lifeExp))\n\n# A tibble: 1 × 1\n  meanlifeExp\n        &lt;dbl&gt;\n1        59.5\n\n\nThis code is returning the average life expectancy for all countries and years in the dataset.\n\nExercise:\n\nWhat happens if you use mutate() instead of summarise() to calculate the average life expectancy for the data? Why is that different from using summarise()?\n\n\ngapminder %&gt;% \n  mutate(meanlifeExp = mean(lifeExp))\n\n\nThis may be useful for your research question, but it is far more likely that you’ll want to summarise a variable like lifeExp by a grouping variable. For instance, what is the average life expectancy from this dataset by year? Anytime you want to summarise a variable “BY” another variable, dplyr::group_by() comes in handy.\n\ngapminder %&gt;% \n  group_by(year) %&gt;% \n  summarise(meanlifeExp = mean(lifeExp))\n\n# A tibble: 12 × 2\n    year meanlifeExp\n   &lt;int&gt;       &lt;dbl&gt;\n 1  1952        49.1\n 2  1957        51.5\n 3  1962        53.6\n 4  1967        55.7\n 5  1972        57.6\n 6  1977        59.6\n 7  1982        61.5\n 8  1987        63.2\n 9  1992        64.2\n10  1997        65.0\n11  2002        65.7\n12  2007        67.0\n\n\nThe above code groups the gapminder dataset by the year variable and computes the average life expectancy per year. This shows the gradual advancement in global average life expectancy over time. Pretty neat!\nThe gapminder dataset begs the question: how has the average life expectancy changed over time and across continents? Said another way, how has average life expectancy changed BY year and BY continent? BY is a hint that you want to group your data to answer your question. Just like previous functions that we’ve learned (such as filter() and mutate()), you can group your data by multiple conditions with comma.\n\ngapminder %&gt;% \n  group_by(year, continent) %&gt;% \n  summarise(meanlifeExp = mean(lifeExp))\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 60 × 3\n# Groups:   year [12]\n    year continent meanlifeExp\n   &lt;int&gt; &lt;fct&gt;           &lt;dbl&gt;\n 1  1952 Africa           39.1\n 2  1952 Americas         53.3\n 3  1952 Asia             46.3\n 4  1952 Europe           64.4\n 5  1952 Oceania          69.3\n 6  1957 Africa           41.3\n 7  1957 Americas         56.0\n 8  1957 Asia             49.3\n 9  1957 Europe           66.7\n10  1957 Oceania          70.3\n# ℹ 50 more rows\n\n\nWith this code, we’re able to see some strong variability in average life expectancy by continent. This is worth exploring more closely.\n\nExercises:\nI’d like to create a new grouped summary dataframe with the gapminder dataset. Group gapminder by year and continent, and generate the following summary calculations:\n\nThe total population (sum())\n\nThe mean life expectancy (mean())\nThe mean GDP per capita (mean())\n\nThe count of countries in the dataset in each continent/year pair (n())\n\nSave this to a new object called gapminder_by_year_continent.\n\ngapminder_by_year_continent &lt;- gapminder %&gt;% \n  group_by(year, continent) %&gt;% \n  summarise(totalPop = sum(pop), \n            meanlifeExp = mean(lifeExp),\n            meanGdpPercap = mean(gdpPercap),\n            count_countries = n())\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\nNow, answer the following questions:\n\nWhich continent had the lowest mean life expectancy in 1982?\n\n\ngapminder_by_year_continent %&gt;% \n  filter(year == 1982) %&gt;% \n  arrange(meanlifeExp)\n\n\nHow many countries in Oceania are in the dataset in 2007?\n\n\ngapminder_by_year_continent %&gt;% \n  filter(year == 2007,\n         continent == 'Oceania')\n\n\nWhat was Africa’s mean GDP per capita value in 1987?\n\n\ngapminder_by_year_continent %&gt;% \n  filter(year == 1987,\n         continent == 'Africa')\n\n\nWhat is the difference between summary() and summarise(), and when would you use them?\n\n\nThere’s so much that you can do with the dplyr verbs working together. group_by() is often combined with summarise(), but you can also use it with mutate(). For instance, you could group the gapminder dataset by year and produce within year rankings for life expectancy.\n\ngapminder %&gt;% \n  group_by(year) %&gt;% \n  mutate(lifeExp_rank = rank(desc(lifeExp))) %&gt;% \n  arrange(year, lifeExp_rank)\n\n# A tibble: 1,704 × 9\n# Groups:   year [12]\n   country        continent  year lifeExp    pop gdpPercap time_dist         gdp\n   &lt;fct&gt;          &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n 1 Norway         Europe     1952    72.7  3.33     10095.         0     3.36e10\n 2 Iceland        Europe     1952    72.5  0.148     7268.         0     1.08e 9\n 3 Netherlands    Europe     1952    72.1 10.4       8942.         0     9.28e10\n 4 Sweden         Europe     1952    71.9  7.12      8528.         0     6.08e10\n 5 Denmark        Europe     1952    70.8  4.33      9692.         0     4.20e10\n 6 Switzerland    Europe     1952    69.6  4.82     14734.         0     7.09e10\n 7 New Zealand    Oceania    1952    69.4  1.99     10557.         0     2.11e10\n 8 United Kingdom Europe     1952    69.2 50.4       9980.         0     5.03e11\n 9 Australia      Oceania    1952    69.1  8.69     10040.         0     8.73e10\n10 Canada         Americas   1952    68.8 14.8      11367.         0     1.68e11\n# ℹ 1,694 more rows\n# ℹ 1 more variable: lifeExp_rank &lt;dbl&gt;"
  },
  {
    "objectID": "workshops/2018-05-15-korbel-skills-tidyverse/index.html#histograms",
    "href": "workshops/2018-05-15-korbel-skills-tidyverse/index.html#histograms",
    "title": "PART ONE: Data Manipulation and Visualization Basics using R",
    "section": "Histograms",
    "text": "Histograms\nLet’s begin with histograms:\n\nFirst, using the piping that we learned previously, filter the gapminder data to only include the 2007 rows.\n\nThen, let’s start defining the ggplot2 elements (data, aesthetics, geoms). Click on the “Code” box below to walk through an example.\n\n\ngapminder %&gt;%                           #DATA\n  filter(year == 2007) %&gt;%              #DATA\n  ggplot(aes(x=lifeExp)) +              #AESTHETICS\n  geom_histogram()                      #GEOM\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nLet’s examine the code before we unpack the graphic:\n\nYou’ll recall from the dplyr functions that we’ve learned so far that piping is an effective way to write code that flows between functions. First, we piped the gapminder data to filter(), then we piped that filtered dataframe to ggplot().\n\nWithin ggplot’s functions (ggplot() and geom_histogram()) we always use a plus sign. Think of ggplot as a canvas where the graphic is built by the addition of elements.\n\nThen, we defined the aesthetics (aes()), or what we wanted to graph. In a histogram, you are simply visualizing the distribution of one variable, so you only need to tell ggplot which variable to visualize (within x=).\n\nFinally, we added the geom to the aesthetic, which tells ggplot the type of graphic that we wanted to create (geom_histogram()).\n\nWe’ll get more practice with ggplot’s three elements – data, aesthetics, and geoms – as we go."
  },
  {
    "objectID": "workshops/2018-05-15-korbel-skills-tidyverse/index.html#scatter-plots",
    "href": "workshops/2018-05-15-korbel-skills-tidyverse/index.html#scatter-plots",
    "title": "PART ONE: Data Manipulation and Visualization Basics using R",
    "section": "Scatter plots",
    "text": "Scatter plots\nWhat might explain the variation in life expectancy across countries (think variables from the Gapminder dataset)?\nScatter plots show the relationship between two variables. Let’s create a viz of the relationship between GDP per capita and life expectancy in 2007. Using the previous histogram code as our starting point, we would need to change two things:\n\nSince scatter plots visualize two variables, we need to add a y=gdpPercap to our aesthetic.\n\nInstead of using geom_histogram(), we will use geom_point() for a scatter plot.\n\n\ngapminder %&gt;%                           #DATA\n  filter(year == 2007) %&gt;%              #DATA\n  ggplot(aes(x=gdpPercap,               #AESTHETICS\n                 y=lifeExp)) +          #AESTHETICS\n  geom_point()                          #GEOM\n\n\n\n\n\n\n\n\nThe graphic shows some interesting behavior:\n\nMany countries are clustered below $10,000 per capita, but the range of average life expectancy is quite dramatic.\nThere is a saturating behavior in the data, meaning that at a certain average life expectancy (about 75 and up), income does not seem to have a strong relationship with life expectancy outcomes.\n\nThere appears to be a strong logarithmic relationship between these variables, meaning that the x-axis has a scale that increases in orders of magnitude (1,000 - 10,000 - 100,000).\n\nThe above graphic is very interesting. However, the gapminder dataset has additional information that could be put to use to extract more value from this viz. Let’s see what this looks like when you color each point by the continent. To do this, add color=continent to the aesthetic.\n\ngapminder %&gt;%                         #DATA\n  filter(year == 2007) %&gt;%            #DATA\n  ggplot(aes(x=gdpPercap,             #AESTHETIC\n             y=lifeExp,               #AESTHETIC    \n             color=continent)) +      #AESTHETIC\n  geom_point()                        #GEOM\n\n\n\n\n\n\n\n\nThis illustrates that many of the poorest performing countries are located in Africa.\n\nExercises:\n\nSet color=\"blue\" in the previous code. What do you see? Why do you think that is happening?\n\n\ngapminder %&gt;%                         #DATA\n  filter(year == 2007) %&gt;%            #DATA\n  ggplot(aes(x=gdpPercap,             #AESTHETIC\n             y=lifeExp,               #AESTHETIC\n             color=\"blue\")) +         #AESTHETIC    \n  geom_point()                        #GEOM\n\n\nThere are dozens of ways that you can customize ggplot2 visualizations. With enough practice, you can have total control of the visualizations that you create. Let’s start with two very common and useful customizations:\n\nPutting the x-axis on a logarithmic scale, and\n\nChanging the axis labels.\n\nTo transform the x-axis to a logarithmic scale, simply add scale_x_log10() to the previous visualization’s code. This will compress the x-axis and show a more linear relationship than the previous graphic.\n\ngapminder %&gt;%                             #DATA\n  filter(year == 2007) %&gt;%                #DATA\n  ggplot(aes(x=gdpPercap,                 #AESTHETIC\n                 y=lifeExp,               #AESTHETIC\n                 color=continent)) +      #AESTHETIC\n  geom_point() +                          #GEOM\n  scale_x_log10()                         #CUSTOMIZING ELEMENT\n\n\n\n\n\n\n\n\nThe labs() command in ggplot allows users to take control of the axis labels.\n\ngapminder %&gt;%                             #DATA\n  filter(year == 2007) %&gt;%                #DATA\n  ggplot(aes(x=gdpPercap,                 #AESTHETIC\n             y=lifeExp,                   #AESTHETIC\n             color=continent)) +          #AESTHETIC\n  geom_point() +                          #GEOM\n  scale_x_log10() +                       #CUSTOMIZING ELEMENT\n  labs(x='GDP per capita (logged)',       #CUSTOMIZING ELEMENT\n       y='Average life expectancy')       #CUSTOMIZING ELEMENT\n\n\n\n\n\n\n\n\n\nExercise:\n\nCreate another scatterplot with another variable combination from the gapminder dataset."
  },
  {
    "objectID": "workshops/2018-05-15-korbel-skills-tidyverse/index.html#line-plots",
    "href": "workshops/2018-05-15-korbel-skills-tidyverse/index.html#line-plots",
    "title": "PART ONE: Data Manipulation and Visualization Basics using R",
    "section": "Line plots",
    "text": "Line plots\nLine plots are typically used to visualize the behavior of a variable over some component of time. Let’s go back to the gapminder_by_year_continent dataframe that we created earlier. This is a dataframe with summary stats by continent and year. We’ll use this data to create a line graph showing the average life expectancy by continent over time.\n\nExercises:\n\nWe have the data element already (gapminder_by_year_continent). What are the other two ggplot elements that we need to create a graphic?\n\nWhat would be our x= and y= arguments in the aesthetic?\n\n\ngapminder_by_year_continent %&gt;% \n  ggplot(aes(x=year,\n             y=meanlifeExp,\n             group=continent,\n             color=continent)) + \n  geom_line() + \n  ggtitle('Life expectancy by continent over time',\n          subtitle = 'Gapminder dataset') + \n  labs(x='Year',\n       y='Average life expectancy')\n\n\nThis plot shows us very clearly the gap between Africa and all other continents in life expectancy over time.\n\nExercises:\n\nPlot another one of the summarised variables from the gapminder_by_year_continent dataframe over time.\n\nUsing the gapminder dataframe, filter() the data on a country of your choice, and visualize a line plot over time (any variable you’d like).\n\n\nex_country &lt;- c('Zambia', 'China', 'Argentina', 'Bulgaria')\n\ngapminder %&gt;% \n  filter(country %in% ex_country) %&gt;% \n  ggplot(aes(x=year,\n             y=lifeExp,\n             color=country)) + \n  ggtitle('Average life expectancy at birth, select countries',\n          subtitle = '1957-2007') + \n  labs(x='Year',\n       y='Life expectancy at birth (years)',\n       caption='Gapminder dataset',\n       color='') + \n  geom_line() + \n  theme(legend.position = 'top')"
  },
  {
    "objectID": "workshops/2018-05-15-korbel-skills-tidyverse/index.html#extended-use-of-ggplot2",
    "href": "workshops/2018-05-15-korbel-skills-tidyverse/index.html#extended-use-of-ggplot2",
    "title": "PART ONE: Data Manipulation and Visualization Basics using R",
    "section": "Extended use of ggplot2",
    "text": "Extended use of ggplot2\nThere are many, many ways that you can customize or expand your visualization repertoire by understanding the three basic elements of ggplot graphics. Here is a look at two other extensions (you can just follow along):\n\nThe ggthemes package allows for replicating the visualization themes of some major publications, including The Economist, The Wall Street Journal, and FiveThirtyEight.\n\n\n#install.packages(\"ggthemes\")\nlibrary(ggthemes)\n\n\nYou can also plot multiple geoms on the same graphic, which can be very useful in highlighting a story from your data.\n\nWhat if you wanted to show a story of the improvement of life expectancy over time? You might want to fit a linear regression model to your scatter plot. The code below demonstrates how this might be done, using the visualization theme from The Economist.\n\ngapminder %&gt;% \n  mutate(generation=ifelse(year &lt; 1970, '1950s-60s',\n                    ifelse(year &gt;= 1970 & year &lt; 1990, '1970s-80s', '1990s-2000s'))) %&gt;% \n  ggplot(aes(x=gdpPercap,\n                 y=lifeExp,\n                 color=generation)) + \n  geom_point() + \n  scale_x_log10() + \n  geom_smooth(method = \"lm\", se=FALSE) + \n  ggtitle('Relationship between GDP per capita, average life expectancy, and time') + \n  labs(x='GDP per capita (logged)',\n       y='Average life expectancy',\n       color='',\n       caption='Gapminder dataset') + \n  theme_economist()\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "workshops/2018-05-15-korbel-skills-tidyverse/index.html#footnotes",
    "href": "workshops/2018-05-15-korbel-skills-tidyverse/index.html#footnotes",
    "title": "PART ONE: Data Manipulation and Visualization Basics using R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.tidyverse.org/packages/↩︎\nhttps://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html↩︎\nFor an extremely in-depth article on pipe operators, including where they came from and why they are so useful, see here: https://www.datacamp.com/community/tutorials/pipe-r-tutorial↩︎\nThis is a useful reference for learning about all of the geom options in ggplot. This might look overwhelming, but it is something that you pick up more as you move into more exotic visualization types. (http://ggplot2.tidyverse.org/reference/#section-layer-geoms)↩︎\nhttps://www.r-bloggers.com/how-to-learn-r-2/↩︎\nhttps://www.coursera.org/specializations/jhu-data-science↩︎\nhttp://www.swirlstats.com/↩︎\nhttps://www.rstudio.com/resources/cheatsheets/↩︎\nhttp://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html↩︎"
  },
  {
    "objectID": "posts/2023-07-11-data-science-workflows-js-1/index.html",
    "href": "posts/2023-07-11-data-science-workflows-js-1/index.html",
    "title": "Data Science Workflows in Javascript: Part One",
    "section": "",
    "text": "Introduction\nData Science Workflows in JavaScript Workshop Series - Session #1\nFollow-along notebook Session 1 slides Session 1 notebook key\nStep 0: Fork this notebook\nPress the Fork button in the upper right of the shared notebook to create your own copy (requires an Observable account). You can follow along without an account in tinker mode, but your work will not be saved.\nStep 1: Quick intro to working in a notebook\nCells and the Add cell menu Running cells Files pane\nStep 2: Attach the data in your notebook\nClick on the link below to download the fiddler crab data from the Environmental Data Initiative Data Portal. The file will be saved as HTL-MAL-FiddlerCrabBodySize.csv.\nAttach the file to your notebook using one of the following methods:\nOpen the Files pane (paperclip icon) in the top right of the notebook, click the plus sign next to File attachments, then find and choose the HTL-MAL-FiddlerCrabBodySize.csv file you downloaded above.\nDrag the file over the Files pane in your notebook to attach it.\nStep 3: Load and take a look at the data\nIn the Files pane, click the “Insert into notebook” icon to the right of the file. This will automatically create a new interactive Data table cell, where you can preview the data in tabular form and even do some basic exploration and data wrangling.\n\ncrabs = FileAttachment(\".//data/HTL-MAR-FiddlerCrabBodySize.csv\").csv({ typed: true })\n\n\n\n\n\n\n\ncrabs\n\n\n\n\n\n\nThere are 9 variables in the Johnson 2019 dataset:\n\nDate: record date\nSite: a 3 character site identifier\nLatitude: the site latitude in degrees\nReplicate: a number, indicating the crab recorded (30 at each site)\ncarapace_width: crab carapace width, in millimeters\nMATA: mean annual air temperature in degrees Celsius\nSATA: standard deviation of annual air temperature in degrees Celsius\nMATW: mean annual water temperature in degrees Celsius\nSATW: standard deviation of annual water temperature in degrees Celsius\n\nWhat is crabs? An array of objects. Let’s take a look at it outside of the Data table cell to see how that looks.\nStep 4: A bit of data wrangling\nThis data is already tidy, but we may want to simplify it a bit more. Here, we will select and rename certain columns in two ways:\nRight in the Data table cell (no code) In JavaScript Then we’ll do the wrangling in JavaScript:\n\ncrabsJS = crabs.map((d) =&gt; ({\n  lat: d.Latitude,\n  site: d[\"Site \"],\n  sizeMm: d.carapace_width, //You could also mutate right within this map function (such as a unit transformation)\n  airTempC: d.MATA,\n  waterTempC: d.MATW\n}))\n\n// This array map method is like a select and rename function in one\n\n\n\n\n\n\nStep 5: Exploratory data visualization\nLet’s make some quick exploratory charts with Observable Plot, a JavaScript library for data visualization by the team that built D3. We’ll do this in several ways:\nUsing the Chart cell in Observable, then ejecting to JS for customization Using code snippets for Observable Plot\nA note on using Observable Plot in our notebooks: a number of JavaScript libraries are automatically available when working in Observable notebooks as recommended libraries in the standard library, including D3, Observable Plot, lodash, and others commonly used on the platform, which is why you don’t need to separately install or load Plot to use it here. In the next section we’ll see how we can load a library that is not automatically available for use in Observable.\nChart cell First, let’s make a histogram of all crab sizes in the dataset. Then, we’ll facet by other variables to see if we can notice any interesting trends. Add a new Chart cell by searching for Chart in the Add cell menu, then choose the variable(s) you want to visualize.\n\nPlot.plot({\n  x: { label: \"Air Temperature (C)\" },\n  marks: [\n    Plot.dot(crabsJS, {\n      x: \"airTempC\",\n      y: \"sizeMm\",\n      stroke: \"#ff5375\",\n      tip: true\n    })\n  ]\n})\n\n\n\n\n\n\nObservable Plot snippets Open the Add cell menu, and begin typing “scatterplot.” Choose the scatterplot snippet, which will automatically add a new JavaScript cell with some skeleton code for a basic scatterplot that we can update.\nStep 6: Simple linear regression\nWe will use the SimpleLinearRegression method from the ml.js JavaScript library. Since ml.js is not automatically available, we’ll use require to access it in our notebook:\n\nML = require(\"https://www.lactame.com/lib/ml/6.0.0/ml.min.js\")\n\n\n\n\n\n\nNow, we have access to the methods in ml.js, including SimpleLinearRegression, which will estimate parameters for a linear model by ordinary least squares.\n\ncrabsLM = new ML.SimpleLinearRegression(crabsJS.map(d =&gt; d.lat), crabsJS.map(d =&gt; d.sizeMm))\n\n\n\n\n\n\nThe slope is 0.485.\nStep 7: Final visualization and summary statement\nTo wrap it up, let’s create a final visualization with a summary statement. We’ll again use Observable Plot to create and customize a chart.\n\nPlot.plot({\n  marks: [Plot.dot(crabsJS, { x: \"lat\", y: \"sizeMm\", tip: true, fill: \"steelblue\"}),\n         Plot.linearRegressionY(crabsJS, {x:\"lat\", y:\"sizeMm\"}), //adds regression line and conf int\n         Plot.frame()],//adds frame around viz\n  x: {label: \"Latitude\"},\n  y: {label: \"Carapace size (mm)\"}\n})\n\n\n\n\n\n\nStep 8: Share your notebook\nThere are a number of ways to share your notebook with others. The easiest is to share the notebook link - that’s right, your notebook is already a live page. At the top of your notebook, click the Share notebook to update the sharing settings, add colleagues as viewers or editors, and even make your own custom URL.\nSend along the link, and anyone with permissions (or the public!) can see your work.\nToday we learned:\n\nHow to attach a CSV in an Observable notebook\nData table cell to inspect & wrangle data\nA little data wrangling in JS\nChart cell and Observable Plot for quick exploratory data viz\nRequire to load a JavaScript module\nSimple linear regression in JS with ml.js\nCreating a final visualization\nReferring to code outputs in markdown with template literals\nSharing a notebook URL\nBergmann’s Rule!"
  },
  {
    "objectID": "posts/2024-07-30-braintree-sql-questions/index.html",
    "href": "posts/2024-07-30-braintree-sql-questions/index.html",
    "title": "BrainTree SQL challenge",
    "section": "",
    "text": "The BrainTree SQL coding challenge is known as a great challenge for data analysts looking to test their skills. In this project, I am putting my skills to the test, using Quarto and the DBI:: and RSQLite:: packages to run SQL queries on an in-memory database.\n\n\nRequired packages:\n\nlibrary(tidyverse)\nlibrary(DBI)\nlibrary(RSQLite)\n\nAbout the dataset:\nHere is some text from the BrainTree challenge site:\n\nThis is an opportunity for you to show us your grasp of SQL which plays a key role in the day-to-day job that you’re applying for. All members of the Analytics Data team have taken this challenge and participated in its creation. If you feel that there are any questions that are either not fair or not clear, please do let us know; this is VERY important to us! A few important things to note before you get started:\n\n\n\nAll work should be done in SQL. Any variant is fine (e.g. MS SQL, Postgres, MySQL, etc.). If you normally use R, SAS, or other similar tools with SQL it’s important that you show that you can work in SQL by itself to produce the correct answers to this challenge.\nIf you are confused by a specific question, you can request clarification by replying to the message that this challenge was attached to. This is NOT intended for you to obtain technical help with solving the problems on this challenge or asking for hints; it should only be used for question clarification. This challenge is due back within 1 week (7 calendar days) of being sent to you.\nIf you cannot answer a question, please do your best, show your work, leave comments, and let us know your general thoughts.\nWe are interested in BOTH your answers and the work/code that you write to get them. Please leave plenty of comments for us to read when we review your work.\nThere are some blank/null values in this set. That’s how we found it and it reflects the nature of imperfect data. Please work with this and provide explanations of any issues or workarounds required to arrive at your answers.\nThere are no intentional gotchas, trick questions, or traps; the challenge is intended to demonstrate some of the typical day-to-day SQL skills that the job requires.\nSome of these questions may seem nonsensical and you may find yourself asking, “why would anyone want to know that?!” They are intended purely as a measure of your SQL skills and not as actual questions that we would expect to ask of this type of data set. Please take them with a grain of salt.\n\n\n\ncountries &lt;- read_csv('.//data/countries.csv')\ncontinent_map &lt;- read_csv('.//data/continent_map.csv')\ncontinents &lt;- read_csv('.//data/continents.csv')\nper_capita &lt;- read_csv('.//data/per_capita.csv')\n\n\n\n\nThe DBI:: package allows you to create an in-memory database to query against. The DBI project site is a great place to learn more about it. I’ll start by doing some initial setup and establishing the connection between the R dataframe and the SQL table name that I’ll query.\n\ncon &lt;- DBI::dbConnect(SQLite(), \":memory:\")\nDBI::dbWriteTable(conn = con, name = \"continent_map\", value = continent_map)\nDBI::dbWriteTable(conn = con, name = \"per_capita\", value = per_capita)\nDBI::dbWriteTable(conn = con, name = \"continents\", value = continents)\nDBI::dbWriteTable(conn = con, name = \"countries\", value = countries)"
  },
  {
    "objectID": "posts/2024-07-30-braintree-sql-questions/index.html#setup-and-data-preparation",
    "href": "posts/2024-07-30-braintree-sql-questions/index.html#setup-and-data-preparation",
    "title": "BrainTree SQL challenge",
    "section": "",
    "text": "Required packages:\n\nlibrary(tidyverse)\nlibrary(DBI)\nlibrary(RSQLite)\n\nAbout the dataset:\nHere is some text from the BrainTree challenge site:\n\nThis is an opportunity for you to show us your grasp of SQL which plays a key role in the day-to-day job that you’re applying for. All members of the Analytics Data team have taken this challenge and participated in its creation. If you feel that there are any questions that are either not fair or not clear, please do let us know; this is VERY important to us! A few important things to note before you get started:\n\n\n\nAll work should be done in SQL. Any variant is fine (e.g. MS SQL, Postgres, MySQL, etc.). If you normally use R, SAS, or other similar tools with SQL it’s important that you show that you can work in SQL by itself to produce the correct answers to this challenge.\nIf you are confused by a specific question, you can request clarification by replying to the message that this challenge was attached to. This is NOT intended for you to obtain technical help with solving the problems on this challenge or asking for hints; it should only be used for question clarification. This challenge is due back within 1 week (7 calendar days) of being sent to you.\nIf you cannot answer a question, please do your best, show your work, leave comments, and let us know your general thoughts.\nWe are interested in BOTH your answers and the work/code that you write to get them. Please leave plenty of comments for us to read when we review your work.\nThere are some blank/null values in this set. That’s how we found it and it reflects the nature of imperfect data. Please work with this and provide explanations of any issues or workarounds required to arrive at your answers.\nThere are no intentional gotchas, trick questions, or traps; the challenge is intended to demonstrate some of the typical day-to-day SQL skills that the job requires.\nSome of these questions may seem nonsensical and you may find yourself asking, “why would anyone want to know that?!” They are intended purely as a measure of your SQL skills and not as actual questions that we would expect to ask of this type of data set. Please take them with a grain of salt.\n\n\n\ncountries &lt;- read_csv('.//data/countries.csv')\ncontinent_map &lt;- read_csv('.//data/continent_map.csv')\ncontinents &lt;- read_csv('.//data/continents.csv')\nper_capita &lt;- read_csv('.//data/per_capita.csv')"
  },
  {
    "objectID": "posts/2024-07-30-braintree-sql-questions/index.html#setting-up-sql-to-execute",
    "href": "posts/2024-07-30-braintree-sql-questions/index.html#setting-up-sql-to-execute",
    "title": "BrainTree SQL challenge",
    "section": "",
    "text": "The DBI:: package allows you to create an in-memory database to query against. The DBI project site is a great place to learn more about it. I’ll start by doing some initial setup and establishing the connection between the R dataframe and the SQL table name that I’ll query.\n\ncon &lt;- DBI::dbConnect(SQLite(), \":memory:\")\nDBI::dbWriteTable(conn = con, name = \"continent_map\", value = continent_map)\nDBI::dbWriteTable(conn = con, name = \"per_capita\", value = per_capita)\nDBI::dbWriteTable(conn = con, name = \"continents\", value = continents)\nDBI::dbWriteTable(conn = con, name = \"countries\", value = countries)"
  },
  {
    "objectID": "posts/2023-07-29-data-science-workflows-js-3/index.html",
    "href": "posts/2023-07-29-data-science-workflows-js-3/index.html",
    "title": "Data Science Workflows in Javascript: Part Three",
    "section": "",
    "text": "Introduction\nToday we’ll be exploring multivariate meteorological data from Mar Casado Beach, Sao Paulo (Brazil). By the end of the lesson, participants will be able to:\nCreate a database in a notebook using DuckDBClient.of() Query the database in SQL cells Visualize relationships between variables Import and reuse existing content using import with Perform and visualize results of principal component analysis In this session, we will work with data from da Silva et al (2019) describing monthly atmospheric and ocean data (e.g. air pressure, windspeed, tides, sea surface temperature).\nData source: Marcos Gonçalves da Silva, Juliana Nascimento Silva, Helena Rodrigues Fragoso, Natalia Pirani Ghilardi-Lopes (2019). Temporal series analysis of abiotic data for a subtropical Brazilian rocky shore. Data in Brief, Volume 24. ISSN 2352-3409, https://doi.org/10.1016/j.dib.2019.103873.\nStep 1: Combine the tables into a database The data are already attached here in two parts: marCasadoAir, and marCasadoSea:\n\nmarCasadoAir = FileAttachment(\".//data/marCasadoAir@5.csv\").csv({typed: true}) // Date issue\n\n\n\n\n\n\n\nmarCasadoSea = FileAttachment(\".//data/marCasadoSea@4.csv\").csv({typed: true}) // Date issue\n\n\n\n\n\n\nWe can create a database containing both tables using DuckDBClient.of():\n\n// Write code to create a database called marCasadoDB, with tables 'air' and 'sea':\nmarCasadoDB = DuckDBClient.of({air: marCasadoAir, sea: marCasadoSea})\n\n\n\n\n\n\nNow, open the Database pane in the right margin of your notebook. There, you can explore the schema and variables of your newly created database.\nStep 2: Wrangle & analyze data in a SQL cell We want to combine some data from both tables in the database. The column we’re able to join by is month. We will also keep the following columns, and add a new column for season:\nFrom air:\n\nmonth\nmeanPressure (mmHg)\nwindSpeed (kilometers per hour)\nPAR (photoactive radiation in E/m s2)\nmeanHumidity (percent)\nwindDirection (degrees)\n\nFrom sea:\n\nmaxTide (meters)\nminTide (meters)\nsalinity (practical salinity units)\nseaSurfaceTemp (degrees Celsius)\n\nFrom da Silva et al (2019): “…two distinct seasons: (I) a hot and moist season from October to March (encompassing Spring and Summer) and (II) a cold and dry season from April to September (encompassing Autumn and Winter); an expected result for subtropical zones.”\nWe’ll also add a new column, season, containing “cool dry” for October thru March, otherwise “hot moist.”\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\nRows: 60 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (14): maxAirTemp, minAirTemp, maxHumidity, minHumidity, meanHumidity, m...\ndate  (1): month\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 60 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (4): seaSurfaceTemp, maxTide, minTide, salinity\ndate (1): month\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nselect  cast(strftime('%m', a.month) as integer) as month,\n        a.meanPressure, \n        a.windSpeed,\n        a.PAR,\n        a.meanHumidity, \n        a.windDirection,\n        s.maxTide,\n        s.minTide,\n        s.salinity,\n        s.seaSurfaceTemp,\n        CASE WHEN cast(strftime('%m', a.month) as integer) IN (10, 11, 12, 1, 2, 3) THEN 'hot moist' ELSE 'cool dry' END AS season\nfrom air as a\nleft join sea as s\non a.month = s.month\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmonth\nmeanPressure\nwindSpeed\nPAR\nmeanHumidity\nwindDirection\nmaxTide\nminTide\nsalinity\nseaSurfaceTemp\nseason\n\n\n\n\n11\n1012.10\n5.47\n50.84\n79.86\n51.80\n1.25\n0.28\n35.50\n26.27\nhot moist\n\n\n12\n1011.89\n3.83\n51.48\n77.34\n71.58\n1.27\n0.26\n35.13\n26.27\nhot moist\n\n\n1\n1013.48\n6.22\n36.19\n79.75\n54.58\n1.30\n0.27\n35.62\n26.57\nhot moist\n\n\n2\n1015.67\n5.37\n35.78\n79.25\n70.64\n1.28\n0.29\n35.67\n24.82\nhot moist\n\n\n3\n1016.32\n4.88\n27.58\n76.40\n66.01\n1.25\n0.26\n35.77\n24.05\nhot moist\n\n\n4\n1017.40\n3.73\n21.69\n81.65\n71.94\n1.25\n0.25\n35.41\n22.37\ncool dry\n\n\n5\n1020.16\n5.27\n25.69\n79.34\n79.75\n1.25\n0.29\n34.45\n20.73\ncool dry\n\n\n6\n1018.84\n4.38\n31.20\n75.15\n90.27\n1.28\n0.29\n32.74\n18.85\ncool dry\n\n\n7\n1017.23\n5.48\n33.23\n76.70\n74.55\n1.32\n0.29\n32.32\n21.05\ncool dry\n\n\n8\n1015.74\n5.91\n38.72\n77.88\n56.02\n1.30\n0.31\n33.13\n21.79\ncool dry\n\n\n\n\n\nIf you want, you can work in SQL to wrangle & analyze an array (doesn’t have to be a database):\n\nSELECT cast(strftime('%m', month) as integer) as month,\n      avg(seaSurfaceTemp) as meanSST \n  FROM sea\nGROUP BY month\n\n\nDisplaying records 1 - 10\n\n\nmonth\nmeanSST\n\n\n\n\n11\n26.27\n\n\n12\n26.27\n\n\n1\n26.57\n\n\n2\n24.82\n\n\n3\n24.05\n\n\n4\n22.37\n\n\n5\n20.73\n\n\n6\n18.85\n\n\n7\n21.05\n\n\n8\n21.79\n\n\n\n\n\n\nmarCasado = FileAttachment(\".//data/marCasado.csv\").csv({typed: true})\n\n\n\n\n\n\n\n// Write Plot code to create a heatmap of sea surface temperature (SST) by year and month, starting from the 'cell' snippet:\nPlot.plot({\n  marks: [\n    Plot.cell(marCasado, {\n      y: d =&gt; d.month.getUTCFullYear(),\n      x: d =&gt; d.month.getUTCMonth(),\n      fill: \"seaSurfaceTemp\",\n      tip: true\n    })\n  ],\n  width: 500,\n  height: 250,\n  y: {tickFormat: \"Y\", padding: 0},\n  x: {padding: 0, tickFormat: Plot.formatMonth()}\n})\n\n\n\n\n\n\nStep 4: Interactive data visualization Use import with to import content from another notebook, and replace what it expects with something new:\n\nimport {PlotMatrix} with {marCasado as data} from \"@observablehq/autoplot-matrix\"\n\n\n\n\n\n\n\n// Use the PlotMatrix function (expecting marCasado) to create a pair plot:\nPlotMatrix(marCasado)\n\n\n\n\n\n\nStep 5: Principal component analysis Imports & libraries for PCA Use require to access methods from the ml.js package (which includes the PCA function we’ll use):\n\nML = require(\"https://www.lactame.com/lib/ml/6.0.0/ml.min.js\")\n\n\n\n\n\n\nImport the scale and asMatrix functions from Christoph Pahmeyer’s hierarchical clustering notebook. The scale function will scale values of a property to have a mean of 0 and standard deviation of 1. We use asMatrix to get a 2D array of predictions to project our points in the PC space.\n\nimport {scale, asMatrix} from \"@chrispahm/hierarchical-clustering\"\n\n\n\n\n\n\nScaling and PCA Create a scaled version of the data, only including the numeric variables (excluding season and month) that will be included in principal component analysis:\n\n// Create a scaled version of the numeric variables\nmarCasadoScaled = scale(marCasado.map(({ season, month, ...rest }) =&gt; rest))\n\n\n\n\n\n\nThen convert the array of objects to an array of arrays (which is what the PCA method from ml.js is expecting):\n\n// Convert to an array of arrays, just containing the values (no keys):\nmarCasadoArray = marCasadoScaled.map(Object.values)\n\n\n\n\n\n\nNow, we’re ready to perform PCA using the PCA function in ml.js (nicknamed ML when we used require above):\n\n// Perform principal component analysis:\nmarCasadoPCA = new ML.PCA(marCasadoArray) // Already scaled above - otherwise can add {scale: true} here!\n\n\n\n\n\n\nExplore PCA results Use getExplainedVariance() to see variance explained by each PC:\n\n// Get variance explained by each PC:\nvariancePC = marCasadoPCA.getExplainedVariance()\n\n\n\n\n\n\nUse getCumulativeVariance() to see cumulative variance explained:\n\n// Get cumulative variance explained: \ncumulativeVariance = marCasadoPCA.getCumulativeVariance()\n\n\n\n\n\n\nStep 6: Visualize PCA results import with to reuse community contributions We’ll again use import with to access and reuse materials, this time from Christoph Pahmeyer’s notebook on principal component analysis.\n\nimport {loadings} from \"@chrispahm/principal-component-analysis\"\n\n\n\n\n\n\n\n// Import viewof loadings from the notebook, with marCasadoScaled as food_scaled:\nimport {viewof loadings} with {marCasadoScaled as food_scaled} from \"@chrispahm/principal-component-analysis\"\n\n\n\n\n\n\n\n// Look at viewof loadings:\nloadings_df = viewof loadings\n\n\n\n\n\n\n\nimport {scores} from \"@chrispahm/principal-component-analysis\"\n\n\n\n\n\n\n\n// import viewof scores from the notebook, with marCasadoScaled as food_scaled and marCasado as food:\nimport {viewof scores} with {marCasadoScaled as food_scaled, marCasado as food} from \"@chrispahm/principal-component-analysis\"\n\n\n\n\n\n\n\n// Look at viewof scores:\nscores_df = viewof scores\n\n\n\n\n\n\n\n// Do some wrangling to get the month and season alongside scores: \nscoresCombined = scores_df.map((d, i) =&gt; ({ ...d, Name: marCasado[i].month, season: marCasado[i].season }))\n\n\n\n\n\n\n\nscalingFactor = 5\n\n\n\n\n\n\n\n// Create a PCA biplot with the scores and loadings\nPlot.plot({\n  marks: [\n    Plot.dot(scoresCombined, { x: \"PC1\", y: \"PC2\", fill: \"season\", r: 5 }),\n    Plot.arrow(loadings_df, {\n      x1: 0, x2: d =&gt; d.PC1 * scalingFactor, y1: 0, y2: (d) =&gt; d.PC2 * scalingFactor\n    }),\n    Plot.text(loadings_df, {\n      x: (d) =&gt; d.PC1 * scalingFactor, y: (d) =&gt; d.PC2 * scalingFactor,\n      text: \"Variable\",\n      dy: -5,\n      dx: 30,\n      fill: \"black\",\n      stroke: \"white\",\n      fontSize: 14\n    })\n  ],\n  color: { legend: true },\n  inset: 20\n})"
  },
  {
    "objectID": "posts/2024-08-18-duckdb-intro/index.html",
    "href": "posts/2024-08-18-duckdb-intro/index.html",
    "title": "Setting up an EPA air quality database using DuckDB",
    "section": "",
    "text": "DuckDB is an in-process SQL database management system designed for fast and efficient data analysis. It’s particularly advantageous for data analysts due to its seamless integration with popular programming languages like Python and R, allowing easy querying of large datasets directly from within those environments. DuckDB’s columnar storage format and vectorized execution enable high-performance analytics on large datasets without the need for complex setups, making it an ideal choice for exploratory data analysis, interactive queries, and handling structured data efficiently on local machines."
  },
  {
    "objectID": "posts/2024-08-18-duckdb-intro/index.html#libraries",
    "href": "posts/2024-08-18-duckdb-intro/index.html#libraries",
    "title": "Setting up an EPA air quality database using DuckDB",
    "section": "Libraries",
    "text": "Libraries\n\nlibrary(tidyverse)\nlibrary(duckdb)\nlibrary(RAQSAPI)"
  },
  {
    "objectID": "posts/2024-08-18-duckdb-intro/index.html#fetching-data-from-the-epas-air-quality-service-data-mart-api",
    "href": "posts/2024-08-18-duckdb-intro/index.html#fetching-data-from-the-epas-air-quality-service-data-mart-api",
    "title": "Setting up an EPA air quality database using DuckDB",
    "section": "Fetching data from the EPA’s Air Quality Service Data Mart API",
    "text": "Fetching data from the EPA’s Air Quality Service Data Mart API\nIf it’s your first time accessing the AQS API, you’ll need to use the RAQSAPI::aqs_sign_up() function, and provide your email address as a string. (Note: when I first set up my credentials, it took about 48 hours to receive the email from their service).\n\n# RAQSAPI::aqs_sign_up(\"abcd@efg.com\")\n\nThen, use the RAQSAPI::aqs_credentials() function to provide your username and key.\nThe AQS service is rich with a variety of functions and arguments that help you pull exactly what you need. There is so much available through their API, and I won’t even begin to cover it here. For this demo, I’d like to look at ozone measurements over the last year in Colorado.\nSome common arguments that you’ll find in RAQSAPI:: package:\n\nparameter=:the specific pollutant that you’re interested in. You can feed this a vector of multiple pollutants.2\nbdate=: start date. If the grain of analysis is daily, the API limits each call to one year of data.\nedate=: end date.\nstateFIPS and county: FIPS codes for your states or counties (if that’s your grain of analysis)\n\n\ndf_ozone &lt;- RAQSAPI::aqs_dailysummary_by_county(\n  # '44201' is ozone\n  # '43201' is methane\n  parameter = '44201',\n  bdate = as.Date('20240101',\n                  format = \"%Y%m%d\"),\n  edate = as.Date('20240810',\n                  format = \"%Y%m%d\"),\n  # '08' is Colorado\n  stateFIPS = '08',\n  # '059' is Jefferson County\n  county = '059'\n)\n\nNow, I’ll dplyr::glimpse() to see the structure of the data.\n\ndf_ozone %&gt;% \n  glimpse()\n\nRows: 1,440\nColumns: 32\n$ state_code           &lt;chr&gt; \"08\", \"08\", \"08\", \"08\", \"08\", \"08\", \"08\", \"08\", \"…\n$ county_code          &lt;chr&gt; \"059\", \"059\", \"059\", \"059\", \"059\", \"059\", \"059\", …\n$ site_number          &lt;chr&gt; \"0006\", \"0006\", \"0006\", \"0006\", \"0006\", \"0006\", \"…\n$ parameter_code       &lt;chr&gt; \"44201\", \"44201\", \"44201\", \"44201\", \"44201\", \"442…\n$ poc                  &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ latitude             &lt;dbl&gt; 39.9128, 39.9128, 39.9128, 39.9128, 39.9128, 39.9…\n$ longitude            &lt;dbl&gt; -105.1886, -105.1886, -105.1886, -105.1886, -105.…\n$ datum                &lt;chr&gt; \"WGS84\", \"WGS84\", \"WGS84\", \"WGS84\", \"WGS84\", \"WGS…\n$ parameter            &lt;chr&gt; \"Ozone\", \"Ozone\", \"Ozone\", \"Ozone\", \"Ozone\", \"Ozo…\n$ sample_duration_code &lt;chr&gt; \"1\", \"W\", \"W\", \"W\", \"1\", \"W\", \"W\", \"W\", \"1\", \"W\",…\n$ sample_duration      &lt;chr&gt; \"1 HOUR\", \"8-HR RUN AVG BEGIN HOUR\", \"8-HR RUN AV…\n$ pollutant_standard   &lt;chr&gt; \"Ozone 1-hour 1979\", \"Ozone 8-Hour 1997\", \"Ozone …\n$ date_local           &lt;chr&gt; \"2024-01-01\", \"2024-01-01\", \"2024-01-01\", \"2024-0…\n$ units_of_measure     &lt;chr&gt; \"Parts per million\", \"Parts per million\", \"Parts …\n$ event_type           &lt;chr&gt; \"No Events\", \"No Events\", \"No Events\", \"No Events…\n$ observation_count    &lt;int&gt; 24, 24, 24, 17, 23, 24, 24, 17, 24, 24, 24, 17, 2…\n$ observation_percent  &lt;dbl&gt; 100, 100, 100, 100, 96, 100, 100, 100, 100, 100, …\n$ validity_indicator   &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\",…\n$ arithmetic_mean      &lt;dbl&gt; 0.041333, 0.041833, 0.041833, 0.042294, 0.033870,…\n$ first_max_value      &lt;dbl&gt; 0.048, 0.046, 0.046, 0.046, 0.046, 0.037, 0.037, …\n$ first_max_hour       &lt;int&gt; 14, 8, 8, 8, 23, 22, 22, 22, 15, 10, 10, 10, 23, …\n$ aqi                  &lt;int&gt; NA, 43, 43, 43, NA, 34, 34, 34, NA, 36, 36, 36, N…\n$ method_code          &lt;chr&gt; \"087\", \"087\", \"087\", \"087\", \"087\", \"087\", \"087\", …\n$ method               &lt;chr&gt; \"INSTRUMENTAL - ULTRA VIOLET ABSORPTION\", \"INSTRU…\n$ local_site_name      &lt;chr&gt; \"ROCKY FLATS-N\", \"ROCKY FLATS-N\", \"ROCKY FLATS-N\"…\n$ site_address         &lt;chr&gt; \"16600 W COLO #128\", \"16600 W COLO #128\", \"16600 …\n$ state                &lt;chr&gt; \"Colorado\", \"Colorado\", \"Colorado\", \"Colorado\", \"…\n$ county               &lt;chr&gt; \"Jefferson\", \"Jefferson\", \"Jefferson\", \"Jefferson…\n$ city                 &lt;chr&gt; \"Not in a city\", \"Not in a city\", \"Not in a city\"…\n$ cbsa_code            &lt;chr&gt; \"19740\", \"19740\", \"19740\", \"19740\", \"19740\", \"197…\n$ cbsa                 &lt;chr&gt; \"Denver-Aurora-Lakewood, CO\", \"Denver-Aurora-Lake…\n$ date_of_last_change  &lt;chr&gt; \"2024-05-24\", \"2024-05-24\", \"2024-05-24\", \"2024-0…"
  },
  {
    "objectID": "posts/2024-08-18-duckdb-intro/index.html#setting-up-and-querying-duckdb-in-r",
    "href": "posts/2024-08-18-duckdb-intro/index.html#setting-up-and-querying-duckdb-in-r",
    "title": "Setting up an EPA air quality database using DuckDB",
    "section": "Setting up and querying DuckDB in R",
    "text": "Setting up and querying DuckDB in R\n(This section is not meant to be analysis! I’m just demonstrating the setup and querying of the database.)\n\ncon &lt;- DBI::dbConnect(duckdb(), path = \":memory:\")\n\nozone &lt;- copy_to(con, df_ozone %&gt;% filter(sample_duration_code == '1'), overwrite = TRUE)\n\n\n# write our data to duckdb table\ntable_name &lt;- \"ozone\"\nduckdb::dbWriteTable(con, table_name, df_ozone)\n\n\ndbGetQuery(con,\n           \"SELECT *\n           FROM ozone\n           limit 10\")\n\n   state_code county_code site_number parameter_code poc latitude longitude\n1          08         059        0006          44201   1  39.9128 -105.1886\n2          08         059        0006          44201   1  39.9128 -105.1886\n3          08         059        0006          44201   1  39.9128 -105.1886\n4          08         059        0006          44201   1  39.9128 -105.1886\n5          08         059        0006          44201   1  39.9128 -105.1886\n6          08         059        0006          44201   1  39.9128 -105.1886\n7          08         059        0006          44201   1  39.9128 -105.1886\n8          08         059        0006          44201   1  39.9128 -105.1886\n9          08         059        0006          44201   1  39.9128 -105.1886\n10         08         059        0006          44201   1  39.9128 -105.1886\n   datum parameter sample_duration_code         sample_duration\n1  WGS84     Ozone                    1                  1 HOUR\n2  WGS84     Ozone                    W 8-HR RUN AVG BEGIN HOUR\n3  WGS84     Ozone                    W 8-HR RUN AVG BEGIN HOUR\n4  WGS84     Ozone                    W 8-HR RUN AVG BEGIN HOUR\n5  WGS84     Ozone                    1                  1 HOUR\n6  WGS84     Ozone                    W 8-HR RUN AVG BEGIN HOUR\n7  WGS84     Ozone                    W 8-HR RUN AVG BEGIN HOUR\n8  WGS84     Ozone                    W 8-HR RUN AVG BEGIN HOUR\n9  WGS84     Ozone                    1                  1 HOUR\n10 WGS84     Ozone                    W 8-HR RUN AVG BEGIN HOUR\n   pollutant_standard date_local  units_of_measure event_type observation_count\n1   Ozone 1-hour 1979 2024-01-01 Parts per million  No Events                24\n2   Ozone 8-Hour 1997 2024-01-01 Parts per million  No Events                24\n3   Ozone 8-Hour 2008 2024-01-01 Parts per million  No Events                24\n4   Ozone 8-hour 2015 2024-01-01 Parts per million  No Events                17\n5   Ozone 1-hour 1979 2024-01-02 Parts per million  No Events                23\n6   Ozone 8-Hour 1997 2024-01-02 Parts per million  No Events                24\n7   Ozone 8-Hour 2008 2024-01-02 Parts per million  No Events                24\n8   Ozone 8-hour 2015 2024-01-02 Parts per million  No Events                17\n9   Ozone 1-hour 1979 2024-01-03 Parts per million  No Events                24\n10  Ozone 8-Hour 1997 2024-01-03 Parts per million  No Events                24\n   observation_percent validity_indicator arithmetic_mean first_max_value\n1                  100                  Y        0.041333           0.048\n2                  100                  Y        0.041833           0.046\n3                  100                  Y        0.041833           0.046\n4                  100                  Y        0.042294           0.046\n5                   96                  Y        0.033870           0.046\n6                  100                  Y        0.033500           0.037\n7                  100                  Y        0.033500           0.037\n8                  100                  Y        0.032471           0.037\n9                  100                  Y        0.037000           0.045\n10                 100                  Y        0.036083           0.039\n   first_max_hour aqi method_code                                 method\n1              14  NA         087 INSTRUMENTAL - ULTRA VIOLET ABSORPTION\n2               8  43         087 INSTRUMENTAL - ULTRA VIOLET ABSORPTION\n3               8  43         087 INSTRUMENTAL - ULTRA VIOLET ABSORPTION\n4               8  43         087 INSTRUMENTAL - ULTRA VIOLET ABSORPTION\n5              23  NA         087 INSTRUMENTAL - ULTRA VIOLET ABSORPTION\n6              22  34         087 INSTRUMENTAL - ULTRA VIOLET ABSORPTION\n7              22  34         087 INSTRUMENTAL - ULTRA VIOLET ABSORPTION\n8              22  34         087 INSTRUMENTAL - ULTRA VIOLET ABSORPTION\n9              15  NA         087 INSTRUMENTAL - ULTRA VIOLET ABSORPTION\n10             10  36         087 INSTRUMENTAL - ULTRA VIOLET ABSORPTION\n   local_site_name      site_address    state    county          city cbsa_code\n1    ROCKY FLATS-N 16600 W COLO #128 Colorado Jefferson Not in a city     19740\n2    ROCKY FLATS-N 16600 W COLO #128 Colorado Jefferson Not in a city     19740\n3    ROCKY FLATS-N 16600 W COLO #128 Colorado Jefferson Not in a city     19740\n4    ROCKY FLATS-N 16600 W COLO #128 Colorado Jefferson Not in a city     19740\n5    ROCKY FLATS-N 16600 W COLO #128 Colorado Jefferson Not in a city     19740\n6    ROCKY FLATS-N 16600 W COLO #128 Colorado Jefferson Not in a city     19740\n7    ROCKY FLATS-N 16600 W COLO #128 Colorado Jefferson Not in a city     19740\n8    ROCKY FLATS-N 16600 W COLO #128 Colorado Jefferson Not in a city     19740\n9    ROCKY FLATS-N 16600 W COLO #128 Colorado Jefferson Not in a city     19740\n10   ROCKY FLATS-N 16600 W COLO #128 Colorado Jefferson Not in a city     19740\n                         cbsa date_of_last_change\n1  Denver-Aurora-Lakewood, CO          2024-05-24\n2  Denver-Aurora-Lakewood, CO          2024-05-24\n3  Denver-Aurora-Lakewood, CO          2024-05-24\n4  Denver-Aurora-Lakewood, CO          2024-05-24\n5  Denver-Aurora-Lakewood, CO          2024-05-24\n6  Denver-Aurora-Lakewood, CO          2024-05-24\n7  Denver-Aurora-Lakewood, CO          2024-05-24\n8  Denver-Aurora-Lakewood, CO          2024-05-24\n9  Denver-Aurora-Lakewood, CO          2024-05-24\n10 Denver-Aurora-Lakewood, CO          2024-05-24\n\n\n\ndbGetQuery(con,\n           \"SELECT\n                sample_duration, \n                local_site_name,\n                AVG(aqi) as avg_aqi,\n                AVG(arithmetic_mean) as avg_arithmetic_mean,\n                AVG(first_max_value) as avg_first_max_value\n           FROM ozone\n           GROUP BY sample_duration, local_site_name\")\n\n          sample_duration                       local_site_name  avg_aqi\n1                  1 HOUR                         ROCKY FLATS-N       NA\n2 8-HR RUN AVG BEGIN HOUR                         ROCKY FLATS-N 47.24793\n3                  1 HOUR NATIONAL RENEWABLE ENERGY LABS - NREL       NA\n4 8-HR RUN AVG BEGIN HOUR NATIONAL RENEWABLE ENERGY LABS - NREL 44.07649\n5                  1 HOUR                             Evergreen       NA\n6 8-HR RUN AVG BEGIN HOUR                             Evergreen 47.28375\n  avg_arithmetic_mean avg_first_max_value\n1          0.04345224          0.05146281\n2          0.04325047          0.04856749\n3          0.04052387          0.04900000\n4          0.04019869          0.04589235\n5          0.04131112          0.05216529\n6          0.04122206          0.04876309"
  },
  {
    "objectID": "posts/2024-08-18-duckdb-intro/index.html#footnotes",
    "href": "posts/2024-08-18-duckdb-intro/index.html#footnotes",
    "title": "Setting up an EPA air quality database using DuckDB",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCRAN documentation can be found at: https://cran.r-project.org/web/packages/RAQSAPI/vignettes/RAQSAPIvignette.html#:~:text=RAQSAPI%20is%20a%20package%20for,of%20ambient%20air%20pollution%20data.↩︎\nSee “parameter code” here: https://aqs.epa.gov/aqsweb/documents/codetables/methods_all.html↩︎"
  },
  {
    "objectID": "posts/2024-10-01-cdcplaces-pt2/index.html",
    "href": "posts/2024-10-01-cdcplaces-pt2/index.html",
    "title": "Creating great U.S. county maps with advanced ggplot styling",
    "section": "",
    "text": "library(tidyverse)\nlibrary(CDCPLACES)\n\nWarning: package 'CDCPLACES' was built under R version 4.4.1\n\nlibrary(ggtext)\nlibrary(maps)\nlibrary(sf)\nlibrary(patchwork)\n\nIn Part One, I used the CDCPLACES:: package to easily access and visualize health data from the Centers for Disease Control. In this post, I’ll demonstrate how to improve the visual style of a U.S. map, while using data from the CDC PLACES project.\n\n\n\nI really like the aesthetic of maps created by the American Inequality project, which uses Datawrapper as its visualization tool for their newsletter. There are several elements of their aesthetic that I think are effective (see the example map below):\n\n\n\nA map showing the distribution of poor mental health days per month by U.S. county\n\n\n\nThe diverging color palette makes it very clear where counties stand1\nThe font (Roboto) is a sans serif font, which is more friendly for visual accessibility\nI like the positioning of the legend (top and horizontal), because I think it’s an efficient use of space, and because it reduces the amount of eye movement between the map, to the legend, and back.\nThe state boundaries are more pronounced than the county boundaries, which makes the map more immediately relatable2\nThe placement of Alaska and Hawaii allow for a condensed graphic, which I think is very clean for distributing in digital platforms\n\n\ndf_depression &lt;- get_places(geography = 'county',\n                            measure = 'DEPRESSION',\n                            release = '2024',\n                            geometry = TRUE)\n\n\n\n\n\n(lower48_map &lt;- df_depression %&gt;% \n  filter(data_value_type == 'Crude prevalence',\n         !stateabbr %in% c('AK', 'HI')) %&gt;% \n  ggplot(.) + \n  geom_sf(aes(fill=data_value/100)) + \n  coord_sf(datum = NA) + \n  scale_fill_gradient2(\n    low = \"#2469b3\",\n    mid = \"#f0eee5\",\n    high = \"#b6202c\",\n    midpoint = median(df_depression$data_value/100, na.rm = TRUE),\n    labels = scales::percent,\n    name = ''\n  ) + \n  ggtitle('Depression among adults aged &gt;= 18 years',\n          subtitle = 'by U.S. county in 2022') + \n  labs(x='',\n       y='') + \n  labs(caption='Source: U.S. Centers for Disease Control (PLACES product)') + \n  guides(color='none') + \n  theme_minimal())\n\n\n\n\n\n\n\n\n\n\n\n\n(lower48_map &lt;- df_depression %&gt;% \n  filter(data_value_type == 'Crude prevalence',\n         !stateabbr %in% c('AK', 'HI')) %&gt;% \n  ggplot(.) + \n  geom_sf(aes(fill=data_value/100)) + \n  coord_sf(datum = NA) + \n  scale_fill_gradient2(\n    low = \"#2469b3\",\n    mid = \"#f0eee5\",\n    high = \"#b6202c\",\n    midpoint = median(df_depression$data_value/100, na.rm = TRUE),\n    labels = scales::percent,\n    name = ''\n  ) + \n  ggtitle('Rates of depression among adults are highest in\\nCentral Appalachia and the Bible Belt',\n          subtitle = '% of adults over 18 years old by U.S. county in 2022') + \n  labs(x='',\n       y='') + \n  labs(caption='Source: U.S. Centers for Disease Control (PLACES product)') + \n  guides(color='none') + \n  theme_minimal() + \n  theme(\n    text = element_text(family = 'Roboto Condensed'),\n    plot.title = element_text(face = 'bold', size=20),\n    plot.subtitle = element_text(size=14),\n    legend.position = \"top\",\n    legend.direction = \"horizontal\",\n    legend.key.height = unit(0.5, \"cm\"),\n    legend.key.width = unit(2, \"cm\"),\n    legend.title.position = \"top\",\n    legend.title = element_blank(),\n    legend.background = element_rect(fill = \"white\", color = NA),\n    legend.spacing.x = unit(0.2, 'cm')\n    ))\n\n\n\n\n\n\n\n\n\n\n\nI want to the map to have thicker state boundaries than county boundaries – I think that’s a nice look and a nice way to orient readers to differences in state outcomes. I’ll use the maps::map_data() function to pull in the state boundaries and overlay those onto my county-based data.\n\nstates &lt;- map_data(\"state\")\n\n\n(lower48_map &lt;- df_depression %&gt;% \n  filter(data_value_type == 'Crude prevalence',\n         !stateabbr %in% c('AK', 'HI')) %&gt;% \n  ggplot(.) + \n  geom_sf(aes(fill=data_value/100), color='white') + \n  # this is what imitates the Datawrapper aesthetic of thicker state borders than county borders\n  geom_polygon(data = states, \n               aes(x = long, y = lat, group = group),\n               fill = NA, color = \"white\", linewidth = 0.5) +\n  coord_sf(datum = NA) + \n  scale_fill_gradient2(\n    low = \"#2469b3\",\n    mid = \"#f0eee5\",\n    high = \"#b6202c\",\n    midpoint = median(df_depression$data_value/100, na.rm = TRUE),\n    labels = scales::percent,\n    name = ''\n  ) + \n  ggtitle('Rates of depression among adults are highest in\\nCentral Appalachia and the Bible Belt',\n          subtitle = '% of adults over 18 years old by U.S. county in 2022') + \n  labs(x='',\n       y='') + \n  labs(caption='Source: U.S. Centers for Disease Control (PLACES product)') + \n  guides(color='none') + \n  theme_minimal() +\n  theme(\n    text = element_text(family = 'Roboto Condensed'),\n    plot.title = element_text(face = 'bold', size=20), \n    plot.subtitle = element_text(size=14),\n    legend.position = \"top\",\n    legend.direction = \"horizontal\",\n    legend.key.height = unit(0.5, \"cm\"),\n    legend.key.width = unit(2, \"cm\"),\n    legend.title.position = \"top\",\n    legend.title = element_blank(),\n    legend.background = element_rect(fill = \"white\", color = NA),\n    legend.spacing.x = unit(0.2, 'cm')\n  ))\n\n\n\n\n\n\n\n\n\n\n\nTo incorporate Alaska and Hawaii with the lower 48 map, I’ll create individual maps for each and then use patchwork::inset_element() to overlay their images in the bottom left corner of the graphic.\n(Note: for these, I used Claude to provide me with the map projections and coordinates – I didn’t know these or use trial and error.)\n\n(ak_map &lt;- df_depression %&gt;% \n  filter(data_value_type == 'Crude prevalence',\n         stateabbr %in% c('AK')) %&gt;% \n  ggplot(.) + \n  geom_sf(aes(fill=data_value/100), color='white') + \n  coord_sf(crs = st_crs(3338), # Alaska Albers projection\n           xlim = c(-2400000, 1600000),\n           ylim = c(400000, 2500000),\n           datum = NA) +\n  scale_fill_gradient2(\n    low = \"#2469b3\",\n    mid = \"#f0eee5\",\n    high = \"#b6202c\",\n    midpoint = median(df_depression$data_value/100, na.rm = TRUE)\n  ) + \n  labs(x='',\n       y='') + \n  labs() + \n  guides(fill='none', color='none') + \n  theme_minimal())\n\n\n\n\n\n\n\n\n\n(hi_map &lt;- df_depression %&gt;% \n  filter(data_value_type == 'Crude prevalence',\n         stateabbr %in% c('HI')) %&gt;% \n  ggplot(.) + \n  geom_sf(aes(fill=data_value/100), color='white') + \n  coord_sf(crs = st_crs(4135), # Hawaii Albers projection\n           xlim = c(-161, -154),\n           ylim = c(18.5, 22.5),\n           datum = NA) +\n  scale_fill_gradient2(\n    low = \"#2469b3\",\n    mid = \"#f0eee5\",\n    high = \"#b6202c\",\n    midpoint = median(df_depression$data_value/100, na.rm = TRUE)\n  ) + \n  labs(x='',\n       y='') + \n  labs() + \n  guides(fill='none', color='none') + \n  theme_minimal())\n\n\n\n\n\n\n\n\nThen, with the patchwork::inset_element() function, I overlayed the Alaska and Hawaii maps to show all 50 states together, as is commonly done.\n\n(combined_map &lt;- lower48_map +\n  inset_element(ak_map, left = -.1, bottom = -.1, right = .3, top = .3) +\n  inset_element(hi_map, left = .1, bottom = -.1, right = 0.5, top = 0.2))\n\n\n\n\n\n\n\n\nThis map shows some strong regional patterns of depression in the U.S., where the Pacific Northwest, the Northeast, and the Bible Belt/ Rust Belt states all show higher rates of depression among adults."
  },
  {
    "objectID": "posts/2024-10-01-cdcplaces-pt2/index.html#setup-and-the-cdcplaces-package",
    "href": "posts/2024-10-01-cdcplaces-pt2/index.html#setup-and-the-cdcplaces-package",
    "title": "Creating great U.S. county maps with advanced ggplot styling",
    "section": "",
    "text": "library(tidyverse)\nlibrary(CDCPLACES)\n\nWarning: package 'CDCPLACES' was built under R version 4.4.1\n\nlibrary(ggtext)\nlibrary(maps)\nlibrary(sf)\nlibrary(patchwork)\n\nIn Part One, I used the CDCPLACES:: package to easily access and visualize health data from the Centers for Disease Control. In this post, I’ll demonstrate how to improve the visual style of a U.S. map, while using data from the CDC PLACES project."
  },
  {
    "objectID": "posts/2024-10-01-cdcplaces-pt2/index.html#styling-the-map",
    "href": "posts/2024-10-01-cdcplaces-pt2/index.html#styling-the-map",
    "title": "Creating great U.S. county maps with advanced ggplot styling",
    "section": "",
    "text": "I really like the aesthetic of maps created by the American Inequality project, which uses Datawrapper as its visualization tool for their newsletter. There are several elements of their aesthetic that I think are effective (see the example map below):\n\n\n\nA map showing the distribution of poor mental health days per month by U.S. county\n\n\n\nThe diverging color palette makes it very clear where counties stand1\nThe font (Roboto) is a sans serif font, which is more friendly for visual accessibility\nI like the positioning of the legend (top and horizontal), because I think it’s an efficient use of space, and because it reduces the amount of eye movement between the map, to the legend, and back.\nThe state boundaries are more pronounced than the county boundaries, which makes the map more immediately relatable2\nThe placement of Alaska and Hawaii allow for a condensed graphic, which I think is very clean for distributing in digital platforms\n\n\ndf_depression &lt;- get_places(geography = 'county',\n                            measure = 'DEPRESSION',\n                            release = '2024',\n                            geometry = TRUE)"
  },
  {
    "objectID": "posts/2024-10-01-cdcplaces-pt2/index.html#diverging-palette",
    "href": "posts/2024-10-01-cdcplaces-pt2/index.html#diverging-palette",
    "title": "Creating great U.S. county maps with advanced ggplot styling",
    "section": "",
    "text": "(lower48_map &lt;- df_depression %&gt;% \n  filter(data_value_type == 'Crude prevalence',\n         !stateabbr %in% c('AK', 'HI')) %&gt;% \n  ggplot(.) + \n  geom_sf(aes(fill=data_value/100)) + \n  coord_sf(datum = NA) + \n  scale_fill_gradient2(\n    low = \"#2469b3\",\n    mid = \"#f0eee5\",\n    high = \"#b6202c\",\n    midpoint = median(df_depression$data_value/100, na.rm = TRUE),\n    labels = scales::percent,\n    name = ''\n  ) + \n  ggtitle('Depression among adults aged &gt;= 18 years',\n          subtitle = 'by U.S. county in 2022') + \n  labs(x='',\n       y='') + \n  labs(caption='Source: U.S. Centers for Disease Control (PLACES product)') + \n  guides(color='none') + \n  theme_minimal())"
  },
  {
    "objectID": "posts/2024-10-01-cdcplaces-pt2/index.html#updating-font-and-legend",
    "href": "posts/2024-10-01-cdcplaces-pt2/index.html#updating-font-and-legend",
    "title": "Creating great U.S. county maps with advanced ggplot styling",
    "section": "",
    "text": "(lower48_map &lt;- df_depression %&gt;% \n  filter(data_value_type == 'Crude prevalence',\n         !stateabbr %in% c('AK', 'HI')) %&gt;% \n  ggplot(.) + \n  geom_sf(aes(fill=data_value/100)) + \n  coord_sf(datum = NA) + \n  scale_fill_gradient2(\n    low = \"#2469b3\",\n    mid = \"#f0eee5\",\n    high = \"#b6202c\",\n    midpoint = median(df_depression$data_value/100, na.rm = TRUE),\n    labels = scales::percent,\n    name = ''\n  ) + \n  ggtitle('Rates of depression among adults are highest in\\nCentral Appalachia and the Bible Belt',\n          subtitle = '% of adults over 18 years old by U.S. county in 2022') + \n  labs(x='',\n       y='') + \n  labs(caption='Source: U.S. Centers for Disease Control (PLACES product)') + \n  guides(color='none') + \n  theme_minimal() + \n  theme(\n    text = element_text(family = 'Roboto Condensed'),\n    plot.title = element_text(face = 'bold', size=20),\n    plot.subtitle = element_text(size=14),\n    legend.position = \"top\",\n    legend.direction = \"horizontal\",\n    legend.key.height = unit(0.5, \"cm\"),\n    legend.key.width = unit(2, \"cm\"),\n    legend.title.position = \"top\",\n    legend.title = element_blank(),\n    legend.background = element_rect(fill = \"white\", color = NA),\n    legend.spacing.x = unit(0.2, 'cm')\n    ))"
  },
  {
    "objectID": "posts/2024-10-01-cdcplaces-pt2/index.html#updating-state-boundaries",
    "href": "posts/2024-10-01-cdcplaces-pt2/index.html#updating-state-boundaries",
    "title": "Creating great U.S. county maps with advanced ggplot styling",
    "section": "",
    "text": "I want to the map to have thicker state boundaries than county boundaries – I think that’s a nice look and a nice way to orient readers to differences in state outcomes. I’ll use the maps::map_data() function to pull in the state boundaries and overlay those onto my county-based data.\n\nstates &lt;- map_data(\"state\")\n\n\n(lower48_map &lt;- df_depression %&gt;% \n  filter(data_value_type == 'Crude prevalence',\n         !stateabbr %in% c('AK', 'HI')) %&gt;% \n  ggplot(.) + \n  geom_sf(aes(fill=data_value/100), color='white') + \n  # this is what imitates the Datawrapper aesthetic of thicker state borders than county borders\n  geom_polygon(data = states, \n               aes(x = long, y = lat, group = group),\n               fill = NA, color = \"white\", linewidth = 0.5) +\n  coord_sf(datum = NA) + \n  scale_fill_gradient2(\n    low = \"#2469b3\",\n    mid = \"#f0eee5\",\n    high = \"#b6202c\",\n    midpoint = median(df_depression$data_value/100, na.rm = TRUE),\n    labels = scales::percent,\n    name = ''\n  ) + \n  ggtitle('Rates of depression among adults are highest in\\nCentral Appalachia and the Bible Belt',\n          subtitle = '% of adults over 18 years old by U.S. county in 2022') + \n  labs(x='',\n       y='') + \n  labs(caption='Source: U.S. Centers for Disease Control (PLACES product)') + \n  guides(color='none') + \n  theme_minimal() +\n  theme(\n    text = element_text(family = 'Roboto Condensed'),\n    plot.title = element_text(face = 'bold', size=20), \n    plot.subtitle = element_text(size=14),\n    legend.position = \"top\",\n    legend.direction = \"horizontal\",\n    legend.key.height = unit(0.5, \"cm\"),\n    legend.key.width = unit(2, \"cm\"),\n    legend.title.position = \"top\",\n    legend.title = element_blank(),\n    legend.background = element_rect(fill = \"white\", color = NA),\n    legend.spacing.x = unit(0.2, 'cm')\n  ))"
  },
  {
    "objectID": "posts/2024-10-01-cdcplaces-pt2/index.html#including-alaska-and-hawaii",
    "href": "posts/2024-10-01-cdcplaces-pt2/index.html#including-alaska-and-hawaii",
    "title": "Creating great U.S. county maps with advanced ggplot styling",
    "section": "",
    "text": "To incorporate Alaska and Hawaii with the lower 48 map, I’ll create individual maps for each and then use patchwork::inset_element() to overlay their images in the bottom left corner of the graphic.\n(Note: for these, I used Claude to provide me with the map projections and coordinates – I didn’t know these or use trial and error.)\n\n(ak_map &lt;- df_depression %&gt;% \n  filter(data_value_type == 'Crude prevalence',\n         stateabbr %in% c('AK')) %&gt;% \n  ggplot(.) + \n  geom_sf(aes(fill=data_value/100), color='white') + \n  coord_sf(crs = st_crs(3338), # Alaska Albers projection\n           xlim = c(-2400000, 1600000),\n           ylim = c(400000, 2500000),\n           datum = NA) +\n  scale_fill_gradient2(\n    low = \"#2469b3\",\n    mid = \"#f0eee5\",\n    high = \"#b6202c\",\n    midpoint = median(df_depression$data_value/100, na.rm = TRUE)\n  ) + \n  labs(x='',\n       y='') + \n  labs() + \n  guides(fill='none', color='none') + \n  theme_minimal())\n\n\n\n\n\n\n\n\n\n(hi_map &lt;- df_depression %&gt;% \n  filter(data_value_type == 'Crude prevalence',\n         stateabbr %in% c('HI')) %&gt;% \n  ggplot(.) + \n  geom_sf(aes(fill=data_value/100), color='white') + \n  coord_sf(crs = st_crs(4135), # Hawaii Albers projection\n           xlim = c(-161, -154),\n           ylim = c(18.5, 22.5),\n           datum = NA) +\n  scale_fill_gradient2(\n    low = \"#2469b3\",\n    mid = \"#f0eee5\",\n    high = \"#b6202c\",\n    midpoint = median(df_depression$data_value/100, na.rm = TRUE)\n  ) + \n  labs(x='',\n       y='') + \n  labs() + \n  guides(fill='none', color='none') + \n  theme_minimal())\n\n\n\n\n\n\n\n\nThen, with the patchwork::inset_element() function, I overlayed the Alaska and Hawaii maps to show all 50 states together, as is commonly done.\n\n(combined_map &lt;- lower48_map +\n  inset_element(ak_map, left = -.1, bottom = -.1, right = .3, top = .3) +\n  inset_element(hi_map, left = .1, bottom = -.1, right = 0.5, top = 0.2))\n\n\n\n\n\n\n\n\nThis map shows some strong regional patterns of depression in the U.S., where the Pacific Northwest, the Northeast, and the Bible Belt/ Rust Belt states all show higher rates of depression among adults."
  },
  {
    "objectID": "posts/2024-10-01-cdcplaces-pt2/index.html#footnotes",
    "href": "posts/2024-10-01-cdcplaces-pt2/index.html#footnotes",
    "title": "Creating great U.S. county maps with advanced ggplot styling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe viridis:: package provides a set of palettes that have are approved for visual accessibility (for those with color vision deficiencies). Generally speaking, I think it’s best to use these palettes, though they may present challenges when printed.↩︎\nAesthetically speaking, I also like the use of white for the boundaries.↩︎"
  },
  {
    "objectID": "posts/2019-06-18-clustering-african-health-outcomes/index.html",
    "href": "posts/2019-06-18-clustering-african-health-outcomes/index.html",
    "title": "PART ONE: Cluster Analysis of African Health Outcomes",
    "section": "",
    "text": "The United Nations created the Sustainable Development Goals (SDGs) to set an ambitious global development agenda to work toward by 2030. A continental development organization in Africa asked, how could we think about SDG 3 (the health goals) in a holistic and aggregate sense?\nTo get a clearer picture of how health outcomes are spread across the continent, I use cluster analysis to group countries by primary health outcomes data.\n\n\n\nThis analysis uses results from the International Futures global forecasting model and its Base Case (or Current Path) scenario. The dataset contains country-level health outcomes projections from 2015-2065. For this clustering exercise, I am using the 2019 results. The variables in this analysis include:\n\nDR_OthCommumDis: Death Rate from other communicable diseases\n\nDR_MaligNeoPl: Death Rate from malignant neoplasms (cancers)\nDR_CardioVasc: Death Rate from cariovascular diseases\nDR_Digestive: Death Rate from digestive illnesses\nDR_Respiratory: Death Rate from respiratory illnesses\nDR_OtherNonComm: Death Rate from other non-communicable diseases\nDR_TrafficAcc: Death Rate from traffic accidents\nDR_UnIntInj: Death Rate from unintentional injuries\nDR_IntInj: Death Rate from intentional injuries\nDR_Diabetes: Death Rate from diabetes\nDR_AIDS: Death Rate from HIV/AIDS\nDR_Diarrhea: Death Rate from diarrheal illnesses\nDR_Malaria: Death Rate from malaria\nDR_RespInfec: Death Rate from respiratory infections\nDR_MentalHealth: Death Rate from mental health illnesses\nCLPC: Calories consumed per capita\nHLSTUNT: Stunting rate for under-5 population\nHLSMOKING: Smoking rate\nINFMOR: Infant mortality rate (deaths per 1,000 live births)\nMATMORTRATIO: Maternal mortality rate (deaths per 100,000 live births)\n\nFor more information on these fields, see the International Futures Health Model documentation.\n\n\n\n\nlibrary(tidyverse)\nlibrary(factoextra)\nlibrary(NbClust)\nlibrary(ggsci)              #for jco color palette\nlibrary(psych)              \n\nmy.theme &lt;- theme(\n  plot.title = element_text(color=\"black\", face=\"plain\", size=17, hjust=0), \n  plot.subtitle = element_text(color=\"black\", size=15, hjust=0), \n  axis.title = element_text(color=\"black\", face=\"plain\", size=14), \n  axis.text.y = element_text(size=16), \n  axis.text.x = element_text(angle = 90, hjust = 1, size=13), \n  plot.caption = element_text(color=\"black\", size=13), \n  panel.background =  element_rect(fill = \"#F7F7F7\", colour = NA), \n  panel.grid.major = element_line(colour = \"grey90\", linewidth = 0.5),\n  panel.grid.minor = element_line(colour = \"grey93\", linewidth = 0.5),\n  panel.border = element_rect(colour = \"black\", linewidth = 0.5, fill=NA, linetype = 1),\n  legend.title=element_blank(), \n  legend.text = element_text(color=\"black\", size=14, hjust=0),\n  legend.spacing.x = unit(.5, 'cm'), \n  legend.position = 'bottom',\n  strip.text = element_text(color=\"black\", face=\"plain\", size=16),\n  strip.background = element_rect(fill = \"white\"))\n\n\ndf &lt;- readxl::read_xlsx('.//data/cluster_health_afr_14mar2019.xlsx')\n\ndf %&gt;% glimpse()\n\nRows: 1,080\nColumns: 56\n$ variable &lt;chr&gt; \"DR_OthCommumDis\", \"DR_MaligNeoPl\", \"DR_CardioVasc\", \"DR_Dige…\n$ country  &lt;chr&gt; \"Algeria\", \"Algeria\", \"Algeria\", \"Algeria\", \"Algeria\", \"Alger…\n$ dim      &lt;chr&gt; \"Total\", \"Total\", \"Total\", \"Total\", \"Total\", \"Total\", \"Total\"…\n$ unit     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ scenario &lt;chr&gt; \"Base\", \"Base\", \"Base\", \"Base\", \"Base\", \"Base\", \"Base\", \"Base…\n$ `2015`   &lt;dbl&gt; 0.4450, 0.5120, 1.7640, 0.0630, 0.1530, 0.6220, 0.2760, 0.146…\n$ `2016`   &lt;dbl&gt; 0.4250, 0.5170, 1.8020, 0.0640, 0.1570, 0.6180, 0.2800, 0.146…\n$ `2017`   &lt;dbl&gt; 0.4100, 0.5210, 1.8300, 0.0640, 0.1610, 0.6170, 0.2790, 0.148…\n$ `2018`   &lt;dbl&gt; 0.3940, 0.5260, 1.8630, 0.0640, 0.1640, 0.6160, 0.2780, 0.149…\n$ `2019`   &lt;dbl&gt; 0.3770, 0.5310, 1.8940, 0.0650, 0.1680, 0.6140, 0.2780, 0.150…\n$ `2020`   &lt;dbl&gt; 0.3600, 0.5360, 1.9260, 0.0660, 0.1720, 0.6110, 0.2780, 0.151…\n$ `2021`   &lt;dbl&gt; 0.3430, 0.5420, 1.9530, 0.0670, 0.1760, 0.6080, 0.2780, 0.151…\n$ `2022`   &lt;dbl&gt; 0.3260, 0.5480, 1.9830, 0.0670, 0.1800, 0.6050, 0.2790, 0.152…\n$ `2023`   &lt;dbl&gt; 0.3110, 0.5550, 2.0140, 0.0680, 0.1850, 0.6040, 0.2790, 0.153…\n$ `2024`   &lt;dbl&gt; 0.2970, 0.5610, 2.0430, 0.0690, 0.1890, 0.6030, 0.2790, 0.153…\n$ `2025`   &lt;dbl&gt; 0.2840, 0.5680, 2.0720, 0.0700, 0.1940, 0.6020, 0.2800, 0.154…\n$ `2026`   &lt;dbl&gt; 0.2720, 0.5750, 2.1020, 0.0710, 0.1980, 0.6030, 0.2810, 0.154…\n$ `2027`   &lt;dbl&gt; 0.2610, 0.5820, 2.1330, 0.0720, 0.2030, 0.6030, 0.2830, 0.154…\n$ `2028`   &lt;dbl&gt; 0.2500, 0.5900, 2.1680, 0.0730, 0.2080, 0.6050, 0.2840, 0.155…\n$ `2029`   &lt;dbl&gt; 0.2400, 0.5980, 2.2020, 0.0740, 0.2140, 0.6070, 0.2860, 0.155…\n$ `2030`   &lt;dbl&gt; 0.2320, 0.6050, 2.2400, 0.0760, 0.2190, 0.6100, 0.2880, 0.156…\n$ `2031`   &lt;dbl&gt; 0.2230, 0.6130, 2.2760, 0.0770, 0.2250, 0.6140, 0.2910, 0.157…\n$ `2032`   &lt;dbl&gt; 0.2160, 0.6210, 2.3170, 0.0780, 0.2310, 0.6180, 0.2930, 0.157…\n$ `2033`   &lt;dbl&gt; 0.2080, 0.6290, 2.3580, 0.0800, 0.2380, 0.6230, 0.2950, 0.158…\n$ `2034`   &lt;dbl&gt; 0.2020, 0.6370, 2.4020, 0.0810, 0.2440, 0.6280, 0.2980, 0.159…\n$ `2035`   &lt;dbl&gt; 0.1960, 0.6440, 2.4440, 0.0830, 0.2510, 0.6330, 0.3000, 0.159…\n$ `2036`   &lt;dbl&gt; 0.1900, 0.6520, 2.4870, 0.0840, 0.2580, 0.6390, 0.3030, 0.160…\n$ `2037`   &lt;dbl&gt; 0.1840, 0.6600, 2.5320, 0.0860, 0.2650, 0.6450, 0.3050, 0.161…\n$ `2038`   &lt;dbl&gt; 0.1790, 0.6680, 2.5770, 0.0870, 0.2720, 0.6510, 0.3080, 0.162…\n$ `2039`   &lt;dbl&gt; 0.1740, 0.6750, 2.6220, 0.0890, 0.2790, 0.6570, 0.3100, 0.163…\n$ `2040`   &lt;dbl&gt; 0.1690, 0.6830, 2.6660, 0.0910, 0.2870, 0.6640, 0.3130, 0.165…\n$ `2041`   &lt;dbl&gt; 0.1650, 0.6910, 2.7110, 0.0930, 0.2950, 0.6700, 0.3150, 0.166…\n$ `2042`   &lt;dbl&gt; 0.1610, 0.6990, 2.7550, 0.0940, 0.3030, 0.6770, 0.3170, 0.167…\n$ `2043`   &lt;dbl&gt; 0.1580, 0.7070, 2.8010, 0.0960, 0.3110, 0.6850, 0.3190, 0.169…\n$ `2044`   &lt;dbl&gt; 0.1560, 0.7150, 2.8470, 0.0990, 0.3190, 0.6940, 0.3210, 0.171…\n$ `2045`   &lt;dbl&gt; 0.1540, 0.7220, 2.8920, 0.1010, 0.3270, 0.7020, 0.3230, 0.173…\n$ `2046`   &lt;dbl&gt; 0.1510, 0.7290, 2.9410, 0.1030, 0.3360, 0.7110, 0.3240, 0.175…\n$ `2047`   &lt;dbl&gt; 0.1490, 0.7360, 2.9880, 0.1050, 0.3450, 0.7190, 0.3250, 0.177…\n$ `2048`   &lt;dbl&gt; 0.1460, 0.7430, 3.0370, 0.1070, 0.3540, 0.7280, 0.3260, 0.179…\n$ `2049`   &lt;dbl&gt; 0.1430, 0.7500, 3.0860, 0.1100, 0.3630, 0.7370, 0.3280, 0.182…\n$ `2050`   &lt;dbl&gt; 0.1400, 0.7560, 3.1340, 0.1120, 0.3720, 0.7440, 0.3290, 0.184…\n$ `2051`   &lt;dbl&gt; 0.1360, 0.7630, 3.1780, 0.1150, 0.3810, 0.7510, 0.3300, 0.186…\n$ `2052`   &lt;dbl&gt; 0.1330, 0.7690, 3.2230, 0.1170, 0.3900, 0.7570, 0.3310, 0.188…\n$ `2053`   &lt;dbl&gt; 0.1300, 0.7750, 3.2690, 0.1190, 0.4000, 0.7640, 0.3320, 0.190…\n$ `2054`   &lt;dbl&gt; 0.1260, 0.7810, 3.3190, 0.1220, 0.4100, 0.7720, 0.3330, 0.193…\n$ `2055`   &lt;dbl&gt; 0.1230, 0.7870, 3.3690, 0.1250, 0.4200, 0.7790, 0.3330, 0.195…\n$ `2056`   &lt;dbl&gt; 0.1200, 0.7920, 3.4220, 0.1280, 0.4300, 0.7870, 0.3340, 0.198…\n$ `2057`   &lt;dbl&gt; 0.1180, 0.7970, 3.4830, 0.1300, 0.4410, 0.7970, 0.3340, 0.201…\n$ `2058`   &lt;dbl&gt; 0.1150, 0.8030, 3.5360, 0.1330, 0.4520, 0.8050, 0.3350, 0.204…\n$ `2059`   &lt;dbl&gt; 0.1120, 0.8080, 3.5950, 0.1370, 0.4640, 0.8130, 0.3360, 0.207…\n$ `2060`   &lt;dbl&gt; 0.1100, 0.8130, 3.6500, 0.1400, 0.4750, 0.8210, 0.3360, 0.210…\n$ `2061`   &lt;dbl&gt; 0.108, 0.817, 3.705, 0.143, 0.487, 0.829, 0.337, 0.213, 0.061…\n$ `2062`   &lt;dbl&gt; 0.105, 0.822, 3.755, 0.146, 0.498, 0.835, 0.338, 0.215, 0.061…\n$ `2063`   &lt;dbl&gt; 0.103, 0.826, 3.805, 0.149, 0.509, 0.842, 0.339, 0.218, 0.061…\n$ `2064`   &lt;dbl&gt; 0.101, 0.830, 3.852, 0.152, 0.520, 0.848, 0.340, 0.222, 0.061…\n$ `2065`   &lt;dbl&gt; 0.099, 0.833, 3.894, 0.155, 0.531, 0.854, 0.340, 0.224, 0.061…\n\n\n\n(df &lt;- df %&gt;% \n  #pivot data\n   gather(year, val, 6:56) %&gt;% \n  #filter on 2019 results\n   filter(year == '2019') %&gt;% \n  #select variable, country, and val\n   select(1:2, 7) %&gt;% \n  #group by variable and generate z-score for clustering\n   group_by(variable) %&gt;% \n   mutate(val = scale(val)) %&gt;% \n   ungroup() %&gt;% \n   spread(variable, val)) \n\n# A tibble: 54 × 21\n   country       CLPC DR_AIDS DR_CardioVasc DR_Diabetes DR_Diarrhea DR_Digestive\n   &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n 1 Algeria     1.63    -0.948         1.19       0.102      -1.42        -1.95  \n 2 Angola     -0.0927  -0.427        -0.951     -0.510       1.10         0.443 \n 3 Benin       0.609   -0.610        -0.427     -0.385       0.815        0.0179\n 4 Botswana   -0.846    1.68         -0.224      0.835      -0.601       -1.28  \n 5 Burkina F…  0.821   -0.709        -0.624     -0.471      -0.0198       0.307 \n 6 Burundi    -1.41    -0.636        -0.777     -0.436       2.16         0.597 \n 7 Cameroon    0.821    0.663        -0.513     -0.178      -0.117        0.346 \n 8 Cape Verde -0.494   -0.637         0.788      0.0205     -1.30        -1.62  \n 9 Central A… -1.29     0.902         0.166     -0.367       1.56         1.81  \n10 Chad       -0.887   -0.621        -0.466     -0.428       3.26         1.48  \n# ℹ 44 more rows\n# ℹ 14 more variables: DR_IntInj &lt;dbl&gt;, DR_Malaria &lt;dbl&gt;, DR_MaligNeoPl &lt;dbl&gt;,\n#   DR_MentalHealth &lt;dbl&gt;, DR_OthCommumDis &lt;dbl&gt;, DR_OtherNonComm &lt;dbl&gt;,\n#   DR_RespInfec &lt;dbl&gt;, DR_Respiratory &lt;dbl&gt;, DR_TrafficAcc &lt;dbl&gt;,\n#   DR_UnIntInj &lt;dbl&gt;, HLSMOKING &lt;dbl&gt;, HLSTUNT &lt;dbl&gt;, INFMOR &lt;dbl&gt;,\n#   MATMORTRATIO &lt;dbl&gt;"
  },
  {
    "objectID": "posts/2019-06-18-clustering-african-health-outcomes/index.html#purpose",
    "href": "posts/2019-06-18-clustering-african-health-outcomes/index.html#purpose",
    "title": "PART ONE: Cluster Analysis of African Health Outcomes",
    "section": "",
    "text": "The United Nations created the Sustainable Development Goals (SDGs) to set an ambitious global development agenda to work toward by 2030. A continental development organization in Africa asked, how could we think about SDG 3 (the health goals) in a holistic and aggregate sense?\nTo get a clearer picture of how health outcomes are spread across the continent, I use cluster analysis to group countries by primary health outcomes data."
  },
  {
    "objectID": "posts/2019-06-18-clustering-african-health-outcomes/index.html#dataset",
    "href": "posts/2019-06-18-clustering-african-health-outcomes/index.html#dataset",
    "title": "PART ONE: Cluster Analysis of African Health Outcomes",
    "section": "",
    "text": "This analysis uses results from the International Futures global forecasting model and its Base Case (or Current Path) scenario. The dataset contains country-level health outcomes projections from 2015-2065. For this clustering exercise, I am using the 2019 results. The variables in this analysis include:\n\nDR_OthCommumDis: Death Rate from other communicable diseases\n\nDR_MaligNeoPl: Death Rate from malignant neoplasms (cancers)\nDR_CardioVasc: Death Rate from cariovascular diseases\nDR_Digestive: Death Rate from digestive illnesses\nDR_Respiratory: Death Rate from respiratory illnesses\nDR_OtherNonComm: Death Rate from other non-communicable diseases\nDR_TrafficAcc: Death Rate from traffic accidents\nDR_UnIntInj: Death Rate from unintentional injuries\nDR_IntInj: Death Rate from intentional injuries\nDR_Diabetes: Death Rate from diabetes\nDR_AIDS: Death Rate from HIV/AIDS\nDR_Diarrhea: Death Rate from diarrheal illnesses\nDR_Malaria: Death Rate from malaria\nDR_RespInfec: Death Rate from respiratory infections\nDR_MentalHealth: Death Rate from mental health illnesses\nCLPC: Calories consumed per capita\nHLSTUNT: Stunting rate for under-5 population\nHLSMOKING: Smoking rate\nINFMOR: Infant mortality rate (deaths per 1,000 live births)\nMATMORTRATIO: Maternal mortality rate (deaths per 100,000 live births)\n\nFor more information on these fields, see the International Futures Health Model documentation."
  },
  {
    "objectID": "posts/2019-06-18-clustering-african-health-outcomes/index.html#setup-and-data-preparation",
    "href": "posts/2019-06-18-clustering-african-health-outcomes/index.html#setup-and-data-preparation",
    "title": "PART ONE: Cluster Analysis of African Health Outcomes",
    "section": "",
    "text": "library(tidyverse)\nlibrary(factoextra)\nlibrary(NbClust)\nlibrary(ggsci)              #for jco color palette\nlibrary(psych)              \n\nmy.theme &lt;- theme(\n  plot.title = element_text(color=\"black\", face=\"plain\", size=17, hjust=0), \n  plot.subtitle = element_text(color=\"black\", size=15, hjust=0), \n  axis.title = element_text(color=\"black\", face=\"plain\", size=14), \n  axis.text.y = element_text(size=16), \n  axis.text.x = element_text(angle = 90, hjust = 1, size=13), \n  plot.caption = element_text(color=\"black\", size=13), \n  panel.background =  element_rect(fill = \"#F7F7F7\", colour = NA), \n  panel.grid.major = element_line(colour = \"grey90\", linewidth = 0.5),\n  panel.grid.minor = element_line(colour = \"grey93\", linewidth = 0.5),\n  panel.border = element_rect(colour = \"black\", linewidth = 0.5, fill=NA, linetype = 1),\n  legend.title=element_blank(), \n  legend.text = element_text(color=\"black\", size=14, hjust=0),\n  legend.spacing.x = unit(.5, 'cm'), \n  legend.position = 'bottom',\n  strip.text = element_text(color=\"black\", face=\"plain\", size=16),\n  strip.background = element_rect(fill = \"white\"))\n\n\ndf &lt;- readxl::read_xlsx('.//data/cluster_health_afr_14mar2019.xlsx')\n\ndf %&gt;% glimpse()\n\nRows: 1,080\nColumns: 56\n$ variable &lt;chr&gt; \"DR_OthCommumDis\", \"DR_MaligNeoPl\", \"DR_CardioVasc\", \"DR_Dige…\n$ country  &lt;chr&gt; \"Algeria\", \"Algeria\", \"Algeria\", \"Algeria\", \"Algeria\", \"Alger…\n$ dim      &lt;chr&gt; \"Total\", \"Total\", \"Total\", \"Total\", \"Total\", \"Total\", \"Total\"…\n$ unit     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ scenario &lt;chr&gt; \"Base\", \"Base\", \"Base\", \"Base\", \"Base\", \"Base\", \"Base\", \"Base…\n$ `2015`   &lt;dbl&gt; 0.4450, 0.5120, 1.7640, 0.0630, 0.1530, 0.6220, 0.2760, 0.146…\n$ `2016`   &lt;dbl&gt; 0.4250, 0.5170, 1.8020, 0.0640, 0.1570, 0.6180, 0.2800, 0.146…\n$ `2017`   &lt;dbl&gt; 0.4100, 0.5210, 1.8300, 0.0640, 0.1610, 0.6170, 0.2790, 0.148…\n$ `2018`   &lt;dbl&gt; 0.3940, 0.5260, 1.8630, 0.0640, 0.1640, 0.6160, 0.2780, 0.149…\n$ `2019`   &lt;dbl&gt; 0.3770, 0.5310, 1.8940, 0.0650, 0.1680, 0.6140, 0.2780, 0.150…\n$ `2020`   &lt;dbl&gt; 0.3600, 0.5360, 1.9260, 0.0660, 0.1720, 0.6110, 0.2780, 0.151…\n$ `2021`   &lt;dbl&gt; 0.3430, 0.5420, 1.9530, 0.0670, 0.1760, 0.6080, 0.2780, 0.151…\n$ `2022`   &lt;dbl&gt; 0.3260, 0.5480, 1.9830, 0.0670, 0.1800, 0.6050, 0.2790, 0.152…\n$ `2023`   &lt;dbl&gt; 0.3110, 0.5550, 2.0140, 0.0680, 0.1850, 0.6040, 0.2790, 0.153…\n$ `2024`   &lt;dbl&gt; 0.2970, 0.5610, 2.0430, 0.0690, 0.1890, 0.6030, 0.2790, 0.153…\n$ `2025`   &lt;dbl&gt; 0.2840, 0.5680, 2.0720, 0.0700, 0.1940, 0.6020, 0.2800, 0.154…\n$ `2026`   &lt;dbl&gt; 0.2720, 0.5750, 2.1020, 0.0710, 0.1980, 0.6030, 0.2810, 0.154…\n$ `2027`   &lt;dbl&gt; 0.2610, 0.5820, 2.1330, 0.0720, 0.2030, 0.6030, 0.2830, 0.154…\n$ `2028`   &lt;dbl&gt; 0.2500, 0.5900, 2.1680, 0.0730, 0.2080, 0.6050, 0.2840, 0.155…\n$ `2029`   &lt;dbl&gt; 0.2400, 0.5980, 2.2020, 0.0740, 0.2140, 0.6070, 0.2860, 0.155…\n$ `2030`   &lt;dbl&gt; 0.2320, 0.6050, 2.2400, 0.0760, 0.2190, 0.6100, 0.2880, 0.156…\n$ `2031`   &lt;dbl&gt; 0.2230, 0.6130, 2.2760, 0.0770, 0.2250, 0.6140, 0.2910, 0.157…\n$ `2032`   &lt;dbl&gt; 0.2160, 0.6210, 2.3170, 0.0780, 0.2310, 0.6180, 0.2930, 0.157…\n$ `2033`   &lt;dbl&gt; 0.2080, 0.6290, 2.3580, 0.0800, 0.2380, 0.6230, 0.2950, 0.158…\n$ `2034`   &lt;dbl&gt; 0.2020, 0.6370, 2.4020, 0.0810, 0.2440, 0.6280, 0.2980, 0.159…\n$ `2035`   &lt;dbl&gt; 0.1960, 0.6440, 2.4440, 0.0830, 0.2510, 0.6330, 0.3000, 0.159…\n$ `2036`   &lt;dbl&gt; 0.1900, 0.6520, 2.4870, 0.0840, 0.2580, 0.6390, 0.3030, 0.160…\n$ `2037`   &lt;dbl&gt; 0.1840, 0.6600, 2.5320, 0.0860, 0.2650, 0.6450, 0.3050, 0.161…\n$ `2038`   &lt;dbl&gt; 0.1790, 0.6680, 2.5770, 0.0870, 0.2720, 0.6510, 0.3080, 0.162…\n$ `2039`   &lt;dbl&gt; 0.1740, 0.6750, 2.6220, 0.0890, 0.2790, 0.6570, 0.3100, 0.163…\n$ `2040`   &lt;dbl&gt; 0.1690, 0.6830, 2.6660, 0.0910, 0.2870, 0.6640, 0.3130, 0.165…\n$ `2041`   &lt;dbl&gt; 0.1650, 0.6910, 2.7110, 0.0930, 0.2950, 0.6700, 0.3150, 0.166…\n$ `2042`   &lt;dbl&gt; 0.1610, 0.6990, 2.7550, 0.0940, 0.3030, 0.6770, 0.3170, 0.167…\n$ `2043`   &lt;dbl&gt; 0.1580, 0.7070, 2.8010, 0.0960, 0.3110, 0.6850, 0.3190, 0.169…\n$ `2044`   &lt;dbl&gt; 0.1560, 0.7150, 2.8470, 0.0990, 0.3190, 0.6940, 0.3210, 0.171…\n$ `2045`   &lt;dbl&gt; 0.1540, 0.7220, 2.8920, 0.1010, 0.3270, 0.7020, 0.3230, 0.173…\n$ `2046`   &lt;dbl&gt; 0.1510, 0.7290, 2.9410, 0.1030, 0.3360, 0.7110, 0.3240, 0.175…\n$ `2047`   &lt;dbl&gt; 0.1490, 0.7360, 2.9880, 0.1050, 0.3450, 0.7190, 0.3250, 0.177…\n$ `2048`   &lt;dbl&gt; 0.1460, 0.7430, 3.0370, 0.1070, 0.3540, 0.7280, 0.3260, 0.179…\n$ `2049`   &lt;dbl&gt; 0.1430, 0.7500, 3.0860, 0.1100, 0.3630, 0.7370, 0.3280, 0.182…\n$ `2050`   &lt;dbl&gt; 0.1400, 0.7560, 3.1340, 0.1120, 0.3720, 0.7440, 0.3290, 0.184…\n$ `2051`   &lt;dbl&gt; 0.1360, 0.7630, 3.1780, 0.1150, 0.3810, 0.7510, 0.3300, 0.186…\n$ `2052`   &lt;dbl&gt; 0.1330, 0.7690, 3.2230, 0.1170, 0.3900, 0.7570, 0.3310, 0.188…\n$ `2053`   &lt;dbl&gt; 0.1300, 0.7750, 3.2690, 0.1190, 0.4000, 0.7640, 0.3320, 0.190…\n$ `2054`   &lt;dbl&gt; 0.1260, 0.7810, 3.3190, 0.1220, 0.4100, 0.7720, 0.3330, 0.193…\n$ `2055`   &lt;dbl&gt; 0.1230, 0.7870, 3.3690, 0.1250, 0.4200, 0.7790, 0.3330, 0.195…\n$ `2056`   &lt;dbl&gt; 0.1200, 0.7920, 3.4220, 0.1280, 0.4300, 0.7870, 0.3340, 0.198…\n$ `2057`   &lt;dbl&gt; 0.1180, 0.7970, 3.4830, 0.1300, 0.4410, 0.7970, 0.3340, 0.201…\n$ `2058`   &lt;dbl&gt; 0.1150, 0.8030, 3.5360, 0.1330, 0.4520, 0.8050, 0.3350, 0.204…\n$ `2059`   &lt;dbl&gt; 0.1120, 0.8080, 3.5950, 0.1370, 0.4640, 0.8130, 0.3360, 0.207…\n$ `2060`   &lt;dbl&gt; 0.1100, 0.8130, 3.6500, 0.1400, 0.4750, 0.8210, 0.3360, 0.210…\n$ `2061`   &lt;dbl&gt; 0.108, 0.817, 3.705, 0.143, 0.487, 0.829, 0.337, 0.213, 0.061…\n$ `2062`   &lt;dbl&gt; 0.105, 0.822, 3.755, 0.146, 0.498, 0.835, 0.338, 0.215, 0.061…\n$ `2063`   &lt;dbl&gt; 0.103, 0.826, 3.805, 0.149, 0.509, 0.842, 0.339, 0.218, 0.061…\n$ `2064`   &lt;dbl&gt; 0.101, 0.830, 3.852, 0.152, 0.520, 0.848, 0.340, 0.222, 0.061…\n$ `2065`   &lt;dbl&gt; 0.099, 0.833, 3.894, 0.155, 0.531, 0.854, 0.340, 0.224, 0.061…\n\n\n\n(df &lt;- df %&gt;% \n  #pivot data\n   gather(year, val, 6:56) %&gt;% \n  #filter on 2019 results\n   filter(year == '2019') %&gt;% \n  #select variable, country, and val\n   select(1:2, 7) %&gt;% \n  #group by variable and generate z-score for clustering\n   group_by(variable) %&gt;% \n   mutate(val = scale(val)) %&gt;% \n   ungroup() %&gt;% \n   spread(variable, val)) \n\n# A tibble: 54 × 21\n   country       CLPC DR_AIDS DR_CardioVasc DR_Diabetes DR_Diarrhea DR_Digestive\n   &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n 1 Algeria     1.63    -0.948         1.19       0.102      -1.42        -1.95  \n 2 Angola     -0.0927  -0.427        -0.951     -0.510       1.10         0.443 \n 3 Benin       0.609   -0.610        -0.427     -0.385       0.815        0.0179\n 4 Botswana   -0.846    1.68         -0.224      0.835      -0.601       -1.28  \n 5 Burkina F…  0.821   -0.709        -0.624     -0.471      -0.0198       0.307 \n 6 Burundi    -1.41    -0.636        -0.777     -0.436       2.16         0.597 \n 7 Cameroon    0.821    0.663        -0.513     -0.178      -0.117        0.346 \n 8 Cape Verde -0.494   -0.637         0.788      0.0205     -1.30        -1.62  \n 9 Central A… -1.29     0.902         0.166     -0.367       1.56         1.81  \n10 Chad       -0.887   -0.621        -0.466     -0.428       3.26         1.48  \n# ℹ 44 more rows\n# ℹ 14 more variables: DR_IntInj &lt;dbl&gt;, DR_Malaria &lt;dbl&gt;, DR_MaligNeoPl &lt;dbl&gt;,\n#   DR_MentalHealth &lt;dbl&gt;, DR_OthCommumDis &lt;dbl&gt;, DR_OtherNonComm &lt;dbl&gt;,\n#   DR_RespInfec &lt;dbl&gt;, DR_Respiratory &lt;dbl&gt;, DR_TrafficAcc &lt;dbl&gt;,\n#   DR_UnIntInj &lt;dbl&gt;, HLSMOKING &lt;dbl&gt;, HLSTUNT &lt;dbl&gt;, INFMOR &lt;dbl&gt;,\n#   MATMORTRATIO &lt;dbl&gt;"
  },
  {
    "objectID": "posts/2019-06-18-clustering-african-health-outcomes/index.html#analysis-of-results",
    "href": "posts/2019-06-18-clustering-african-health-outcomes/index.html#analysis-of-results",
    "title": "PART ONE: Cluster Analysis of African Health Outcomes",
    "section": "Analysis of results",
    "text": "Analysis of results\nFirst, I’ll use the fiz_cluster() function to visualize cluster membership. (There’s further clean-up needed, but I want to see the initial viz).\n\n(kmeans_viz &lt;- fviz_cluster(kmeans_sdg3, \n             data=df %&gt;% select(-1),\n             stand = FALSE, \n             ellipse.type = \"convex\",\n             palette = \"jco\") + \n  my.theme)\n\n\n\n\n\n\n\n\nWhat do each of these clusters mean? What do they represent?\n\ncluster_fill &lt;- c(\"#868686FF\", \"#CD534CFF\", \"#EFC000FF\", \"#0073C2FF\")\n\ndf_cluster_4_results %&gt;% \n  gather(variable, val, 4:23) %&gt;% \n  filter(variable %in% spot_vars) %&gt;% \n  mutate(variable = recode(variable,\n                           'DR_AIDS' = 'AIDS death rate',\n                           'DR_CardioVasc' = 'Cardiovascular death rate',\n                           'MATMORTRATIO' = 'Maternal mortality rate',\n                           'DR_Respiratory' = 'Respiratory mortality rate', \n                           'HLSTUNT' = 'Stunting rate',\n                           'DR_IntInj' = 'Intentional injury death rate', \n                           'DR_Malaria' = 'Malaria death rate', \n                           'DR_Diabetes' = 'Diabetes death rate'),\n         clust_name = case_when(.cluster == '1' ~ 'CDs',\n                                .cluster == '2' ~ 'NCDs',\n                                .cluster == '3' ~ 'NCDs & HIV',\n                                .cluster == '4' ~ 'Double Burden')) %&gt;% \n  ggplot(., \n         aes(x=.cluster,\n             y=val,\n             fill=.cluster)) + \n  geom_violin() + \n  geom_jitter(width = .2, height = .1) + \n  facet_wrap(~variable, \n             nrow = 2,\n             scales = 'free',\n             labeller = label_wrap_gen()) + \n  ggtitle('',\n          subtitle = 'Each dot represents an African country within its cluster') + \n  labs(y='z-score',\n       x='Cluster') + \n  my.theme + \n  scale_fill_manual(values = cluster_fill) + \n  theme(strip.text = element_text(size = 12),\n        strip.text.x = element_text(margin = margin(.25,0,.25,0, \"cm\")),\n        legend.position = 'none')\n\n\n\n\n\n\n\n\nCluster 1: Higher levels of non-communicable disease reflecting higher development.\nCluster 2: Highest burdens of HIV/AIDS, diabetes, respiratory illnesses, traffic fatalities, and deaths from intentional injuries.\nCluster 3: Countries in the middle of the “double burden” of disease, with elevated (but generally falling) burden of communicable disease and rising burdens from non-communicable disease.\nCluster 4: Communicable disease mortality characterizes these countries. Countries in this cluster are at the beginning of development towards the “double burden” of disease.\nSome quick clean-up to support putting labels on the cluster graphic.\n\n(df_label_help &lt;- df %&gt;% \n  select(1) %&gt;%\n  rownames_to_column())\n\n# A tibble: 54 × 2\n   rowname country     \n   &lt;chr&gt;   &lt;chr&gt;       \n 1 1       Algeria     \n 2 2       Angola      \n 3 3       Benin       \n 4 4       Botswana    \n 5 5       Burkina Faso\n 6 6       Burundi     \n 7 7       Cameroon    \n 8 8       Cape Verde  \n 9 9       Central AfR \n10 10      Chad        \n# ℹ 44 more rows\n\n\n\n\n   name          x           y       coord cluster\n1     1 -4.3024636  1.80865284 21.78241826       1\n2     2  2.5755214 -0.07889390  6.63953471       4\n3     3  1.0169684  0.33706615  1.14783841       3\n4     4 -2.3642511  0.63494494  5.99283814       2\n5     5  0.8993824  0.82947169  1.49691205       3\n6     6  3.5790154  0.12038452 12.82384342       4\n7     7  0.9569876 -1.10554744  2.13806044       4\n8     8 -3.1992280  1.56693482 12.69034466       1\n9     9  4.1259704 -2.22199890 21.96091056       4\n10   10  5.6425998 -2.51144500 38.14628795       4\n11   11  0.6317898  0.79935106  1.03812047       3\n12   12  2.8034932  0.29375838  7.94586811       4\n13   13  0.3687783  1.02798718  1.19275508       3\n14   14  1.2758298 -2.43850287  7.57403791       4\n15   15 -0.2122328 -0.08452750  0.05218765       3\n16   16 -4.8746473  0.56767475 24.08444092       1\n17   17  2.1326087 -1.89742082  8.14822560       4\n18   18  1.0316133  1.46454137  3.20910741       3\n19   19  0.8334748  1.56408985  3.14105736       3\n20   20 -0.8819849  0.08342655  0.78485739       3\n21   21  0.4273448  0.99698232  1.17659728       3\n22   22 -0.7456978  0.84322822  1.26709904       3\n23   23  1.0721493 -0.06478235  1.15370079       3\n24   24  1.9407232 -0.56190798  4.08214716       4\n25   25  0.6686306  3.08182329  9.94470168       3\n26   26 -1.6442519 -4.73840005 25.15599931       2\n27   27  0.8823869  1.08552443  1.95696991       3\n28   28 -5.0939159  1.05318693 27.05718209       1\n29   29 -0.1219820  1.81595847  3.31258478       3\n30   30  0.7539831  1.67684599  3.38030304       3\n31   31  0.9899307  0.48061503  1.21095368       3\n32   32  0.4174827  0.35810341  0.30252987       3\n33   33 -7.1835908 -2.57711364 58.24549129       1\n34   34 -4.8250325  1.68917653 26.13425564       1\n35   35  0.8549720  0.78819666  1.35223117       3\n36   36 -1.8082677 -0.69571408  3.75385013       2\n37   37  2.1517074  1.51364425  6.92096363       3\n38   38  3.2091357 -2.09075227 14.66979676       4\n39   39 -0.2361615  2.02429432  4.15353976       3\n40   40 -0.9941374  0.91320146  1.82224606       3\n41   41 -0.9138675  2.43145802  6.74714196       3\n42   42 -5.2103788 -3.34420231 38.33173673       1\n43   43  2.1218142 -2.41530504 10.33579390       4\n44   44  5.2056186 -0.93944297 27.98101823       4\n45   45 -3.6462041 -4.98956383 38.19055169       2\n46   46 -1.0108568 -0.38050415  1.16661488       3\n47   47  4.0455876 -0.62946960 16.76301062       4\n48   48 -0.6776526 -1.84331557  3.85702536       2\n49   49  0.3131814  1.70183621  2.99432904       3\n50   50  0.2815917  0.64157101  0.49090723       3\n51   51 -6.5797150 -0.47025448 43.51378831       1\n52   52  1.8420731  0.91715107  4.23439936       3\n53   53  1.3506020  1.25844087  3.40779929       3\n54   54  0.1235718 -0.29045781  0.09963573       3\n                         country\n1                        Algeria\n2                         Angola\n3                          Benin\n4                       Botswana\n5                   Burkina Faso\n6                        Burundi\n7                       Cameroon\n8                     Cape Verde\n9                    Central AfR\n10                          Chad\n11                       Comoros\n12 Congo, Democratic Republic of\n13            Congo, Republic of\n14                 Cote d'Ivoire\n15                      Djibouti\n16                         Egypt\n17                   Equa Guinea\n18                       Eritrea\n19                      Ethiopia\n20                         Gabon\n21                        Gambia\n22                         Ghana\n23                        Guinea\n24                    GuineaBiss\n25                         Kenya\n26                       Lesotho\n27                       Liberia\n28                         Libya\n29                    Madagascar\n30                        Malawi\n31                          Mali\n32                    Mauritania\n33                     Mauritius\n34                       Morocco\n35                    Mozambique\n36                       Namibia\n37                         Niger\n38                       Nigeria\n39                        Rwanda\n40         Sao Tome and Principe\n41                       Senegal\n42                    Seychelles\n43                     SierraLeo\n44                       Somalia\n45                  South Africa\n46                         Sudan\n47                   Sudan South\n48                     Swaziland\n49                      Tanzania\n50                          Togo\n51                       Tunisia\n52                        Uganda\n53                        Zambia\n54                      Zimbabwe\n\n\n\n\n   name          x           y       coord cluster\n1     1 -4.3024636  1.80865284 21.78241826       1\n2     2  2.5755214 -0.07889390  6.63953471       4\n3     3  1.0169684  0.33706615  1.14783841       3\n4     4 -2.3642511  0.63494494  5.99283814       2\n5     5  0.8993824  0.82947169  1.49691205       3\n6     6  3.5790154  0.12038452 12.82384342       4\n7     7  0.9569876 -1.10554744  2.13806044       4\n8     8 -3.1992280  1.56693482 12.69034466       1\n9     9  4.1259704 -2.22199890 21.96091056       4\n10   10  5.6425998 -2.51144500 38.14628795       4\n11   11  0.6317898  0.79935106  1.03812047       3\n12   12  2.8034932  0.29375838  7.94586811       4\n13   13  0.3687783  1.02798718  1.19275508       3\n14   14  1.2758298 -2.43850287  7.57403791       4\n15   15 -0.2122328 -0.08452750  0.05218765       3\n16   16 -4.8746473  0.56767475 24.08444092       1\n17   17  2.1326087 -1.89742082  8.14822560       4\n18   18  1.0316133  1.46454137  3.20910741       3\n19   19  0.8334748  1.56408985  3.14105736       3\n20   20 -0.8819849  0.08342655  0.78485739       3\n21   21  0.4273448  0.99698232  1.17659728       3\n22   22 -0.7456978  0.84322822  1.26709904       3\n23   23  1.0721493 -0.06478235  1.15370079       3\n24   24  1.9407232 -0.56190798  4.08214716       4\n25   25  0.6686306  3.08182329  9.94470168       3\n26   26 -1.6442519 -4.73840005 25.15599931       2\n27   27  0.8823869  1.08552443  1.95696991       3\n28   28 -5.0939159  1.05318693 27.05718209       1\n29   29 -0.1219820  1.81595847  3.31258478       3\n30   30  0.7539831  1.67684599  3.38030304       3\n31   31  0.9899307  0.48061503  1.21095368       3\n32   32  0.4174827  0.35810341  0.30252987       3\n33   33 -7.1835908 -2.57711364 58.24549129       1\n34   34 -4.8250325  1.68917653 26.13425564       1\n35   35  0.8549720  0.78819666  1.35223117       3\n36   36 -1.8082677 -0.69571408  3.75385013       2\n37   37  2.1517074  1.51364425  6.92096363       3\n38   38  3.2091357 -2.09075227 14.66979676       4\n39   39 -0.2361615  2.02429432  4.15353976       3\n40   40 -0.9941374  0.91320146  1.82224606       3\n41   41 -0.9138675  2.43145802  6.74714196       3\n42   42 -5.2103788 -3.34420231 38.33173673       1\n43   43  2.1218142 -2.41530504 10.33579390       4\n44   44  5.2056186 -0.93944297 27.98101823       4\n45   45 -3.6462041 -4.98956383 38.19055169       2\n46   46 -1.0108568 -0.38050415  1.16661488       3\n47   47  4.0455876 -0.62946960 16.76301062       4\n48   48 -0.6776526 -1.84331557  3.85702536       2\n49   49  0.3131814  1.70183621  2.99432904       3\n50   50  0.2815917  0.64157101  0.49090723       3\n51   51 -6.5797150 -0.47025448 43.51378831       1\n52   52  1.8420731  0.91715107  4.23439936       3\n53   53  1.3506020  1.25844087  3.40779929       3\n54   54  0.1235718 -0.29045781  0.09963573       3\n                         country    clust_name africa\n1                        Algeria          NCDs      1\n2                         Angola           CDs      1\n3                          Benin Double Burden      1\n4                       Botswana    NCDs & HIV      1\n5                   Burkina Faso Double Burden      1\n6                        Burundi           CDs      1\n7                       Cameroon           CDs      1\n8                     Cape Verde          NCDs      1\n9                    Central AfR           CDs      1\n10                          Chad           CDs      1\n11                       Comoros Double Burden      1\n12 Congo, Democratic Republic of           CDs      1\n13            Congo, Republic of Double Burden      1\n14                 Cote d'Ivoire           CDs      1\n15                      Djibouti Double Burden      1\n16                         Egypt          NCDs      1\n17                   Equa Guinea           CDs      1\n18                       Eritrea Double Burden      1\n19                      Ethiopia Double Burden      1\n20                         Gabon Double Burden      1\n21                        Gambia Double Burden      1\n22                         Ghana Double Burden      1\n23                        Guinea Double Burden      1\n24                    GuineaBiss           CDs      1\n25                         Kenya Double Burden      1\n26                       Lesotho    NCDs & HIV      1\n27                       Liberia Double Burden      1\n28                         Libya          NCDs      1\n29                    Madagascar Double Burden      1\n30                        Malawi Double Burden      1\n31                          Mali Double Burden      1\n32                    Mauritania Double Burden      1\n33                     Mauritius          NCDs      1\n34                       Morocco          NCDs      1\n35                    Mozambique Double Burden      1\n36                       Namibia    NCDs & HIV      1\n37                         Niger Double Burden      1\n38                       Nigeria           CDs      1\n39                        Rwanda Double Burden      1\n40         Sao Tome and Principe Double Burden      1\n41                       Senegal Double Burden      1\n42                    Seychelles          NCDs      1\n43                     SierraLeo           CDs      1\n44                       Somalia           CDs      1\n45                  South Africa    NCDs & HIV      1\n46                         Sudan Double Burden      1\n47                   Sudan South           CDs      1\n48                     Swaziland    NCDs & HIV      1\n49                      Tanzania Double Burden      1\n50                          Togo Double Burden      1\n51                       Tunisia          NCDs      1\n52                        Uganda Double Burden      1\n53                        Zambia Double Burden      1\n54                      Zimbabwe Double Burden      1\n\n\n\nkmeans_viz_data %&gt;% \n  ggplot(aes(x=x,\n             y=y,\n             color=clust_name,\n             shape=clust_name)) + \n  geom_point(size=3) + \n  ggrepel::geom_text_repel(data=. %&gt;% filter(country %in% spotlight),\n                           aes(label = country), force = 10, size=4) + \n  #ggtitle('K-means') + \n  labs(x=expression('' %&lt;-% '  Higher NCD burden, higher life expectancy    -    Higher CD burden, lower life expectancy  ' %-&gt;% ''),\n       y=expression('' %&lt;-% '  Higher burden from AIDS, respiratory and injury deaths')) + \n  scale_color_jco() + \n  my.theme"
  },
  {
    "objectID": "posts/2024-08-28-tidycensus-exploration-pt3/index.html",
    "href": "posts/2024-08-28-tidycensus-exploration-pt3/index.html",
    "title": "Part Three: Exploring even smaller geographies with tidycensus",
    "section": "",
    "text": "In Part One, I demonstrated how to fetch data and do some basic analysis of U.S. Census data. Each API call with the tidycensus:: package can only be for one year of data, so to do longitudinal analysis requires some additional wrangling. In this post, I’ll build a script that iterates through the available years, fetches the data, then combines the data into a single dataframe. Then, I’ll unpack some of the trends seen in child poverty in Colorado.\nIn Part Two, I showed how to fetch multiple years of data from the American Community Survey and create a single dataframe using purrr::map_df(). Then, I showed some different ways of visualizing the data, including margins of error, time series, and using Observable Plot.\nIn this post, I’d like to drill down further – to the Census tract level – to demonstrate how to fetch, wrangle, and visualize data using tidycensus::. I’ll also use some of the tigris:: package to enhance my geospatial plots.\n\n\n\nlibrary(tidyverse)\nlibrary(tidycensus)\n1library(scales)\n2library(janitor)\n3library(gt)\nlibrary(tigris)\n\n4# census_api_key('INSERT KEY HERE', install = TRUE)\n\n\n1\n\nLoading the scales:: package to transform ggplot scales simply (some people choose to explicitly define scales:: in their code rather than loading the library).\n\n2\n\nThe janitor::clean_names() function tidies the column names of your dataset to use the snake case convention. Very handy!\n\n3\n\nThe gt:: library provides functionality for creating ggplot-esque tables.\n\n4\n\nThe first time that you’re working with the tidycensus:: package, you need to request an API key at https://api.census.gov/data/key_signup.html. The install= argument will install your personal key to the .Renviron file, and you won’t need to use the census_api_key() function again."
  },
  {
    "objectID": "posts/2024-08-28-tidycensus-exploration-pt3/index.html#setup",
    "href": "posts/2024-08-28-tidycensus-exploration-pt3/index.html#setup",
    "title": "Part Three: Exploring even smaller geographies with tidycensus",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidycensus)\n1library(scales)\n2library(janitor)\n3library(gt)\nlibrary(tigris)\n\n4# census_api_key('INSERT KEY HERE', install = TRUE)\n\n\n1\n\nLoading the scales:: package to transform ggplot scales simply (some people choose to explicitly define scales:: in their code rather than loading the library).\n\n2\n\nThe janitor::clean_names() function tidies the column names of your dataset to use the snake case convention. Very handy!\n\n3\n\nThe gt:: library provides functionality for creating ggplot-esque tables.\n\n4\n\nThe first time that you’re working with the tidycensus:: package, you need to request an API key at https://api.census.gov/data/key_signup.html. The install= argument will install your personal key to the .Renviron file, and you won’t need to use the census_api_key() function again."
  },
  {
    "objectID": "posts/2024-08-28-tidycensus-exploration-pt3/index.html#a-note-on-mapping-with-tidycensus",
    "href": "posts/2024-08-28-tidycensus-exploration-pt3/index.html#a-note-on-mapping-with-tidycensus",
    "title": "Part Three: Exploring even smaller geographies with tidycensus",
    "section": "A note on mapping with tidycensus::",
    "text": "A note on mapping with tidycensus::\nIn Part One, I used the usmap:: package to quickly map state and county-level data. This package is handy for a quick US map, but it does have limitations, including:\n\nIt doesn’t map using ggplot2, so it’s not as pretty or customizable than I like, and\nIt’s only built for US maps of states and counties (and in this post, I want to map smaller areas).\n\nFortunately, tidycensus:: is built for this, too! The get_decennial() and get_acs() functions have an argument for geometry=, which if set to TRUE, will return the polygons for your data along with the fields requested in the API call. Geographic options include: “state”, “county”, “block group”, “tract”, “block”, and “zcta” (zip code tabulation area).1\n\nacs_data_tract &lt;- get_acs(\n  geography = \"tract\",\n  state = \"CO\",\n  county = c(\"Denver\", \"Jefferson\"),\n  # table = \"B27001\",\n  variables = c(male_u6_pop = \"B27001_003\", \n                male_u6_insured = \"B27001_004\", \n                female_u6_pop = \"B27001_031\", \n                female_u6_insured = \"B27001_032\"),\n  year = 2022,\n  survey = \"acs5\",\n  output = 'wide',\n  geometry = TRUE\n)\n\nI’ll now create some aggregate measures of insurance coverage estimates, along with the corresponding re-weighted margin of error values.\n\nacs_data_tract &lt;- acs_data_tract %&gt;% \n  mutate(total_u6_popE = male_u6_popE + female_u6_popE,\n         total_u6_insuredE = male_u6_insuredE + female_u6_insuredE,\n         perc_u6_insured = total_u6_insuredE / total_u6_popE,\n         total_u6_insured_MOEcalc = moe_sum(male_u6_insuredM, female_u6_insuredM, \n                                            estimate = c(male_u6_insuredE, female_u6_insuredE)),\n         perc_u6_insured_MOEcalc = total_u6_insured_MOEcalc / total_u6_popE)\n\nWith the tigris:: package, you can download major map features, including roads and water areas, which make maps more immediately relatable. In this post, I’m exploring Census tract data, which is not a well known unit, so adding roads and water area helps orient readers to the map.\n\nden_roads &lt;- roads(\"CO\", \"Denver\") %&gt;% \n  # this filter limits the roads to major streets only\n  filter(RTTYP %in% c(\"I\", \"S\", \"U\", \"C\"))\n\nden_water &lt;- area_water(\"CO\", \"Denver\")\n\njeff_roads &lt;- roads(\"CO\", \"Jefferson\") %&gt;%\n  filter(RTTYP %in% c(\"I\", \"S\", \"U\", \"C\"))\n\njeff_water &lt;- area_water(\"CO\", \"Jefferson\")"
  },
  {
    "objectID": "posts/2024-08-28-tidycensus-exploration-pt3/index.html#footnotes",
    "href": "posts/2024-08-28-tidycensus-exploration-pt3/index.html#footnotes",
    "title": "Part Three: Exploring even smaller geographies with tidycensus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOther geometries, such as school districts, can be accessed directly from the tigris:: package.↩︎"
  },
  {
    "objectID": "posts/2024-08-02-mapboxgl-exploration/index.html#setup",
    "href": "posts/2024-08-02-mapboxgl-exploration/index.html#setup",
    "title": "Exploring interactive mapping with mapboxgl",
    "section": "Setup",
    "text": "Setup\nRequired packages:\n\nlibrary(tidyverse)\nlibrary(mapgl)\n\nI followed Kyle’s instructions on this Youtube video (and plan to purchase his material for deeper learning)."
  },
  {
    "objectID": "posts/2024-08-02-mapboxgl-exploration/index.html#mapping",
    "href": "posts/2024-08-02-mapboxgl-exploration/index.html#mapping",
    "title": "Exploring interactive mapping with mapboxgl",
    "section": "Mapping",
    "text": "Mapping\nSteps:\n\nCreate a Mapbox account.\nGenerate an access key.\nUsing the usethis::edit_r_environ() function, add the Mapbox access token.\nRender the map as shown in the demo.\n\n\n# commenting out code as it doesn't render on Github pages, but it works in Quarto\n# mapboxgl(\n#   style = mapbox_style('satellite-streets'),\n#   center = c(-114.26608, 32.7213),\n#   zoom = 14,\n#   pitch = 80,\n#   bearing = 41\n# ) |&gt;\n#   add_raster_dem_source(\n#     id = 'mapbox-dem',\n#     url = 'mapbox://mapbox.mapbox-terrain-dem-v1',\n#     tileSize = 512,\n#     maxzoom = 14\n#   ) |&gt;\n#   set_terrain(\n#     source = 'mapbox-dem',\n#     exaggeration = 1.5\n#   )\n\nSome further pieces still to figure out:\n\nIt looks like I’m unable to render the interactive map on my Github pages site (I receive an error that it’s not an in-memory calc).\nThere’s also functionality to overlay data (such as Census data), which would be a neat continuation to explore.\nSoon, I’d like to use this to map Mount Blue Sky – our most recent 14er hike in Colorado!"
  },
  {
    "objectID": "posts/2024-09-15-pandas-datareader/index.html",
    "href": "posts/2024-09-15-pandas-datareader/index.html",
    "title": "Analyzing CPI with Python and R",
    "section": "",
    "text": "The pandas-datareader package is a powerful tool for easily accessing financial and economic data through various APIs. In this post, we’ll explore how to use it to fetch data from FRED (Federal Reserve Economic Data). Then, I’ll show some visualization techniques using matplotlib, plotnine, and ggplot2.\n\n\nFirst, import the packages:\n\n1import pandas as pd\nfrom matplotlib import pyplot as plt\nimport pandas_datareader as pdr\nfrom datetime import datetime\n\n\n1\n\nIf it’s your first time running code with these libraries, you’ll need to first use the pip install command. Since these are already installed for me locally, I can just import.\n\n\n\n\nNext, I’ll set a variable for the time frame that I’d like to use for this demonstration.\n\nstart = datetime(2016, 1, 1)\nend = datetime.now()"
  },
  {
    "objectID": "posts/2024-09-15-pandas-datareader/index.html#setup",
    "href": "posts/2024-09-15-pandas-datareader/index.html#setup",
    "title": "Analyzing CPI with Python and R",
    "section": "",
    "text": "First, import the packages:\n\n1import pandas as pd\nfrom matplotlib import pyplot as plt\nimport pandas_datareader as pdr\nfrom datetime import datetime\n\n\n1\n\nIf it’s your first time running code with these libraries, you’ll need to first use the pip install command. Since these are already installed for me locally, I can just import.\n\n\n\n\nNext, I’ll set a variable for the time frame that I’d like to use for this demonstration.\n\nstart = datetime(2016, 1, 1)\nend = datetime.now()"
  },
  {
    "objectID": "posts/2024-09-15-pandas-datareader/index.html#footnotes",
    "href": "posts/2024-09-15-pandas-datareader/index.html#footnotes",
    "title": "Analyzing CPI with Python and R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe Federal Reserve website is a great place to comb through to see all the available data (https://fred.stlouisfed.org/).↩︎"
  },
  {
    "objectID": "posts/2024-08-20-tidycensus-exploration-pt2/index.html",
    "href": "posts/2024-08-20-tidycensus-exploration-pt2/index.html",
    "title": "Part Two: Wrangling Census data for longitudinal analysis of child poverty",
    "section": "",
    "text": "In Part One, I demonstrated how to fetch data and do some basic analysis of U.S. Census data. Each API call with the tidycensus:: package can only be for one year of data, so to do longitudinal analysis requires some additional wrangling.\nIn this post, I’ll build a script that iterates through the available years, fetches the data, then combines the data into a single dataframe. Then, I’ll unpack some of the trends seen in child poverty in Colorado.\n\n\n\nlibrary(tidyverse)\nlibrary(tidycensus)\n1library(scales)\n2library(gt)\n3library(glue)\n\n4# census_api_key('INSERT KEY HERE', install = TRUE)\n\n\n1\n\nLoading the scales:: package to transform ggplot scales simply (some people choose to explicitly define scales:: in their code rather than loading the library).\n\n2\n\nThe gt:: library provides functionality for creating ggplot-esque tables.\n\n3\n\nThe glue:: package allows for simple addition of HTML to ggplot graphics.\n\n4\n\nThe first time that you’re working with the tidycensus:: package, you need to request an API key at https://api.census.gov/data/key_signup.html. The install= argument will install your personal key to the .Renviron file, and you won’t need to use the census_api_key() function again."
  },
  {
    "objectID": "posts/2024-08-20-tidycensus-exploration-pt2/index.html#setup",
    "href": "posts/2024-08-20-tidycensus-exploration-pt2/index.html#setup",
    "title": "Part Two: Wrangling Census data for longitudinal analysis of child poverty",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidycensus)\n1library(scales)\n2library(gt)\n3library(glue)\n\n4# census_api_key('INSERT KEY HERE', install = TRUE)\n\n\n1\n\nLoading the scales:: package to transform ggplot scales simply (some people choose to explicitly define scales:: in their code rather than loading the library).\n\n2\n\nThe gt:: library provides functionality for creating ggplot-esque tables.\n\n3\n\nThe glue:: package allows for simple addition of HTML to ggplot graphics.\n\n4\n\nThe first time that you’re working with the tidycensus:: package, you need to request an API key at https://api.census.gov/data/key_signup.html. The install= argument will install your personal key to the .Renviron file, and you won’t need to use the census_api_key() function again."
  },
  {
    "objectID": "posts/2024-08-20-tidycensus-exploration-pt2/index.html#fetching-from-the-tidycensus-api",
    "href": "posts/2024-08-20-tidycensus-exploration-pt2/index.html#fetching-from-the-tidycensus-api",
    "title": "Part Two: Wrangling Census data for longitudinal analysis of child poverty",
    "section": "Fetching from the tidycensus:: API",
    "text": "Fetching from the tidycensus:: API\n\n5years &lt;- seq(2005, 2022) %&gt;%\n6  setdiff(2020)\n\n\n5\n\nData for the ACS-1 product begins in 2005.\n\n6\n\nDue to the COVID-19 pandemic, the U.S. Census Bureau does not have standard ACS products available for 2020. The setdiff() function removes 2020 from the vector.\n\n\n\n\nNext, I’ll define a function to fetch the ACS data for each year in the vector.\n\nfetch_acs_data &lt;- function(year) {\n  get_acs(geography = \"county\", \n          state = \"Colorado\",\n          survey = 'acs1',\n          variables = c(male_u5_pop = 'B01001_003', \n                        female_u5_pop = 'B01001_027', \n                        male_u5_poverty = 'B17001_004', \n                        female_u5_poverty = 'B17001_018'),\n          year = year,\n          output = 'wide') %&gt;% \n    mutate(year = year)\n}\n\nThen, I’ll use purrr::map_df() to apply each year to the fetch_acs_data() function that I created, which will result in a single dataframe of all years.\n\ncombined_acs_data &lt;- map_df(years, fetch_acs_data)"
  },
  {
    "objectID": "posts/2024-08-20-tidycensus-exploration-pt2/index.html#data-wrangling",
    "href": "posts/2024-08-20-tidycensus-exploration-pt2/index.html#data-wrangling",
    "title": "Part Two: Wrangling Census data for longitudinal analysis of child poverty",
    "section": "Data wrangling",
    "text": "Data wrangling\n\nCreating some fields to combine gender-based poverty estimates and calculate a percent of the child population measure\n\n\ncombined_acs_data &lt;- combined_acs_data %&gt;% \n  mutate(total_u5_popE = male_u5_popE + female_u5_popE,\n         total_u5_povertyE = male_u5_povertyE + female_u5_povertyE,\n         perc_u5_poverty = total_u5_povertyE / total_u5_popE)\n\n\nCreating a county field that cleans the county_state field to only include the county name\n\n\ncombined_acs_data &lt;- combined_acs_data %&gt;% \n  mutate(county = str_remove(NAME, \" County.*\")) %&gt;% \n  select(county, everything())\n\n\nReading in a table that maps Colorado’s counties to a region of the state\n\n\ncolorado_regions &lt;- read_csv('.//data/colorado_regions.csv', show_col_types = FALSE) %&gt;%\n  mutate(region = as.factor(region))\n\n\n(Finally) creating the combined_acs_data dataframe for longitudinal analysis\n\n\ncombined_acs_data &lt;- combined_acs_data %&gt;% \n  left_join(x=.,\n            y=colorado_regions,\n            by='county')"
  },
  {
    "objectID": "posts/2024-08-20-tidycensus-exploration-pt2/index.html#footnotes",
    "href": "posts/2024-08-20-tidycensus-exploration-pt2/index.html#footnotes",
    "title": "Part Two: Wrangling Census data for longitudinal analysis of child poverty",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nScreenshot taken from the ACS 2018 Handbook, found here: https://www.census.gov/content/dam/Census/library/publications/2018/acs/acs_general_handbook_2018_ch03.pdf.↩︎\nBy default the tidycensus:: API provides the margin of error at the 90% confidence interval, but you can change this with the moe_level argument within get_acs().↩︎"
  },
  {
    "objectID": "workshops.html",
    "href": "workshops.html",
    "title": "Workshops",
    "section": "",
    "text": "PART TWO: Data Manipulation and Visualization Basics using R\n\n\n\nR\n\n\ntidyverse\n\n\nggplot\n\n\ndata-viz\n\n\ndomestic-politics\n\n\n\nThis skills workshop introduced R and the tidyverse to Master’s students at the University of Denver\n\n\n\nMickey Rafa\n\n\nMay 15, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPART ONE: Data Manipulation and Visualization Basics using R\n\n\n\nR\n\n\ntidyverse\n\n\nggplot\n\n\ndata-viz\n\n\ninternational-development\n\n\n\nThis skills workshop introduced R and the tidyverse to Master’s students at the University of Denver\n\n\n\nMickey Rafa\n\n\nMay 15, 2018\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "In progress"
  },
  {
    "objectID": "posts/2024-09-12-tidycensus-poverty-update/index.html",
    "href": "posts/2024-09-12-tidycensus-poverty-update/index.html",
    "title": "Reviewing the ACS-1 2023 child poverty estimates",
    "section": "",
    "text": "In a previous tidycensus:: post, I showed how to fetch data and do some basic, longitudinal analysis of U.S. Census data. Today, the Census Bureau released the 1-year estimates from the American Community Survey (ACS), so I’d like to see how child poverty seems to be changing from the 2022 to 2023 releases.\n\n\n\nlibrary(tidyverse)\nlibrary(tidycensus)\n1library(scales)\n2library(gt)\n3library(glue)\n4library(ggforce)\n\n5# census_api_key('INSERT KEY HERE', install = TRUE)\n\n\n1\n\nLoading the scales:: package to transform ggplot scales simply (some people choose to explicitly define scales:: in their code rather than loading the library).\n\n2\n\nThe gt:: library provides functionality for creating ggplot-esque tables.\n\n3\n\nThe glue:: package allows for simple addition of HTML to ggplot graphics.\n\n4\n\nThe ggforce:: package includes a geom_link() function, which I’ll use to create the comet effect in the comet plots that I use.\n\n5\n\nThe first time that you’re working with the tidycensus:: package, you need to request an API key at https://api.census.gov/data/key_signup.html. The install= argument will install your personal key to the .Renviron file, and you won’t need to use the census_api_key() function again."
  },
  {
    "objectID": "posts/2024-09-12-tidycensus-poverty-update/index.html#setup",
    "href": "posts/2024-09-12-tidycensus-poverty-update/index.html#setup",
    "title": "Reviewing the ACS-1 2023 child poverty estimates",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidycensus)\n1library(scales)\n2library(gt)\n3library(glue)\n4library(ggforce)\n\n5# census_api_key('INSERT KEY HERE', install = TRUE)\n\n\n1\n\nLoading the scales:: package to transform ggplot scales simply (some people choose to explicitly define scales:: in their code rather than loading the library).\n\n2\n\nThe gt:: library provides functionality for creating ggplot-esque tables.\n\n3\n\nThe glue:: package allows for simple addition of HTML to ggplot graphics.\n\n4\n\nThe ggforce:: package includes a geom_link() function, which I’ll use to create the comet effect in the comet plots that I use.\n\n5\n\nThe first time that you’re working with the tidycensus:: package, you need to request an API key at https://api.census.gov/data/key_signup.html. The install= argument will install your personal key to the .Renviron file, and you won’t need to use the census_api_key() function again."
  },
  {
    "objectID": "posts/2024-09-12-tidycensus-poverty-update/index.html#fetching-from-the-tidycensus-api",
    "href": "posts/2024-09-12-tidycensus-poverty-update/index.html#fetching-from-the-tidycensus-api",
    "title": "Reviewing the ACS-1 2023 child poverty estimates",
    "section": "Fetching from the tidycensus:: API",
    "text": "Fetching from the tidycensus:: API\n\nyears &lt;- c(2022, 2023)\n\nNext, I’ll define a function to fetch the ACS data for each year in the vector.\n\nfetch_acs_data &lt;- function(year) {\n  get_acs(geography = \"state\", \n          survey = 'acs1',\n          variables = c(male_u5_pop = 'B01001_003', \n                        female_u5_pop = 'B01001_027', \n                        male_u5_poverty = 'B17001_004', \n                        female_u5_poverty = 'B17001_018'),\n          year = year,\n          output = 'wide') %&gt;% \n    mutate(year = year,\n6           total_u5_popE = male_u5_popE + female_u5_popE,\n           total_u5_povertyE = male_u5_povertyE + female_u5_povertyE,\n           perc_u5_poverty = total_u5_povertyE / total_u5_popE)\n}\n\nfetch_acs_data_county &lt;- function(year) {\n  get_acs(geography = \"county\", \n          survey = 'acs1',\n          variables = c(male_u5_pop = 'B01001_003', \n                        female_u5_pop = 'B01001_027', \n                        male_u5_poverty = 'B17001_004', \n                        female_u5_poverty = 'B17001_018'),\n          year = year,\n          geometry = TRUE,\n          output = 'wide') %&gt;% \n    mutate(year = year,\n           total_u5_popE = male_u5_popE + female_u5_popE,\n           total_u5_povertyE = male_u5_povertyE + female_u5_povertyE,\n           perc_u5_poverty = total_u5_povertyE / total_u5_popE,\n7           county = str_remove(NAME, \" County.*\"),\n           state = str_extract(NAME, \"[\\\\w\\\\s]+$\") %&gt;% str_trim()) %&gt;%\n    select(county, state, everything())\n}\n\n\n6\n\nCreating some fields to combine gender-based poverty estimates and calculate a percent of the child population measure\n\n7\n\nCreating a state and county fields to manipulate the dataframe and visualize with more simplicity\n\n\n\n\nThen, I’ll use purrr::map_df() to apply each year to the fetch_acs_data() function that I created, which will result in a single dataframe of all years.\n\ncombined_acs_data &lt;- map_df(years, fetch_acs_data)\n\n\ncombined_acs_data_county &lt;- map_df(years, fetch_acs_data_county)\n\nGetting data from the 2022 1-year ACS\n\n\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\n\nDownloading feature geometry from the Census website.  To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`.\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |==                                                                    |   4%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |================                                                      |  24%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=====================================                                 |  54%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |============================================                          |  64%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |==========================================================            |  84%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |=================================================================     |  94%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\n\nGetting data from the 2023 1-year ACS\n\n\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\n\nDownloading feature geometry from the Census website.  To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`.\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |==                                                                    |   4%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |==============================                                        |  44%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=====================================                                 |  54%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |============================================                          |  64%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%"
  }
]